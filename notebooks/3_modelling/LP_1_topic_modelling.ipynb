{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "02d72bb0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/12.8 MB 2.7 MB/s eta 0:00:05\n",
            "     --- ------------------------------------ 1.0/12.8 MB 2.8 MB/s eta 0:00:05\n",
            "     ---- ----------------------------------- 1.6/12.8 MB 2.5 MB/s eta 0:00:05\n",
            "     ------ --------------------------------- 2.1/12.8 MB 2.6 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 2.6/12.8 MB 2.5 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 2.9/12.8 MB 2.4 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 3.1/12.8 MB 2.2 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ -------------------------- 4.2/12.8 MB 820.9 kB/s eta 0:00:11\n",
            "     -------------- ------------------------ 4.7/12.8 MB 864.4 kB/s eta 0:00:10\n",
            "     --------------- ----------------------- 5.0/12.8 MB 890.1 kB/s eta 0:00:09\n",
            "     --------------- ----------------------- 5.2/12.8 MB 895.1 kB/s eta 0:00:09\n",
            "     --------------- ----------------------- 5.2/12.8 MB 895.1 kB/s eta 0:00:09\n",
            "     ---------------- ---------------------- 5.5/12.8 MB 876.7 kB/s eta 0:00:09\n",
            "     ---------------- ---------------------- 5.5/12.8 MB 876.7 kB/s eta 0:00:09\n",
            "     ----------------- --------------------- 5.8/12.8 MB 858.8 kB/s eta 0:00:09\n",
            "     ----------------- --------------------- 5.8/12.8 MB 858.8 kB/s eta 0:00:09\n",
            "     ----------------- --------------------- 5.8/12.8 MB 858.8 kB/s eta 0:00:09\n",
            "     ------------------ -------------------- 6.0/12.8 MB 830.5 kB/s eta 0:00:09\n",
            "     ------------------- ------------------- 6.3/12.8 MB 837.0 kB/s eta 0:00:08\n",
            "     ------------------- ------------------- 6.6/12.8 MB 853.4 kB/s eta 0:00:08\n",
            "     -------------------- ------------------ 6.8/12.8 MB 867.8 kB/s eta 0:00:07\n",
            "     ---------------------- ---------------- 7.3/12.8 MB 892.9 kB/s eta 0:00:07\n",
            "     ----------------------- --------------- 7.9/12.8 MB 933.5 kB/s eta 0:00:06\n",
            "     -------------------------- ------------ 8.7/12.8 MB 996.6 kB/s eta 0:00:05\n",
            "     ---------------------------- ----------- 9.2/12.8 MB 1.0 MB/s eta 0:00:04\n",
            "     ------------------------------- -------- 10.0/12.8 MB 1.1 MB/s eta 0:00:03\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 11.0/12.8 MB 1.1 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 11.0/12.8 MB 1.1 MB/s eta 0:00:02\n",
            "     ------------------------------------ --- 11.5/12.8 MB 1.1 MB/s eta 0:00:02\n",
            "     -------------------------------------- - 12.3/12.8 MB 1.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 1.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 1.2 MB/s eta 0:00:00\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"LP_topic_modelling_extended\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "06c9977d-6e3d-45dc-f29f-863252d7e086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2==3.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: nltk>=3.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: spacy>=3.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.7)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 6)) (3.10.3)\n",
            "Requirement already satisfied: seaborn in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: torch in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 9)) (2.7.1)\n",
            "Requirement already satisfied: transformers in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 10)) (4.52.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 11)) (2.3.0)\n",
            "Collecting bertopic (from -r requirements.txt (line 12))\n",
            "  Using cached bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting umap-learn (from -r requirements.txt (line 13))\n",
            "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting python-dev-tools (from -r requirements.txt (line 14))\n",
            "  Downloading python_dev_tools-2023.3.24-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting hdbscan==0.8.40 (from -r requirements.txt (line 15))\n",
            "  Using cached hdbscan-0.8.40.tar.gz (6.9 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting sentence-transformers (from -r requirements.txt (line 16))\n",
            "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: click in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (75.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->nltk>=3.0.0->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (7.1.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.17.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 9)) (3.16.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 9)) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: fsspec in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 9)) (2025.5.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers->-r requirements.txt (line 10)) (0.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers->-r requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers->-r requirements.txt (line 10)) (0.5.3)\n",
            "Collecting plotly>=4.7.0 (from bertopic->-r requirements.txt (line 12))\n",
            "  Using cached plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting numba>=0.51.2 (from umap-learn->-r requirements.txt (line 13))\n",
            "  Using cached numba-0.61.2-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
            "Collecting pynndescent>=0.5 (from umap-learn->-r requirements.txt (line 13))\n",
            "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting Sphinx<7,>=6 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinx-6.2.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting autoflake<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading autoflake-1.7.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting black<24,>=23 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading black-23.12.1-py3-none-any.whl.metadata (68 kB)\n",
            "Collecting coverage<8,>=7 (from coverage[toml]<8,>=7->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading coverage-7.9.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
            "Collecting darglint<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading darglint-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dlint<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading dlint-0.16.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting doc8<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading doc8-1.1.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting docformatter<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading docformatter-1.7.7-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting flake8<6,>=5 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting flake8-2020<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_2020-1.8.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting flake8-aaa<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_aaa-0.17.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting flake8-annotations<4,>=3 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations-3.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-annotations-complexity<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations_complexity-0.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-annotations-coverage<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations_coverage-0.0.6-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-bandit<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_bandit-4.1.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-black<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_black-0.3.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-blind-except<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-blind-except-0.2.1.tar.gz (3.7 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-breakpoint<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_breakpoint-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-broken-line<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_broken_line-0.6.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-bugbear<24,>=23 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_bugbear-23.12.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting flake8-builtins<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting flake8-class-attributes-order<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_class_attributes_order-0.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting flake8-coding<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_coding-1.3.2-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-cognitive-complexity<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_cognitive_complexity-0.1.0.tar.gz (3.1 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-comments<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_comments-0.1.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting flake8-comprehensions<4,>=3 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_comprehensions-3.16.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-debugger<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_debugger-4.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting flake8-django<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_django-1.4.tar.gz (8.4 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-docstrings<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_docstrings-1.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-encodings<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_encodings-0.5.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting flake8-eradicate<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_eradicate-1.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flake8-executable<3,>=2 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_executable-2.1.3-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-expression-complexity<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_expression_complexity-0.0.11-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting flake8-fastapi<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_fastapi-0.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting flake8-fixme<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_fixme-1.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting flake8-functions<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_functions-0.0.8-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-functions-names<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_functions_names-0.4.0.tar.gz (10 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-future-annotations<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_future_annotations-0.0.5-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-isort<7,>=6 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_isort-6.1.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting flake8-literal<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_literal-1.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting flake8-logging-format<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-logging-format-0.9.0.tar.gz (10 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-markdown<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_markdown-0.6.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting flake8-mutable<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-mutable-1.2.0.tar.gz (3.0 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-no-pep420<3,>=2 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_no_pep420-2.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting flake8-noqa<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_noqa-1.4.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting flake8-pie<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_pie-0.16.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-pyi<23,>=22 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_pyi-22.11.0-py37-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-pylint<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_pylint-0.2.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting flake8-pytest-style<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_pytest_style-1.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-quotes<4,>=3 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-quotes-3.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-rst-docstrings<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_rst_docstrings-0.3.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-secure-coding-standard<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_secure_coding_standard-1.4.1-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting flake8-simplify<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_simplify-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-string-format<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_string_format-0.3.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting flake8-tidy-imports<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_tidy_imports-4.11.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting flake8-typing-imports<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_typing_imports-1.16.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-use-fstring<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-use-fstring-1.4.tar.gz (5.8 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting flake8-use-pathlib<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_use_pathlib-0.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting flake8-useless-assert<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_useless_assert-0.4.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting flake8-variables-names<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_variables_names-0.0.6-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting flake8-warnings<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_warnings-0.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting jupyterlab-flake8<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading jupyterlab_flake8-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pandas-vet<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pandas_vet-0.2.3-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pep8-naming<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pep8_naming-0.15.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pip<23,>=22 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pip-22.3.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pybetter<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pybetter-0.4.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pycln<3,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pycln-2.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pycodestyle<3,>=2 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pydocstyle<7,>=6 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pydocstyle-6.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pytest<8,>=7 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting pytest-cov<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pytest_cov-4.1.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting pytest-sugar<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pytest_sugar-0.9.7-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pyupgrade<4,>=3 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pyupgrade-3.20.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting removestar<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading removestar-1.5.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ssort<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading ssort-0.14.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tox<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading tox-4.26.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tox-travis<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading tox_travis-0.13-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pyflakes<3,>=1.1.0 (from autoflake<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pyflakes-2.5.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (4.3.6)\n",
            "Collecting docutils<=0.21.2,>=0.19 (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting restructuredtext-lint>=0.7 (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading restructuredtext_lint-1.4.0.tar.gz (16 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting stevedore (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: Pygments in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.19.1)\n",
            "Collecting untokenize<0.2.0,>=0.1.1 (from docformatter<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading untokenize-0.1.1.tar.gz (3.1 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8<6,>=5->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<3,>=2 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pycodestyle-2.9.1-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: asttokens>=2 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from flake8-aaa<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.0.0)\n",
            "Collecting attrs>=21.4 (from flake8-annotations<4,>=3->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting bandit>=1.7.3 (from flake8-bandit<5,>=4->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading bandit-1.8.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting flake8-plugin-utils<2.0,>=1.0 (from flake8-breakpoint<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_plugin_utils-1.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "INFO: pip is looking at multiple versions of flake8-bugbear to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flake8-bugbear<24,>=23 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_bugbear-23.11.28-py3-none-any.whl.metadata (29 kB)\n",
            "  Downloading flake8_bugbear-23.11.26-py3-none-any.whl.metadata (29 kB)\n",
            "  Downloading flake8_bugbear-23.9.16-py3-none-any.whl.metadata (28 kB)\n",
            "  Downloading flake8_bugbear-23.7.10-py3-none-any.whl.metadata (27 kB)\n",
            "  Downloading flake8_bugbear-23.6.5-py3-none-any.whl.metadata (26 kB)\n",
            "  Downloading flake8_bugbear-23.5.9-py3-none-any.whl.metadata (26 kB)\n",
            "  Downloading flake8_bugbear-23.3.23-py3-none-any.whl.metadata (25 kB)\n",
            "INFO: pip is still looking at multiple versions of flake8-bugbear to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading flake8_bugbear-23.3.12-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting cognitive_complexity (from flake8-cognitive-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading cognitive_complexity-1.3.0.tar.gz (5.7 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting astroid<3.0.0,>=2.15.2 (from flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading astroid-2.15.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting lazy-object-proxy>=1.4.0 (from astroid<3.0.0,>=2.15.2->flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading lazy_object_proxy-1.11.0-cp313-cp313-win_amd64.whl.metadata (8.7 kB)\n",
            "Collecting astatine>=0.3.1 (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading astatine-0.3.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting domdf-python-tools>=2.8.1 (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading domdf_python_tools-3.10.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting flake8-helper>=0.1.1 (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_helper-0.2.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting eradicate<3.0,>=2.0 (from flake8-eradicate<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading eradicate-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting astpretty (from flake8-expression-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading astpretty-3.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting fastapi>=0.65.1 (from flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting mr-proper (from flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading mr_proper-0.0.7-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting isort<7,>=5.0.0 (from flake8-isort<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading isort-6.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pylint (from flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pylint-3.3.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting astor>=0.1 (from flake8-simplify<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "INFO: pip is looking at multiple versions of flake8-warnings to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flake8-warnings<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_warnings-0.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting hypothesmith<0.2.0,>=0.1.8 (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading hypothesmith-0.1.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting libcst<0.5.0,>=0.4.1 (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading libcst-0.4.10.tar.gz (752 kB)\n",
            "     ---------------------------------------- 0.0/752.7 kB ? eta -:--:--\n",
            "     -------------------------------------- 752.7/752.7 kB 4.1 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting pyemojify<0.3.0,>=0.2.0 (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pyemojify-0.2.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hypothesis>=5.41.0 (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading hypothesis-6.135.9-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting lark-parser>=0.7.2 (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting typing_inspect>=0.4.0 (from libcst<0.5.0,>=0.4.1->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tomlkit>=0.11.1 (from pycln<3,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting snowballstemmer>=2.2.0 (from pydocstyle<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting iniconfig (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pluggy<2.0,>=0.12 (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting termcolor>=2.1.0 (from pytest-sugar<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting tokenize-rt>=6.1.0 (from pyupgrade<4,>=3->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting sphinxcontrib-applehelp (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-devhelp (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-jsmath (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-serializinghtml>=1.1.5 (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sphinxcontrib-qthelp (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting docutils<=0.21.2,>=0.19 (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting babel>=2.9 (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting alabaster<0.8,>=0.7 (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting imagesize>=1.3 (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting cachetools>=5.5.1 (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading cachetools-6.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting chardet>=5.2 (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pyproject-api>=1.8 (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pyproject_api-1.9.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting virtualenv>=20.31 (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "INFO: pip is looking at multiple versions of tox-travis to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tox-travis<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading tox_travis-0.12-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting natsort>=7.0.1 (from domdf-python-tools>=2.8.1->flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis>=5.41.0->hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn->-r requirements.txt (line 13))\n",
            "  Using cached llvmlite-0.44.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 11))\n",
            "  Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting narwhals>=1.15.1 (from plotly>=4.7.0->bertopic->-r requirements.txt (line 12))\n",
            "  Using cached narwhals-1.42.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.2)\n",
            "Collecting pbr>=2.0.0 (from stevedore->doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from virtualenv>=20.31->tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (0.3.9)\n",
            "Collecting stdlib-list>=0.5.0 (from mr-proper->flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading stdlib_list-0.11.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "INFO: pip is looking at multiple versions of pylint to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pylint (from flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pylint-3.3.6-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.3.5-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.6 (from pylint->flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pylint (from flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading pylint-3.3.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.3.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.3.0-py3-none-any.whl.metadata (12 kB)\n",
            "INFO: pip is still looking at multiple versions of pylint to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading pylint-3.2.7-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.6-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.5-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading pylint-3.2.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.0.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading pylint-2.17.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting isort<7,>=5.0.0 (from flake8-isort<7,>=6->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Using cached bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
            "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "Downloading python_dev_tools-2023.3.24-py3-none-any.whl (9.4 kB)\n",
            "Downloading autoflake-1.7.8-py3-none-any.whl (31 kB)\n",
            "Downloading black-23.12.1-py3-none-any.whl (194 kB)\n",
            "Downloading coverage-7.9.1-cp313-cp313-win_amd64.whl (215 kB)\n",
            "Downloading darglint-1.8.1-py3-none-any.whl (120 kB)\n",
            "Downloading dlint-0.16.0-py3-none-any.whl (44 kB)\n",
            "Downloading doc8-1.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading docformatter-1.7.7-py3-none-any.whl (33 kB)\n",
            "Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n",
            "Downloading flake8_2020-1.8.1-py2.py3-none-any.whl (4.9 kB)\n",
            "Downloading flake8_aaa-0.17.0-py3-none-any.whl (20 kB)\n",
            "Downloading flake8_annotations-3.1.1-py3-none-any.whl (16 kB)\n",
            "Downloading flake8_annotations_complexity-0.1.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading flake8_annotations_coverage-0.0.6-py3-none-any.whl (5.0 kB)\n",
            "Downloading flake8_bandit-4.1.1-py3-none-any.whl (4.8 kB)\n",
            "Downloading flake8_black-0.3.6-py3-none-any.whl (9.9 kB)\n",
            "Downloading flake8_breakpoint-1.1.0-py3-none-any.whl (5.1 kB)\n",
            "Downloading flake8_broken_line-0.6.0-py3-none-any.whl (4.3 kB)\n",
            "Downloading flake8_bugbear-23.3.12-py3-none-any.whl (28 kB)\n",
            "Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flake8_class_attributes_order-0.3.0-py3-none-any.whl (10.0 kB)\n",
            "Downloading flake8_coding-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading flake8_comments-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading flake8_comprehensions-3.16.0-py3-none-any.whl (8.2 kB)\n",
            "Downloading flake8_debugger-4.1.2-py3-none-any.whl (7.9 kB)\n",
            "Downloading astroid-2.15.8-py3-none-any.whl (278 kB)\n",
            "Downloading flake8_docstrings-1.7.0-py2.py3-none-any.whl (5.0 kB)\n",
            "Downloading flake8_encodings-0.5.1-py3-none-any.whl (25 kB)\n",
            "Downloading flake8_eradicate-1.5.0-py3-none-any.whl (5.1 kB)\n",
            "Downloading eradicate-2.3.0-py3-none-any.whl (6.1 kB)\n",
            "Downloading flake8_executable-2.1.3-py3-none-any.whl (35 kB)\n",
            "Downloading flake8_expression_complexity-0.0.11-py3-none-any.whl (8.2 kB)\n",
            "Downloading flake8_fastapi-0.7.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading flake8_fixme-1.1.1-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading flake8_functions-0.0.8-py3-none-any.whl (6.9 kB)\n",
            "Downloading flake8_future_annotations-0.0.5-py3-none-any.whl (8.7 kB)\n",
            "Downloading flake8_isort-6.1.2-py3-none-any.whl (18 kB)\n",
            "Downloading flake8_literal-1.4.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading flake8_markdown-0.6.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading flake8_no_pep420-2.8.0-py3-none-any.whl (4.8 kB)\n",
            "Downloading flake8_noqa-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading flake8_pie-0.16.0-py3-none-any.whl (49 kB)\n",
            "Downloading flake8_plugin_utils-1.3.3-py3-none-any.whl (9.7 kB)\n",
            "Downloading flake8_pyi-22.11.0-py37-none-any.whl (24 kB)\n",
            "Downloading flake8_pylint-0.2.1-py3-none-any.whl (3.8 kB)\n",
            "Downloading flake8_pytest_style-1.7.2-py3-none-any.whl (19 kB)\n",
            "Downloading flake8_rst_docstrings-0.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading flake8_secure_coding_standard-1.4.1-py2.py3-none-any.whl (14 kB)\n",
            "Downloading flake8_simplify-0.22.0-py3-none-any.whl (26 kB)\n",
            "Downloading flake8_string_format-0.3.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading flake8_tidy_imports-4.11.0-py3-none-any.whl (10 kB)\n",
            "Downloading flake8_typing_imports-1.16.0-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading flake8_use_pathlib-0.3.0-py3-none-any.whl (4.9 kB)\n",
            "Downloading flake8_useless_assert-0.4.4-py3-none-any.whl (5.8 kB)\n",
            "Downloading flake8_variables_names-0.0.6-py3-none-any.whl (6.1 kB)\n",
            "Downloading flake8_warnings-0.4.0-py3-none-any.whl (10 kB)\n",
            "Downloading jupyterlab_flake8-0.7.1-py3-none-any.whl (207 kB)\n",
            "Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading pandas_vet-0.2.3-py3-none-any.whl (8.9 kB)\n",
            "Downloading pep8_naming-0.15.1-py3-none-any.whl (9.6 kB)\n",
            "Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.5/2.1 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.0/2.1 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.3/2.1 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.6/2.1 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 1.6 MB/s eta 0:00:00\n",
            "Downloading pybetter-0.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading hypothesmith-0.1.9-py3-none-any.whl (18 kB)\n",
            "Downloading pycln-2.5.0-py3-none-any.whl (38 kB)\n",
            "Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
            "Downloading pydocstyle-6.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading pyemojify-0.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
            "Downloading pytest-7.4.4-py3-none-any.whl (325 kB)\n",
            "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
            "Downloading pytest_cov-4.1.0-py3-none-any.whl (21 kB)\n",
            "Downloading pytest_sugar-0.9.7-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pyupgrade-3.20.0-py2.py3-none-any.whl (62 kB)\n",
            "Downloading removestar-1.5.2-py3-none-any.whl (16 kB)\n",
            "Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 2.2 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.6/3.0 MB 1.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 2.1/3.0 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 2.6/3.0 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.0/3.0 MB 2.2 MB/s eta 0:00:00\n",
            "Downloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "   ---------------------------------------- 0.0/570.5 kB ? eta -:--:--\n",
            "   ------------------ --------------------- 262.1/570.5 kB ? eta -:--:--\n",
            "   -------------------------------------- 570.5/570.5 kB 916.8 kB/s eta 0:00:00\n",
            "Downloading ssort-0.14.0-py3-none-any.whl (26 kB)\n",
            "Downloading tox-4.26.0-py3-none-any.whl (172 kB)\n",
            "Downloading tox_travis-0.12-py2.py3-none-any.whl (10 kB)\n",
            "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "Downloading astatine-0.3.3-py3-none-any.whl (17 kB)\n",
            "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
            "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/10.2 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.3/10.2 MB 3.4 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 2.1/10.2 MB 3.6 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 2.9/10.2 MB 3.7 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 3.7/10.2 MB 3.7 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 4.5/10.2 MB 3.7 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 5.0/10.2 MB 3.5 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 6.0/10.2 MB 3.6 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.8/10.2 MB 3.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 7.3/10.2 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.9/10.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.4/10.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.9/10.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.4/10.2 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.0/10.2 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.2/10.2 MB 3.1 MB/s eta 0:00:00\n",
            "Downloading bandit-1.8.3-py3-none-any.whl (129 kB)\n",
            "Downloading cachetools-6.0.0-py3-none-any.whl (10 kB)\n",
            "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "Downloading domdf_python_tools-3.10.0-py3-none-any.whl (126 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "Downloading flake8_helper-0.2.2-py3-none-any.whl (13 kB)\n",
            "Downloading hypothesis-6.135.9-py3-none-any.whl (518 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "Downloading lazy_object_proxy-1.11.0-cp313-cp313-win_amd64.whl (28 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
            "Using cached numba-0.61.2-cp313-cp313-win_amd64.whl (2.8 MB)\n",
            "Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
            "Using cached llvmlite-0.44.0-cp313-cp313-win_amd64.whl (30.3 MB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Using cached plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
            "Using cached narwhals-1.42.1-py3-none-any.whl (359 kB)\n",
            "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "Downloading pyproject_api-1.9.1-py3-none-any.whl (13 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
            "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
            "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
            "Downloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "Downloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl (6.0 kB)\n",
            "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "   ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.5/6.1 MB 3.3 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 1.3/6.1 MB 3.6 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 1.8/6.1 MB 3.4 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 2.1/6.1 MB 2.9 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 2.9/6.1 MB 2.9 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.1/6.1 MB 2.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.1/6.1 MB 2.6 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 3.4/6.1 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 3.7/6.1 MB 1.9 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 3.7/6.1 MB 1.9 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 3.9/6.1 MB 1.7 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 4.5/6.1 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 4.7/6.1 MB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 5.0/6.1 MB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.2/6.1 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.2/6.1 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.2/6.1 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.5/6.1 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 5.8/6.1 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.1/6.1 MB 1.4 MB/s eta 0:00:00\n",
            "Downloading astpretty-3.0.0-py2.py3-none-any.whl (4.9 kB)\n",
            "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading mr_proper-0.0.7-py3-none-any.whl (11 kB)\n",
            "Downloading stdlib_list-0.11.1-py3-none-any.whl (83 kB)\n",
            "Downloading pylint-2.17.7-py3-none-any.whl (537 kB)\n",
            "   ---------------------------------------- 0.0/537.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/537.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/537.2 kB ? eta -:--:--\n",
            "   ------------------- -------------------- 262.1/537.2 kB ? eta -:--:--\n",
            "   ------------------- -------------------- 262.1/537.2 kB ? eta -:--:--\n",
            "   -------------------------------------- 537.2/537.2 kB 671.0 kB/s eta 0:00:00\n",
            "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
            "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
            "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: To modify pip, please run the following command:\n",
            "C:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install -r requirements.txt\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "    \n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6147c7a",
      "metadata": {},
      "source": [
        "## Local Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f2963e1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file\n",
        "from src.utils.common_helpers import read_list_from_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2569d135",
      "metadata": {},
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a60bdaad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_df(df, group_by_columns, agg_column='content'):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by specified columns and aggregates the content column.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: DataFrame to group\n",
        "    - group_by_columns: List of columns to group by\n",
        "    - agg_column: Column to aggregate (default is 'content')\n",
        "    \n",
        "    Returns:\n",
        "    - Grouped DataFrame with aggregated content\n",
        "    \"\"\"\n",
        "    return df.groupby(group_by_columns, as_index=False).agg({agg_column: ' '.join})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f87c1ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4f87c1ff",
        "outputId": "a5156662-3175-436c-f078-7caafaec09f8"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')\n",
        "\n",
        "\n",
        "# Goldman Sachs\n",
        "grouped_gs_discussion_df = group_df(gs_discussion_df, ['quarter', 'year'])\n",
        "grouped_gs_qna_df = group_df(gs_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n",
        "\n",
        "# JP Morgan\n",
        "grouped_jp_discussion_df = group_df(jp_discussion_df, ['quarter', 'year'])\n",
        "grouped_jp_qna_df = group_df(jp_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {},
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "23cf4579",
      "metadata": {},
      "outputs": [],
      "source": [
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b95b9b4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "all_stop_words = nlp.Defaults.stop_words.union(gs_stopwords)\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "    \n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "    \n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ed49464d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bertopic\n",
            "  Using cached bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Using cached hdbscan-0.8.40.tar.gz (6.9 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bertopic) (2.3.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bertopic) (2.3.0)\n",
            "Collecting plotly>=4.7.0 (from bertopic)\n",
            "  Using cached plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bertopic) (1.7.0)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bertopic) (4.67.1)\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Collecting narwhals>=1.15.1 (from plotly>=4.7.0->bertopic)\n",
            "  Using cached narwhals-1.42.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.32.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
            "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
            "  Using cached numba-0.61.2-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
            "  Using cached llvmlite-0.44.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting numpy>=1.20.0 (from bertopic)\n",
            "  Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2025.4.26)\n",
            "Using cached bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
            "Using cached plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
            "Using cached narwhals-1.42.1-py3-none-any.whl (359 kB)\n",
            "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "Using cached numba-0.61.2-cp313-cp313-win_amd64.whl (2.8 MB)\n",
            "Using cached llvmlite-0.44.0-cp313-cp313-win_amd64.whl (30.3 MB)\n",
            "Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
            "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml): started\n",
            "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
            "Failed to build hdbscan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Ã— Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
            "  â”‚ exit code: 1\n",
            "  â•°â”€> [39 lines of output]\n",
            "      C:\\Users\\zifen\\AppData\\Local\\Temp\\pip-build-env-qwvygn_c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
            "        warnings.warn(msg)\n",
            "      C:\\Users\\zifen\\AppData\\Local\\Temp\\pip-build-env-qwvygn_c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
            "        warnings.warn(msg)\n",
            "      C:\\Users\\zifen\\AppData\\Local\\Temp\\pip-build-env-qwvygn_c\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "      \n",
            "              License :: OSI Approved\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        self._finalize_license_expression()\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\branches.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
            "      creating build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_branches.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
            "      running build_ext\n",
            "      building 'hdbscan._hdbscan_tree' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for hdbscan\n",
            "ERROR: Failed to build installable wheels for some pyproject.toml based projects (hdbscan)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bertopic'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install bertopic\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bertopic'"
          ]
        }
      ],
      "source": [
        "!pip install bertopic \n",
        "\n",
        "from bertopic import BERTopic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fb801083",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERTopic, UMAP, HDBSCAN, or Sentence Transformers not found. BERTopic functionality will be disabled.\n",
            "\n",
            "\n",
            "##################################\n",
            "### Running Topic Modeling with LDA ###\n",
            "##################################\n",
            "\n",
            "--- Fitting LDA Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (LDA) with 5 topics...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 346\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m##################################\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    341\u001b[39m lda_pipeline_instance = TopicModelingPipeline(\n\u001b[32m    342\u001b[39m     model_type=\u001b[33m'\u001b[39m\u001b[33mlda\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    343\u001b[39m     n_components=\u001b[32m5\u001b[39m, \u001b[38;5;66;03m# Number of topics for LDA\u001b[39;00m\n\u001b[32m    344\u001b[39m     min_df=\u001b[32m2\u001b[39m \u001b[38;5;66;03m# Adjusted min_df for dummy data\u001b[39;00m\n\u001b[32m    345\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[43mlda_pipeline_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_gs_qna_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m lda_model = lda_pipeline_instance.get_topic_model()\n\u001b[32m    349\u001b[39m tfidf_vectorizer = lda_pipeline_instance.get_vectorizer()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 294\u001b[39m, in \u001b[36mTopicModelingPipeline.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fits the entire pipeline to the input data.\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Fitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_type.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Topic Modeling Pipeline ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py:661\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    656\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    657\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    658\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    659\u001b[39m             all_params=params,\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mLDATopicModeler.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Phase 3: Topic Modeling (LDA) with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m topics...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m.lda_model = LatentDirichletAllocation(\n\u001b[32m     77\u001b[39m     n_components=\u001b[38;5;28mself\u001b[39m.n_components,\n\u001b[32m     78\u001b[39m     max_iter=\u001b[38;5;28mself\u001b[39m.max_iter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     evaluate_every=\u001b[38;5;28mself\u001b[39m.evaluate_every\n\u001b[32m     84\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLDA model fitting complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m topics.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:666\u001b[39m, in \u001b[36mLatentDirichletAllocation.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m learning_method == \u001b[33m\"\u001b[39m\u001b[33monline\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx_slice \u001b[38;5;129;01min\u001b[39;00m gen_batches(n_samples, batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    673\u001b[39m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28mself\u001b[39m._em_step(\n\u001b[32m    675\u001b[39m         X, total_samples=n_samples, batch_update=\u001b[38;5;28;01mTrue\u001b[39;00m, parallel=parallel\n\u001b[32m    676\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001b[39m, in \u001b[36mLatentDirichletAllocation._em_step\u001b[39m\u001b[34m(self, X, total_samples, batch_update, parallel)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[32m    497\u001b[39m \n\u001b[32m    498\u001b[39m \u001b[33;03mupdate `component_` by batch VB or online VB.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m \u001b[33;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m _, suff_stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001b[39m, in \u001b[36mLatentDirichletAllocation._e_step\u001b[39m\u001b[34m(self, X, cal_sstats, random_init, parallel)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    465\u001b[39m     parallel = Parallel(n_jobs=n_jobs, verbose=\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.verbose - \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[32m    480\u001b[39m doc_topics, sstats_list = \u001b[38;5;28mzip\u001b[39m(*results)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zifen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "\n",
        "# Conditional imports for BERTopic\n",
        "try:\n",
        "    from bertopic import BERTopic\n",
        "    from umap import UMAP\n",
        "    import hdbscan\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    BERTOPIC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"BERTopic, UMAP, HDBSCAN, or Sentence Transformers not found. BERTopic functionality will be disabled.\")\n",
        "    BERTOPIC_AVAILABLE = False\n",
        "\n",
        "# --- Mock Preprocessing Function (replace with your actual preprocess_text) ---\n",
        "def preprocess_text(text, stop_words=None, abbreviations=None):\n",
        "    \"\"\"\n",
        "    A mock preprocessing function to clean and prepare text.\n",
        "    In a real application, this would include tokenization, stemming/lemmatization,\n",
        "    punctuation removal, number handling, etc.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    # Simple tokenization and stop word removal for demonstration\n",
        "    words = text.split()\n",
        "    if stop_words:\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "    # Simple abbreviation handling (example: 'ai' -> 'artificial intelligence')\n",
        "    if abbreviations:\n",
        "        for abbr, full in abbreviations.items():\n",
        "            words = [full if word == abbr else word for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# --- Custom Transformer for Text Preprocessing ---\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn transformer to apply text preprocessing.\n",
        "    It wraps the 'preprocess_text' function.\n",
        "    \"\"\"\n",
        "    def __init__(self, stop_words=None, abbreviations=None):\n",
        "        self.stop_words = stop_words\n",
        "        self.abbreviations = abbreviations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(\"Starting Phase 1: Preprocessing...\")\n",
        "        preprocessed_X = [preprocess_text(text, self.stop_words, self.abbreviations) for text in X]\n",
        "        print(\"Preprocessing complete.\")\n",
        "        return pd.Series(preprocessed_X)\n",
        "\n",
        "# --- Custom Estimator for LDA Topic Modeling ---\n",
        "class LDATopicModeler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn estimator that wraps LatentDirichletAllocation.\n",
        "    Allows for fitting and transforming data using an LDA model within a pipeline.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components=10, max_iter=10, learning_method='online',\n",
        "                 learning_decay=0.7, random_state=42, n_jobs=-1, evaluate_every=10):\n",
        "        self.n_components = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_method = learning_method\n",
        "        self.learning_decay = learning_decay\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.evaluate_every = evaluate_every\n",
        "        self.lda_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(f\"\\nStarting Phase 3: Topic Modeling (LDA) with {self.n_components} topics...\")\n",
        "        self.lda_model = LatentDirichletAllocation(\n",
        "            n_components=self.n_components,\n",
        "            max_iter=self.max_iter,\n",
        "            learning_method=self.learning_method,\n",
        "            learning_decay=self.learning_decay,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=self.n_jobs,\n",
        "            evaluate_every=self.evaluate_every\n",
        "        )\n",
        "        self.lda_model.fit(X)\n",
        "        print(f\"LDA model fitting complete for {self.n_components} topics.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.lda_model is None:\n",
        "            raise RuntimeError(\"LDA model not fitted. Call fit() first.\")\n",
        "        print(f\"Transforming data with fitted LDA model (k={self.n_components})...\")\n",
        "        topic_distribution = self.lda_model.transform(X)\n",
        "        print(\"Transformation complete.\")\n",
        "        return topic_distribution\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.lda_model\n",
        "\n",
        "# --- Custom Estimator for BERTopic Modeling ---\n",
        "class BERTopicWrapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn estimator that wraps BERTopic.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model='all-MiniLM-L6-v2', umap_args=None, hdbscan_args=None,\n",
        "                 vectorizer_args=None, nr_topics=\"auto\", calculate_probabilities=True, **bertopic_kwargs):\n",
        "        if not BERTOPIC_AVAILABLE:\n",
        "            raise ImportError(\"BERTopic and its dependencies are required for BERTopicWrapper.\")\n",
        "\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.umap_args = umap_args if umap_args is not None else {}\n",
        "        self.hdbscan_args = hdbscan_args if hdbscan_args is not None else {}\n",
        "        self.vectorizer_args = vectorizer_args if vectorizer_args is not None else {}\n",
        "        self.nr_topics = nr_topics\n",
        "        self.calculate_probabilities = calculate_probabilities\n",
        "        self.bertopic_kwargs = bertopic_kwargs\n",
        "        self.bertopic_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"\\nStarting Phase 3: Topic Modeling (BERTopic)...\")\n",
        "\n",
        "        # Initialize UMAP and HDBSCAN models\n",
        "        umap_model = UMAP(**self.umap_args)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(\n",
        "            min_cluster_size=10,  # Default, can be overridden by hdbscan_args\n",
        "            metric='euclidean',\n",
        "            cluster_selection_method='eom',\n",
        "            prediction_data=True, # Required for transform to assign topics to new data\n",
        "            **self.hdbscan_args\n",
        "        )\n",
        "        # Initialize CountVectorizer for BERTopic (can be customized)\n",
        "        vectorizer_model = TfidfVectorizer(\n",
        "            min_df=10,  # Default, can be overridden by vectorizer_args\n",
        "            ngram_range=(1, 3),\n",
        "            **self.vectorizer_args\n",
        "        )\n",
        "\n",
        "        # Initialize SentenceTransformer\n",
        "        embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        self.bertopic_model = BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            nr_topics=self.nr_topics,\n",
        "            calculate_probabilities=self.calculate_probabilities,\n",
        "            **self.bertopic_kwargs\n",
        "        )\n",
        "\n",
        "        # X is expected to be a pandas Series of preprocessed text\n",
        "        self.topics, self.probs = self.bertopic_model.fit_transform(X.tolist())\n",
        "        print(\"BERTopic model fitting complete.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.bertopic_model is None:\n",
        "            raise RuntimeError(\"BERTopic model not fitted. Call fit() first.\")\n",
        "        print(\"Transforming data with fitted BERTopic model...\")\n",
        "        topics, probs = self.bertopic_model.transform(X.tolist())\n",
        "        print(\"Transformation complete.\")\n",
        "        return topics # Return topic assignments\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.bertopic_model\n",
        "\n",
        "# --- Utility function to display topics (adapted for both LDA and BERTopic) ---\n",
        "def display_topics(model, vectorizer=None, no_top_words=10, file=None, model_type='lda'):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic.\n",
        "    Args:\n",
        "        model: The fitted topic model (LDA or BERTopic).\n",
        "        vectorizer (TfidfVectorizer, optional): The fitted TF-IDF vectorizer (for LDA).\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "        model_type (str): 'lda' or 'bertopic' to specify model type for appropriate display.\n",
        "    \"\"\"\n",
        "    if model_type == 'lda':\n",
        "        if vectorizer is None:\n",
        "            raise ValueError(\"Vectorizer must be provided for LDA topic display.\")\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            topic_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "            output_str = f\"Topic {topic_idx + 1}:\\n{topic_words}\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "    elif model_type == 'bertopic':\n",
        "        if not BERTOPIC_AVAILABLE:\n",
        "            print(\"BERTopic not available, cannot display BERTopic topics.\")\n",
        "            return\n",
        "\n",
        "        topic_info = model.get_topic_info()\n",
        "        output_str = \"\\nBERTopic - Top Words per Topic:\\n\"\n",
        "        if file:\n",
        "            file.write(output_str)\n",
        "        else:\n",
        "            print(output_str)\n",
        "\n",
        "        # Iterate through all topics, excluding the noise topic (-1)\n",
        "        for topic_id in topic_info.Topic.unique():\n",
        "            if topic_id == -1: # Skip noise topic\n",
        "                continue\n",
        "            # Get the top words for the current topic\n",
        "            words = model.get_topic(topic_id)\n",
        "            if words:\n",
        "                top_words = \", \".join([word for word, _ in words[:no_top_words]])\n",
        "                topic_name = topic_info[topic_info['Topic'] == topic_id]['Name'].iloc[0]\n",
        "                output_str = f\"Topic {topic_id} ({topic_name}): {top_words}\\n\"\n",
        "                if file:\n",
        "                    file.write(output_str)\n",
        "                else:\n",
        "                    print(output_str)\n",
        "            else:\n",
        "                output_str = f\"Topic {topic_id}: No words found.\\n\"\n",
        "                if file:\n",
        "                    file.write(output_str)\n",
        "                else:\n",
        "                    print(output_str)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_type. Choose 'lda' or 'bertopic'.\")\n",
        "\n",
        "\n",
        "# --- Main Topic Modeling Pipeline Class ---\n",
        "class TopicModelingPipeline:\n",
        "    def __init__(self, model_type='lda', **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the topic modeling pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of topic model to use ('lda' or 'bertopic').\n",
        "            **kwargs: Arguments specific to the chosen model or pipeline steps.\n",
        "                      For LDA: max_df, min_df, ngram_range (for TF-IDF), n_components, max_iter, etc.\n",
        "                      For BERTopic: embedding_model, umap_args, hdbscan_args, vectorizer_args, nr_topics, etc.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.pipeline = self._build_pipeline(**kwargs)\n",
        "\n",
        "    def _build_pipeline(self, **kwargs):\n",
        "        \"\"\"Builds the scikit-learn pipeline based on the specified model_type.\"\"\"\n",
        "        preprocessor_kwargs = {\n",
        "            'stop_words': kwargs.pop('stop_words', ['a', 'the', 'is', 'and', 'of', 'to', 'in', 'for']),\n",
        "            'abbreviations': kwargs.pop('abbreviations', {'ai': 'artificial intelligence', 'ml': 'machine learning'})\n",
        "        }\n",
        "\n",
        "        pipeline_steps = [\n",
        "            ('preprocessor', TextPreprocessor(**preprocessor_kwargs))\n",
        "        ]\n",
        "\n",
        "        if self.model_type == 'lda':\n",
        "            tfidf_kwargs = {\n",
        "                'max_df': kwargs.pop('max_df', 0.95),\n",
        "                'min_df': kwargs.pop('min_df', 2), # Adjusted for dummy data\n",
        "                'ngram_range': kwargs.pop('ngram_range', (1, 3))\n",
        "            }\n",
        "            lda_kwargs = {\n",
        "                'n_components': kwargs.pop('n_components', 5), # Default LDA topics\n",
        "                'random_state': kwargs.pop('random_state', 42),\n",
        "                'max_iter': kwargs.pop('max_iter', 10),\n",
        "                'learning_method': kwargs.pop('learning_method', 'online'),\n",
        "                'learning_decay': kwargs.pop('learning_decay', 0.7),\n",
        "                'n_jobs': kwargs.pop('n_jobs', -1),\n",
        "                'evaluate_every': kwargs.pop('evaluate_every', 10)\n",
        "            }\n",
        "            pipeline_steps.append(('tfidf_vectorizer', TfidfVectorizer(**tfidf_kwargs)))\n",
        "            pipeline_steps.append(('topic_modeler', LDATopicModeler(**lda_kwargs)))\n",
        "        elif self.model_type == 'bertopic':\n",
        "            if not BERTOPIC_AVAILABLE:\n",
        "                raise ImportError(\"BERTopic and its dependencies must be installed to use model_type='bertopic'.\")\n",
        "\n",
        "            bertopic_kwargs = {\n",
        "                'embedding_model': kwargs.pop('embedding_model', 'all-MiniLM-L6-v2'),\n",
        "                'umap_args': kwargs.pop('umap_args', {}),\n",
        "                'hdbscan_args': kwargs.pop('hdbscan_args', {}),\n",
        "                'vectorizer_args': kwargs.pop('vectorizer_args', {}),\n",
        "                'nr_topics': kwargs.pop('nr_topics', \"auto\"),\n",
        "                'calculate_probabilities': kwargs.pop('calculate_probabilities', True),\n",
        "                **kwargs # Pass any remaining kwargs directly to BERTopicWrapper\n",
        "            }\n",
        "            pipeline_steps.append(('topic_modeler', BERTopicWrapper(**bertopic_kwargs)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_type: {self.model_type}. Choose 'lda' or 'bertopic'.\")\n",
        "\n",
        "        # Any remaining kwargs are ignored if not consumed by model-specific initializations\n",
        "        if kwargs:\n",
        "            print(f\"Warning: Unused keyword arguments passed to pipeline: {kwargs}\")\n",
        "\n",
        "        return Pipeline(pipeline_steps)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the entire pipeline to the input data.\"\"\"\n",
        "        print(f\"\\n--- Fitting {self.model_type.upper()} Topic Modeling Pipeline ---\")\n",
        "        self.pipeline.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforms the input data and returns topic assignments/distributions.\"\"\"\n",
        "        return self.pipeline.transform(X)\n",
        "\n",
        "    def get_topic_model(self):\n",
        "        \"\"\"Returns the underlying fitted topic model (LDA or BERTopic).\"\"\"\n",
        "        return self.pipeline.named_steps['topic_modeler'].get_model()\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\"Returns the fitted vectorizer (TF-IDF for LDA, None for BERTopic).\"\"\"\n",
        "        if self.model_type == 'lda':\n",
        "            return self.pipeline.named_steps['tfidf_vectorizer']\n",
        "        return None\n",
        "\n",
        "# --- Demonstration with Dummy Data ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy DataFrame similar to your 'grouped_gs_qna_df'\n",
        "    dummy_data = {\n",
        "        'content': [\n",
        "            \"This is a document about machine learning and artificial intelligence.\",\n",
        "            \"Natural language processing is a fascinating field.\",\n",
        "            \"Deep learning models are used in computer vision.\",\n",
        "            \"Another document discussing data science and big data analytics.\",\n",
        "            \"Quantum computing is a cutting-edge area of research.\",\n",
        "            \"This document talks about machine learning algorithms and data analysis.\",\n",
        "            \"Artificial intelligence applications in healthcare are growing.\",\n",
        "            \"Big data analytics helps in making informed decisions.\",\n",
        "            \"Exploring the world of natural language generation.\",\n",
        "            \"A comprehensive guide to computer vision techniques.\"\n",
        "        ],\n",
        "        'other_column': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    }\n",
        "    # grouped_gs_qna_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "    output_dir = \"data/temp/leslie_topic_modelling_fine_tuning\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    no_top_words = 10\n",
        "\n",
        "    # --- Run with LDA ---\n",
        "    print(\"\\n\\n##################################\")\n",
        "    print(\"### Running Topic Modeling with LDA ###\")\n",
        "    print(\"##################################\")\n",
        "\n",
        "    lda_pipeline_instance = TopicModelingPipeline(\n",
        "        model_type='lda',\n",
        "        n_components=5, # Number of topics for LDA\n",
        "        min_df=2 # Adjusted min_df for dummy data\n",
        "    )\n",
        "    lda_pipeline_instance.fit(grouped_gs_qna_df['content'])\n",
        "\n",
        "    lda_model = lda_pipeline_instance.get_topic_model()\n",
        "    tfidf_vectorizer = lda_pipeline_instance.get_vectorizer()\n",
        "\n",
        "    output_filename_lda = f\"{output_dir}/lda_topics_k{lda_model.n_components}.txt\"\n",
        "    with open(output_filename_lda, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"--- LDA Topic Model with {lda_model.n_components} Topics ---\\n\\n\")\n",
        "        f.write(\"Interpreting Topics:\\n\")\n",
        "        display_topics(lda_model, tfidf_vectorizer, no_top_words, file=f, model_type='lda')\n",
        "    print(f\"LDA Topics saved to {output_filename_lda}\")\n",
        "\n",
        "    # Assign dominant topics for LDA\n",
        "    lda_topic_assignments = lda_pipeline_instance.transform(grouped_gs_qna_df['content'])\n",
        "    grouped_gs_qna_df[f'dominant_topic_lda_k{lda_model.n_components}'] = np.argmax(lda_topic_assignments, axis=1)\n",
        "\n",
        "    print(f\"\\nExample (LDA): Documents with their dominant topic (first 5 rows) for k={lda_model.n_components}:\")\n",
        "    for i in range(min(5, len(grouped_gs_qna_df))):\n",
        "        original_content = grouped_gs_qna_df.loc[i, 'content']\n",
        "        truncated_content = (original_content[:100] + '...') if len(original_content) > 100 else original_content\n",
        "        dominant_topic = grouped_gs_qna_df.loc[i, f'dominant_topic_lda_k{lda_model.n_components}']\n",
        "        print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "    print(\"\\nUpdated DataFrame (LDA results - first 5 rows):\")\n",
        "    print(grouped_gs_qna_df.head())\n",
        "\n",
        "\n",
        "    # --- Run with BERTopic ---\n",
        "    if BERTOPIC_AVAILABLE:\n",
        "        print(\"\\n\\n#####################################\")\n",
        "        print(\"### Running Topic Modeling with BERTopic ###\")\n",
        "        print(\"#####################################\")\n",
        "\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let BERTopic determine number of topics\n",
        "            calculate_probabilities=True,\n",
        "            # You can pass BERTopic specific arguments here:\n",
        "            # umap_args={'n_neighbors': 15, 'n_components': 5},\n",
        "            # hdbscan_args={'min_cluster_size': 10},\n",
        "            # vectorizer_args={'min_df': 5}\n",
        "        )\n",
        "        bertopic_pipeline_instance.fit(grouped_gs_qna_df['content'])\n",
        "\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        output_filename_bertopic = f\"{output_dir}/bertopic_topics.txt\"\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"--- BERTopic Model ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f, model_type='bertopic')\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        # Assign dominant topics for BERTopic\n",
        "        bertopic_topic_assignments = bertopic_pipeline_instance.transform(grouped_gs_qna_df['content'])\n",
        "        grouped_gs_qna_df['dominant_topic_bertopic'] = bertopic_topic_assignments\n",
        "\n",
        "        print(\"\\nExample (BERTopic): Documents with their dominant topic (first 5 rows):\")\n",
        "        for i in range(min(5, len(grouped_gs_qna_df))):\n",
        "            original_content = grouped_gs_qna_df.loc[i, 'content']\n",
        "            truncated_content = (original_content[:100] + '...') if len(original_content) > 100 else original_content\n",
        "            dominant_topic = grouped_gs_qna_df.loc[i, 'dominant_topic_bertopic']\n",
        "            print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "        print(\"\\nUpdated DataFrame (BERTopic results - first 5 rows):\")\n",
        "        print(grouped_gs_qna_df.head())\n",
        "    else:\n",
        "        print(\"\\nSkipping BERTopic demonstration as required libraries are not installed.\")\n",
        "\n",
        "    print(\"\\nOverall processing complete. Check the 'data/temp/leslie_topic_modelling_fine_tuning' directory for output files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9d8da6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def vectorize_text(data_series, max_df=0.95, min_df=5, stop_words=None, ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Performs TF-IDF vectorization on a given text series.\n",
        "    Args:\n",
        "        data_series (pd.Series): A pandas Series containing preprocessed text content.\n",
        "        max_df (float): When building the vocabulary ignore terms that have a document\n",
        "                        frequency strictly higher than the given threshold (corpus-specific stop words).\n",
        "        min_df (int): When building the vocabulary ignore terms that have a document\n",
        "                      frequency strictly lower than the given threshold.\n",
        "        stop_words (str or list, optional): 'english' for a built-in stop word list,\n",
        "                                            a list of custom stop words, or None.\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "            - vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 3: Vectorization (TF-IDF)...\")\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words, ngram_range=ngram_range)\n",
        "    dtm = vectorizer.fit_transform(data_series)\n",
        "    print(f\"TF-IDF Vectorization complete. Document-Term Matrix shape: {dtm.shape}\")\n",
        "    return dtm, vectorizer\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic from an LDA model.\n",
        "    Args:\n",
        "        model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        feature_names (list): List of feature names (words) from the vectorizer.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "                                      Otherwise, they will be printed to the console.\n",
        "    \"\"\"\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        if file:\n",
        "            file.write(f\"\\nTopic {topic_idx + 1}:\\n\")\n",
        "            file.write(f\"{topic_words}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "            print(topic_words)\n",
        "\n",
        "def run_lda_topic_modeling(dtm, vectorizer, num_topics_range, output_dir=\"data/temp/leslie_topic_modelling_fine_tuning\", no_top_words=10, df_to_update=None):\n",
        "    \"\"\"\n",
        "    Performs LDA topic modeling for a range of topic numbers and saves the results.\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix from TF-IDF vectorization.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics_range (list): A list of integers representing the number of topics to experiment with.\n",
        "        output_dir (str): Directory to save the topic modeling output files.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        df_to_update (pd.DataFrame, optional): The DataFrame to which dominant topics will be\n",
        "                                                assigned. If None, dominant topics won't be added.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 4: Topic Modeling (LDA) for a range of topics...\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for num_topics in num_topics_range:\n",
        "        output_filename = f\"{output_dir}/lda_topics_k{num_topics}.txt\"\n",
        "        print(f\"\\n--- Processing with {num_topics} topics. Outputting to {output_filename} ---\")\n",
        "\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- LDA Topic Model with {num_topics} Topics ---\\n\\n\")\n",
        "\n",
        "            lda = LatentDirichletAllocation(n_components=num_topics,\n",
        "                                            max_iter=10,\n",
        "                                            learning_method='online',\n",
        "                                            learning_decay=0.7,\n",
        "                                            random_state=42,\n",
        "                                            n_jobs=-1,\n",
        "                                            evaluate_every=10)\n",
        "\n",
        "            lda.fit(dtm)\n",
        "            f.write(f\"LDA model fitting complete for {num_topics} topics.\\n\\n\")\n",
        "\n",
        "            f.write(\"\\nInterpreting Topics:\\n\")\n",
        "            display_topics(lda, feature_names, no_top_words, file=f)\n",
        "\n",
        "            if df_to_update is not None:\n",
        "                topic_distribution = lda.transform(dtm)\n",
        "                df_to_update[f'dominant_topic_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "                print(f\"\\nExample: Documents with their dominant topic (first 5 rows) for k={num_topics}:\")\n",
        "                for i in range(min(5, len(df_to_update))):\n",
        "                    original_content = df_to_update.loc[i, 'content']\n",
        "                    truncated_content = (original_content[:200] + '...') if len(original_content) > 200 else original_content\n",
        "                    dominant_topic = df_to_update.loc[i, f'dominant_topic_k{num_topics}']\n",
        "                    print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "\n",
        "grouped_gs_qna_df['preprocessed_content'] = grouped_gs_qna_df['content'].apply(lambda x: preprocess_text(x, all_stop_words, abbreviations))\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'], ngram_range=(1, 3))\n",
        "\n",
        "# num_topics_range = [8, 9, 10]\n",
        "# num_topics_range = [2, 5, 8, 11, 14, 17, 20, 23]\n",
        "num_topics_range = [5, 8, 11, 14]\n",
        "run_lda_topic_modeling(dtm_matrix, tfidf_vectorizer, num_topics_range, df_to_update=grouped_gs_qna_df, )\n",
        "\n",
        "print(\"\\nProcessing complete. Check the 'data/temp/leslie_topic_modelling_fine_tuning' directory for output files.\")\n",
        "print(\"\\nUpdated DataFrame with dominant topics (first 5 rows):\")\n",
        "print(grouped_gs_qna_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583eab95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Conceptual - requires FinBERT model loading and appropriate tokenization)\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def get_finbert_embeddings(texts):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "    model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the [CLS] token embedding as the sentence embedding\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "def run_finbert_topic_clustering(df, num_topics_range, output_dir=\"data/temp/finbert_topic_clustering\"):\n",
        "    print(\"\\nStarting Topic Clustering using FinBERT embeddings...\")\n",
        "\n",
        "    # 1. Get FinBERT embeddings for your documents\n",
        "    document_embeddings = get_finbert_embeddings(df['content'].tolist())\n",
        "    print(f\"Generated {document_embeddings.shape[0]} document embeddings of size {document_embeddings.shape[1]}.\")\n",
        "\n",
        "    for num_topics in num_topics_range:\n",
        "        print(f\"\\n--- Processing with {num_topics} clusters (topics) using FinBERT embeddings ---\")\n",
        "        kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10) # n_init for stability\n",
        "        kmeans.fit(document_embeddings)\n",
        "        df[f'finbert_dominant_topic_k{num_topics}'] = kmeans.labels_\n",
        "\n",
        "        # 2. Interpret topics (example: finding top words per cluster using TF-IDF on cluster documents)\n",
        "        print(f\"\\nInterpreting Clusters for k={num_topics}:\")\n",
        "        for i in range(num_topics):\n",
        "            cluster_docs = df[df[f'finbert_dominant_topic_k{num_topics}'] == i]['content']\n",
        "            if len(cluster_docs) > 0:\n",
        "                # Use TF-IDF to find important words in this cluster\n",
        "                tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "                tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_docs)\n",
        "                feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "                sums_tfidf = tfidf_matrix.sum(axis=0)\n",
        "                # Get top words\n",
        "                top_words_indices = sums_tfidf.argsort()[0, ::-1][:10]\n",
        "                top_words = [feature_names[idx] for idx in top_words_indices.tolist()[0]]\n",
        "                print(f\"Cluster {i} (Top Words): {', '.join(top_words)}\")\n",
        "            else:\n",
        "                print(f\"Cluster {i}: No documents in this cluster.\")\n",
        "\n",
        "        print(f\"\\nExample: Documents with their dominant topic (first 5 rows) for k={num_topics}:\")\n",
        "        for i in range(min(5, len(df))):\n",
        "            original_content = df.loc[i, 'content']\n",
        "            truncated_content = (original_content[:200] + '...') if len(original_content) > 200 else original_content\n",
        "            dominant_topic = df.loc[i, f'finbert_dominant_topic_k{num_topics}']\n",
        "            print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# To run this, you would need to:\n",
        "# 1. Install transformers and torch: pip install transformers torch scikit-learn pandas\n",
        "# 2. Add the actual run_finbert_topic_clustering call with your dtm and df_to_update\n",
        "# For example: df_to_update = run_finbert_topic_clustering(df_to_update.copy(), num_topics_range=[2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9889a4cc",
      "metadata": {},
      "source": [
        "Conclusion and Recommendation\n",
        "The addition of stopwords has certainly helped in some areas, making certain topics clearer. However, some conversational noise still persists, especially words related to the Q&A format or general conversational patterns. The term \"Apple\" continues to be grouped with \"deposit\" in some k values, which is still a bit puzzling without specific context.\n",
        "\n",
        "Based on this comprehensive analysis, the most sensible k value is a trade-off between granularity, coherence, and minimizing \"junk\" topics.\n",
        "\n",
        "k=8: Offers good clarity for key themes (Credit Card, Headcount/Severance, Asset/Fundraising), but is still quite broad and has some remaining conversational noise.\n",
        "k=9: Introduces very strong \"GSIB\" and \"Wealth Management\" topics.\n",
        "k=10: Shows strong \"Investment/Platform\" and \"Fundraising\" themes.\n",
        "k=11: This k value demonstrates the best balance in this new set of runs.\n",
        "It produces several very distinct and interpretable financial/business topics: \"Wealth Management/European Footprint\" , \"Severance/Headcount/Capital\" , \"Credit Card/Consumer\" , \"GSIB/Allocation\" , \"Bank/Acquisition/Advisory\" , \"FICC/Equity/Commodity\" , and \"Deposit/Capital/Market/Exposure\".\n",
        "\n",
        "Crucially, the \"Apple\" anomaly is not present in the top words of any topic for k=11, suggesting a cleaner separation of terms.\n",
        "While some conversational noise is still present (Topics 2, 4, 9 in k=11), the quality of the interpretable topics is high.\n",
        "k=12, k=13, k=14: Beyond k=11, the topics generally become more fragmented, or reintroduce the \"Apple\" anomaly, and the number of less coherent/conversational topics increases, making overall interpretation more challenging. For example, k=12 recombines \"funding/deposits\" with \"severance/headcount\", which is less ideal.\n",
        "Therefore, my strongest recommendation is k=11. It provides a good level of detail for key financial aspects of Goldman Sachs' earnings calls while offering significantly improved topic coherence and distinctiveness, and effectively mitigating some of the persistent noise terms seen in other k values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaaa03d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Assuming this is already imported and used\n",
        "import pandas as pd # Assuming grouped_gs_qna_df is a pandas DataFrame\n",
        "\n",
        "def get_lda_topic_model_and_data(dtm, vectorizer, num_topics=11, no_top_words=10, **lda_params):\n",
        "    \"\"\"\n",
        "    Initializes and fits an LDA model, then extracts top words and their weights for each topic.\n",
        "\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics (int): The number of topics for the LDA model.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        **lda_params: Arbitrary keyword arguments to pass to the LatentDirichletAllocation constructor.\n",
        "                      Common parameters include max_iter, learning_method, learning_decay,\n",
        "                      random_state, n_jobs, evaluate_every.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "            - topics_data (list): A list of dictionaries, one for each topic,\n",
        "                                  containing 'topic_idx', 'top_words', and 'word_weights'.\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting LDA model fitting with {num_topics} topics...\")\n",
        "\n",
        "    default_lda_params = {\n",
        "        'max_iter': 10,\n",
        "        'learning_method': 'online',\n",
        "        'learning_decay': 0.7,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'evaluate_every': 10\n",
        "    }\n",
        "    effective_lda_params = {**default_lda_params, **lda_params}\n",
        "\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, **effective_lda_params)\n",
        "    lda_model.fit(dtm)\n",
        "    print(\"LDA model fitting complete.\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics_data = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words_indices = topic.argsort()[:-no_top_words - 1:-1]\n",
        "        top_words = [feature_names[i] for i in top_words_indices]\n",
        "        word_weights = [topic[i] for i in top_words_indices]\n",
        "\n",
        "        topics_data.append({\n",
        "            'topic_idx': topic_idx,\n",
        "            'top_words': top_words,\n",
        "            'word_weights': word_weights\n",
        "        })\n",
        "    return lda_model, topics_data\n",
        "\n",
        "def assign_dominant_topics_to_dataframe(df, lda_model, dtm, column_prefix='dominant_topic'):\n",
        "    \"\"\"\n",
        "    Assigns the dominant topic to each document in a DataFrame based on an LDA model.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to which dominant topics will be assigned.\n",
        "        lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix used for fitting the LDA model.\n",
        "        column_prefix (str): Prefix for the new column name (e.g., 'dominant_topic_k11').\n",
        "    \"\"\"\n",
        "    topic_distribution = lda_model.transform(dtm)\n",
        "    num_topics = lda_model.n_components\n",
        "    df[f'{column_prefix}_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "    print(f\"Dominant topics assigned to DataFrame under column '{column_prefix}_k{num_topics}'.\")\n",
        "\n",
        "\n",
        "# Phase 3: Vectorization (using the previous function)\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'])\n",
        "\n",
        "# Phase 4: Topic Modeling (using the new improved function)\n",
        "chosen_num_topics = 11 # From previous analysis, or experiment here\n",
        "top_words_count = 10\n",
        "\n",
        "# You can pass custom LDA parameters if needed, e.g., max_iter=20, learning_decay=0.9\n",
        "lda_model_fitted, topics_data_extracted = get_lda_topic_model_and_data(\n",
        "    dtm=dtm_matrix,\n",
        "    vectorizer=tfidf_vectorizer,\n",
        "    num_topics=chosen_num_topics,\n",
        "    no_top_words=top_words_count,\n",
        "    random_state=42 # Explicitly passing for clarity, though it's a default\n",
        ")\n",
        "\n",
        "# Print extracted topics data\n",
        "print(\"\\n--- Extracted Topics Data ---\")\n",
        "for topic in topics_data_extracted:\n",
        "    print(f\"Topic {topic['topic_idx'] + 1}: {' '.join(topic['top_words'])}\")\n",
        "\n",
        "# Assign dominant topics back to the DataFrame\n",
        "assign_dominant_topics_to_dataframe(grouped_gs_qna_df, lda_model_fitted, dtm_matrix)\n",
        "\n",
        "print(\"\\nProcessing complete. The DataFrame now contains dominant topics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a2cca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated DataFrame with dominant topics (first 5 rows)\n",
        "grouped_gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5525b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for topic in topics_data_extracted:\n",
        "    print(f\"\\nTopic {topic['topic_idx'] + 1}:\")\n",
        "    for word, weight in zip(topic['top_words'], topic['word_weights']):\n",
        "        print(f\"{word} (weight: {weight:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8108189b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "topic_labels_map = {\n",
        "    0: \"Strategic Positioning & Platform\",\n",
        "    1: \"Wealth Management & European Markets\",\n",
        "    2: \"Alternative Investments & Fee Income\",\n",
        "    3: \"Headcount & Workforce Management\",\n",
        "    4: \"Consumer Credit & Card Performance\",\n",
        "    5: \"FICC & Equity Trading Performance\",\n",
        "    6: \"Regulatory Capital & Institutional Allocation\",\n",
        "    7: \"Client-Centric Growth & Solutions\",\n",
        "    8: \"M&A, Valuations & Advisory\",\n",
        "    9: \"FICC & Market Environment\",\n",
        "    10: \"Deposits, Capital & Funding\"\n",
        "}\n",
        "\n",
        "# Assign labels to the topics_data\n",
        "for topic_info in topics_data:\n",
        "    topic_info['label'] = topic_labels_map.get(topic_info['topic_idx'], f\"Unlabeled Topic {topic_info['topic_idx']}\")\n",
        "\n",
        "print(\"\\n--- Topics with assigned labels and top words ---\")\n",
        "for topic_info in topics_data:\n",
        "    print(f\"Topic {topic_info['topic_idx'] + 1}: {topic_info['label']}\")\n",
        "    print(f\"  Top Words: {' '.join(topic_info['top_words'])}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- Visual Display of Topics ---\n",
        "print(\"\\n--- Generating visual display of topics ---\")\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = (num_topics + n_cols - 1) // n_cols\n",
        "plt.figure(figsize=(n_cols * 6, n_rows * 4), dpi=100)\n",
        "\n",
        "for i, topic_info in enumerate(topics_data):\n",
        "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "    df_plot = pd.DataFrame({\n",
        "        'word': topic_info['top_words'],\n",
        "        'weight': topic_info['word_weights']\n",
        "    })\n",
        "    df_plot = df_plot.sort_values(by='weight', ascending=True)\n",
        "    sns.barplot(x='weight', y='word', data=df_plot, palette='magma', ax=ax)\n",
        "    ax.set_title(f\"{topic_info['label']}\", fontsize=11, fontweight='bold', pad=10)\n",
        "    ax.set_xlabel(\"Word Importance (Weight)\", fontsize=9)\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "    sns.despine(ax=ax, top=True, right=True, left=False, bottom=False)\n",
        "    ax.tick_params(axis='y', length=0)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.suptitle(\n",
        "    f\"Top Words for {num_topics} Topics in Goldman Sachs Earnings Calls (Q&A Section)\",\n",
        "    y=1.00, fontsize=16, fontweight='bold'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisual display generated. Please review the plots and verify the topic labels.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f760200",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959f9349",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='year',\n",
        "    hue='quarter',\n",
        "    palette='tab10'\n",
        ")\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Distribution of Documents by Year and Quarter\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Quarter')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Now, for dominant topics over both year and quarter:\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='dominant_topic_k11',\n",
        "    hue='year',\n",
        "    palette='tab10'\n",
        ")\n",
        "topic_labels = [topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())]\n",
        "plt.xticks(ticks=range(len(topic_labels)), labels=topic_labels, rotation=45, ha='right')\n",
        "plt.xlabel('Dominant Topic (k=11)')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Dominant Topics (k=11) by Year\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Year')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e53850b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "years = sorted(grouped_gs_qna_df['year'].unique())\n",
        "n_years = len(years)\n",
        "\n",
        "fig, axes = plt.subplots(n_years, 1, figsize=(10, 4 * n_years), sharex=True)\n",
        "\n",
        "if n_years == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette='tab10',\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    ax.legend(\n",
        "        title='Dominant Topic',\n",
        "        loc='upper right',\n",
        "        labels=[topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(data['dominant_topic_k11'].unique())]\n",
        "    )\n",
        "\n",
        "# Fix color mapping so each topic always has the same color across years\n",
        "unique_topics = sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())\n",
        "topic_palette = sns.color_palette('tab10', n_colors=len(unique_topics))\n",
        "topic_color_dict = {topic: topic_palette[i % len(topic_palette)] for i, topic in enumerate(unique_topics)}\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    # Use the same color mapping for all years\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette=topic_color_dict,\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    # Always use the same order and labels for legend\n",
        "    ordered_labels = [topic_labels_map.get(t, f\"Topic {t}\") for t in unique_topics]\n",
        "    ax.legend(handles, ordered_labels, title='Dominant Topic', loc='upper right')\n",
        "\n",
        "plt.suptitle(\n",
        "    \"Quarterly Distribution of Dominant Topics (k=11) by Year\\nGoldman Sachs Earnings Call Transcript (Q&A Section)\",\n",
        "    fontsize=16, fontweight='bold', y=1.02\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7817bea",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb15dc4f",
        "outputId": "230dcdf8-192c-4bd7-97fe-1b128a442bd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_dir = 'data/temp/'\n",
        "file_name = 'dummy_test_output_new.csv'\n",
        "dummy_pf = pd.DataFrame({'from_colab': [IS_COLAB, True, 'hello']})\n",
        "\n",
        "\n",
        "if OUTPUT_PROCESSED_FILES:\n",
        "    if IS_COLAB:\n",
        "        AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "\n",
        "        # Configure Git user (important for committing)\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {target_dir}{file_name}\n",
        "        print(f\"Added '{target_dir}{file_name}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {target_dir}{file_name}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "        print(\"Push command executed. Check output for success or prompt.\")\n",
        "    else:\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "        print(\"Processed files saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
