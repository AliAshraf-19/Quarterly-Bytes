{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"LP_topic_modelling_extended\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "e8f3820c-799c-4e8a-ac97-17781bf28b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not in a git directory\n",
            "Directory 'src' already exists. Pulling latest changes...\n",
            "/content/src\n",
            "/content\n",
            "/content/src\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.52.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.17.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.5.7)\n",
            "Requirement already satisfied: python-dev-tools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (2023.3.24)\n",
            "Requirement already satisfied: hdbscan==0.8.40 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.8.40)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.1.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.33.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.5.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic->-r requirements.txt (line 12)) (5.24.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.5.13)\n",
            "Requirement already satisfied: Sphinx<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.2.1)\n",
            "Requirement already satisfied: autoflake<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.8)\n",
            "Requirement already satisfied: black<24,>=23 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (23.12.1)\n",
            "Requirement already satisfied: coverage[toml]<8,>=7 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (7.9.1)\n",
            "Requirement already satisfied: darglint<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: dlint<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: doc8<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.2)\n",
            "Requirement already satisfied: docformatter<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.7)\n",
            "Requirement already satisfied: flake8<6,>=5 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (5.0.4)\n",
            "Requirement already satisfied: flake8-2020<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: flake8-aaa<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.17.0)\n",
            "Requirement already satisfied: flake8-annotations<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.1.1)\n",
            "Requirement already satisfied: flake8-annotations-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.0)\n",
            "Requirement already satisfied: flake8-annotations-coverage<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.6)\n",
            "Requirement already satisfied: flake8-bandit<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.1)\n",
            "Requirement already satisfied: flake8-black<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.6)\n",
            "Requirement already satisfied: flake8-blind-except<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.1)\n",
            "Requirement already satisfied: flake8-breakpoint<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: flake8-broken-line<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: flake8-bugbear<24,>=23 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (23.3.12)\n",
            "Requirement already satisfied: flake8-builtins<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.3)\n",
            "Requirement already satisfied: flake8-class-attributes-order<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-coding<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.3.2)\n",
            "Requirement already satisfied: flake8-cognitive-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.0)\n",
            "Requirement already satisfied: flake8-comments<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: flake8-comprehensions<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.16.0)\n",
            "Requirement already satisfied: flake8-debugger<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.2)\n",
            "Requirement already satisfied: flake8-django<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4)\n",
            "Requirement already satisfied: flake8-docstrings<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.0)\n",
            "Requirement already satisfied: flake8-encodings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.5.1)\n",
            "Requirement already satisfied: flake8-eradicate<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.0)\n",
            "Requirement already satisfied: flake8-executable<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: flake8-expression-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.11)\n",
            "Requirement already satisfied: flake8-fastapi<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: flake8-fixme<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.1)\n",
            "Requirement already satisfied: flake8-functions<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.8)\n",
            "Requirement already satisfied: flake8-functions-names<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: flake8-future-annotations<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.5)\n",
            "Requirement already satisfied: flake8-isort<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.1.2)\n",
            "Requirement already satisfied: flake8-literal<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: flake8-logging-format<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: flake8-markdown<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: flake8-mutable<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: flake8-no-pep420<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.8.0)\n",
            "Requirement already satisfied: flake8-noqa<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: flake8-pie<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: flake8-pyi<23,>=22 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (22.11.0)\n",
            "Requirement already satisfied: flake8-pylint<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.1)\n",
            "Requirement already satisfied: flake8-pytest-style<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.2)\n",
            "Requirement already satisfied: flake8-quotes<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.4.0)\n",
            "Requirement already satisfied: flake8-rst-docstrings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.1)\n",
            "Requirement already satisfied: flake8-secure-coding-standard<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: flake8-simplify<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.22.0)\n",
            "Requirement already satisfied: flake8-string-format<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-tidy-imports<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.11.0)\n",
            "Requirement already satisfied: flake8-typing-imports<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: flake8-use-fstring<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4)\n",
            "Requirement already satisfied: flake8-use-pathlib<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-useless-assert<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.4)\n",
            "Requirement already satisfied: flake8-variables-names<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.6)\n",
            "Requirement already satisfied: flake8-warnings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: jupyterlab-flake8<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.7.1)\n",
            "Requirement already satisfied: pandas-vet<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.3)\n",
            "Requirement already satisfied: pep8-naming<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.15.1)\n",
            "Requirement already satisfied: pip<23,>=22 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (22.3.1)\n",
            "Requirement already satisfied: pybetter<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.1)\n",
            "Requirement already satisfied: pycln<3,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: pycodestyle<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.9.1)\n",
            "Requirement already satisfied: pydocstyle<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.3.0)\n",
            "Requirement already satisfied: pytest<8,>=7 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (7.4.4)\n",
            "Requirement already satisfied: pytest-cov<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.0)\n",
            "Requirement already satisfied: pytest-sugar<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.9.7)\n",
            "Requirement already satisfied: pyupgrade<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.20.0)\n",
            "Requirement already satisfied: removestar<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.2)\n",
            "Requirement already satisfied: ssort<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: tox<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.27.0)\n",
            "Requirement already satisfied: tox-travis<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.12)\n",
            "Requirement already satisfied: pyflakes<3,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autoflake<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (4.3.8)\n",
            "Requirement already satisfied: docutils<=0.21.2,>=0.19 in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.19)\n",
            "Requirement already satisfied: restructuredtext-lint>=0.7 in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: stevedore in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (5.4.1)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.19.1)\n",
            "Requirement already satisfied: charset_normalizer<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from docformatter<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (3.4.2)\n",
            "Requirement already satisfied: untokenize<0.2.0,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from docformatter<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.1.1)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from flake8<6,>=5->python-dev-tools->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: asttokens>=2 in /usr/local/lib/python3.11/dist-packages (from flake8-aaa<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: attrs>=21.4 in /usr/local/lib/python3.11/dist-packages (from flake8-annotations<4,>=3->python-dev-tools->-r requirements.txt (line 14)) (25.3.0)\n",
            "Requirement already satisfied: bandit>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from flake8-bandit<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (1.8.5)\n",
            "Requirement already satisfied: flake8-plugin-utils<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from flake8-breakpoint<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.3.3)\n",
            "Requirement already satisfied: cognitive-complexity in /usr/local/lib/python3.11/dist-packages (from flake8-cognitive-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: astroid<3.0.0,>=2.15.2 in /usr/local/lib/python3.11/dist-packages (from flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.15.8)\n",
            "Requirement already satisfied: astatine>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.3.3)\n",
            "Requirement already satisfied: domdf-python-tools>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: flake8-helper>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: eradicate<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from flake8-eradicate<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.3.0)\n",
            "Requirement already satisfied: astpretty in /usr/local/lib/python3.11/dist-packages (from flake8-expression-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: fastapi>=0.65.1 in /usr/local/lib/python3.11/dist-packages (from flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.115.12)\n",
            "Requirement already satisfied: mr-proper in /usr/local/lib/python3.11/dist-packages (from flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.0.7)\n",
            "Requirement already satisfied: isort<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from flake8-isort<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (5.13.2)\n",
            "Requirement already satisfied: pylint in /usr/local/lib/python3.11/dist-packages (from flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (2.17.7)\n",
            "Requirement already satisfied: astor>=0.1 in /usr/local/lib/python3.11/dist-packages (from flake8-simplify<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 10)) (1.1.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn->-r requirements.txt (line 13)) (0.43.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 12)) (9.1.2)\n",
            "Requirement already satisfied: hypothesmith<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.1.9)\n",
            "Requirement already satisfied: libcst<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.4.10)\n",
            "Requirement already satisfied: pyemojify<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: tomlkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from pycln<3,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.13.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pydocstyle<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (3.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: termcolor>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytest-sugar<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: tokenize-rt>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from pyupgrade<4,>=3->python-dev-tools->-r requirements.txt (line 14)) (6.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2025.6.15)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.17.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.5)\n",
            "Requirement already satisfied: cachetools>=5.5.1 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (5.5.2)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (5.2.0)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: pyproject-api>=1.8 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.31 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (20.31.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (7.1.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from astroid<3.0.0,>=2.15.2->flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.11.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from astroid<3.0.0,>=2.15.2->flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.17.2)\n",
            "Requirement already satisfied: natsort>=7.0.1 in /usr/local/lib/python3.11/dist-packages (from domdf-python-tools>=2.8.1->flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (8.4.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.46.2)\n",
            "Requirement already satisfied: hypothesis>=5.41.0 in /usr/local/lib/python3.11/dist-packages (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (6.135.14)\n",
            "Requirement already satisfied: lark-parser>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from libcst<0.5.0,>=0.4.1->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore->doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.31->tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (0.3.9)\n",
            "Requirement already satisfied: stdlib-list>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mr-proper->flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.11.1)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from pylint->flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.3.7)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis>=5.41.0->hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Install Imports"
      ],
      "metadata": {
        "id": "ryvB1Hpx1C3V"
      },
      "id": "ryvB1Hpx1C3V"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4EyBg8j1FgP",
        "outputId": "ef8ce3b8-9bf7-415b-b78d-a6f18ee7ffdc"
      },
      "id": "x4EyBg8j1FgP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "02d72bb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02d72bb0",
        "outputId": "622c5cf4-3a88-4c39-b614-e307c5de0d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6147c7a",
      "metadata": {
        "id": "c6147c7a"
      },
      "source": [
        "## Local Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2963e1f",
      "metadata": {
        "id": "f2963e1f"
      },
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file, read_list_from_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2569d135",
      "metadata": {
        "id": "2569d135"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a60bdaad",
      "metadata": {
        "id": "a60bdaad"
      },
      "outputs": [],
      "source": [
        "def group_df(df, group_by_columns, agg_column='content'):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by specified columns and aggregates the content column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to group\n",
        "    - group_by_columns: List of columns to group by\n",
        "    - agg_column: Column to aggregate (default is 'content')\n",
        "\n",
        "    Returns:\n",
        "    - Grouped DataFrame with aggregated content\n",
        "    \"\"\"\n",
        "    return df.groupby(group_by_columns, as_index=False).agg({agg_column: ' '.join})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4f87c1ff",
      "metadata": {
        "id": "4f87c1ff"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')\n",
        "\n",
        "\n",
        "# Goldman Sachs\n",
        "# grouped_gs_discussion_df = group_df(gs_discussion_df, ['quarter', 'year'])\n",
        "grouped_gs_qna_df = group_df(gs_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n",
        "\n",
        "# JP Morgan\n",
        "# grouped_jp_discussion_df = group_df(jp_discussion_df, ['quarter', 'year'])\n",
        "grouped_jp_qna_df = group_df(jp_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Global variable to store the spaCy model, to prevent re-loading multiple times\n",
        "_nlp_model = None\n",
        "\n",
        "def get_spacy_model():\n",
        "    \"\"\"\n",
        "    Loads and returns the spaCy 'en_core_web_sm' model.\n",
        "    Downloads it if not already present.\n",
        "    \"\"\"\n",
        "    global _nlp_model\n",
        "    if _nlp_model is None:\n",
        "        try:\n",
        "            # Try loading without parser and NER for speed if only sentencizer is needed\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "        except OSError:\n",
        "            print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "            # This command is for direct execution in environments like Colab\n",
        "            # In a standard Python script, you might run this once before your script\n",
        "            import subprocess\n",
        "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "    return _nlp_model\n",
        "\n",
        "def split_text_into_sentence_chunks(\n",
        "    df: pd.DataFrame,\n",
        "    text_column: str,\n",
        "    sentences_per_chunk: int = 2,\n",
        "    new_column_name: str = 'content_chunked'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Splits the content of a specified text column in a DataFrame into\n",
        "    chunks of a given number of sentences, and returns a new DataFrame\n",
        "    with rows exploded by these chunks.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input Pandas DataFrame.\n",
        "    - text_column: The name of the column in the DataFrame containing the text to be split.\n",
        "    - sentences_per_chunk: The approximate number of sentences per chunk. Defaults to 2.\n",
        "    - new_column_name: The name for the new column containing the text chunks.\n",
        "                       Defaults to 'content_chunked'.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with text content split into chunks and exploded into new rows.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # Ensure spaCy model is loaded\n",
        "    nlp_model_instance = get_spacy_model()\n",
        "\n",
        "    def _split_content_into_sentences_internal(text):\n",
        "        \"\"\"\n",
        "        Internal helper to split a single text into chunks of sentences.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return [] # Return empty list for non-string input\n",
        "\n",
        "        doc = nlp_model_instance(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()] # Filter empty sentences\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            current_chunk.append(sentence)\n",
        "            # Create a chunk if we've reached the desired number of sentences\n",
        "            # or if it's the last sentence(s) of the text\n",
        "            if (i + 1) % sentences_per_chunk == 0 or i == len(sentences) - 1:\n",
        "                if current_chunk: # Only add if the chunk is not empty\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [] # Reset for the next chunk\n",
        "        return chunks\n",
        "    df_copy[new_column_name] = df_copy[text_column].apply(_split_content_into_sentences_internal)\n",
        "    exploded_df = df_copy.explode(new_column_name)\n",
        "    exploded_df = exploded_df.drop(columns=[text_column])\n",
        "    exploded_df = exploded_df.rename(columns={new_column_name: text_column})\n",
        "\n",
        "    return exploded_df\n",
        "\n",
        "processed_gs_discussion_df = split_text_into_sentence_chunks(\n",
        "    gs_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")\n",
        "\n",
        "processed_jp_discussion_df = split_text_into_sentence_chunks(\n",
        "    jp_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")"
      ],
      "metadata": {
        "id": "xmjQuyT6hJJs"
      },
      "id": "xmjQuyT6hJJs",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_jp_discussion_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "H3-z0lfGijmo",
        "outputId": "fab8251a-e2cf-4fea-d382-faa14164f8d6"
      },
      "id": "H3-z0lfGijmo",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         speaker                     role               company  year  \\\n",
              "0  Jeremy Barnum  Chief Financial Officer  JPMorgan Chase & Co.  2022   \n",
              "0  Jeremy Barnum  Chief Financial Officer  JPMorgan Chase & Co.  2022   \n",
              "\n",
              "   quarter                                            content  \n",
              "0        1  Thanks, operator. Good morning, everyone. The ...  \n",
              "0        1  These results include approximately $900 milli...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-421e0039-7128-4a7b-b59e-93f59425da78\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>year</th>\n",
              "      <th>quarter</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "      <td>Thanks, operator. Good morning, everyone. The ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "      <td>These results include approximately $900 milli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-421e0039-7128-4a7b-b59e-93f59425da78')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-421e0039-7128-4a7b-b59e-93f59425da78 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-421e0039-7128-4a7b-b59e-93f59425da78');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5cc967e6-ca65-4bd9-8a1d-58ae0c0cb162\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5cc967e6-ca65-4bd9-8a1d-58ae0c0cb162')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5cc967e6-ca65-4bd9-8a1d-58ae0c0cb162 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_jp_discussion_df",
              "summary": "{\n  \"name\": \"processed_jp_discussion_df\",\n  \"rows\": 341,\n  \"fields\": [\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Jeremy Barnum\",\n          \"Jamie Dimon\",\n          \"Jamie  Dimon\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"role\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Chairman, Chief Executive Officer\",\n          \"Chief Financial Officer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"company\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"JPMorgan Chase & Co.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2022,\n        \"max\": 2025,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 340,\n        \"samples\": [\n          \"Finally, credit costs were $316 million, driven by higher net lending activity, including in Markets,  and downgrades,\\npartially offset by improved macroeconomic variables. Then to complete our lines of business, AWM on page 6. Asset & Wealth Management reported net income of $1.4 billion with pre -tax margin\\nof 33%. For the quarter, revenue of $5.4 billion was up 9% year -on-year, driven by growth in management fees on higher average market\\nlevels and strong net inflows, investment valuation gains compared to losses in the prior -year, and higher brokerage activity, partially offset by\\ndeposit margin compression.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {
        "id": "73420e11"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23cf4579",
      "metadata": {
        "id": "23cf4579"
      },
      "outputs": [],
      "source": [
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "jp_stopwords = set(read_list_from_text_file('src/data_processing/jp_morgan_topic_modelling_stopwords.txt'))\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2504.15683\n",
        "Use FinTextSim"
      ],
      "metadata": {
        "id": "Ctb_E-tzCBU_"
      },
      "id": "Ctb_E-tzCBU_"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b95b9b4e",
      "metadata": {
        "id": "b95b9b4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "gs_stopwords = nlp.Defaults.stop_words.union(gs_stopwords)\n",
        "jp_stopwords = nlp.Defaults.stop_words.union(jp_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fb801083",
      "metadata": {
        "id": "fb801083"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "\n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "\n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# --- Custom Transformer for Text Preprocessing ---\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn transformer to apply text preprocessing.\n",
        "    It wraps the 'preprocess_text' function.\n",
        "    \"\"\"\n",
        "    def __init__(self, stop_words=None, abbreviations=None):\n",
        "        self.stop_words = stop_words\n",
        "        self.abbreviations = abbreviations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(\"Starting Phase 1: Preprocessing...\")\n",
        "        preprocessed_X = [preprocess_text(text, self.stop_words, self.abbreviations) for text in X]\n",
        "        print(\"Preprocessing complete.\")\n",
        "        return pd.Series(preprocessed_X)\n",
        "\n",
        "\n",
        "# --- Custom Estimator for BERTopic Modeling ---\n",
        "class BERTopicWrapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn estimator that wraps BERTopic.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model='all-MiniLM-L6-v2', umap_args=None, hdbscan_args=None,\n",
        "                 vectorizer_args=None, nr_topics=\"auto\", calculate_probabilities=True, **bertopic_kwargs):\n",
        "\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.umap_args = umap_args if umap_args is not None else {}\n",
        "        self.hdbscan_args = hdbscan_args if hdbscan_args is not None else {}\n",
        "        self.vectorizer_args = vectorizer_args if vectorizer_args is not None else {}\n",
        "        self.nr_topics = nr_topics\n",
        "        self.calculate_probabilities = calculate_probabilities\n",
        "        self.bertopic_kwargs = bertopic_kwargs\n",
        "        self.bertopic_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"\\nStarting Phase 3: Topic Modeling (BERTopic)...\")\n",
        "\n",
        "        # Initialize UMAP and HDBSCAN models\n",
        "        umap_model = UMAP(**self.umap_args)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(\n",
        "            # metric='euclidean',\n",
        "            # cluster_selection_method='eom',\n",
        "            # prediction_data=True, # Required for transform to assign topics to new data\n",
        "            **self.hdbscan_args\n",
        "        )\n",
        "\n",
        "        # Initialize SentenceTransformer\n",
        "        embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        default_min_df_for_bertopic_vectorizer = 1 # Changed from 10 to 1 for higher permissiveness\n",
        "\n",
        "        # Combine default vectorizer args with user-provided args\n",
        "        combined_vectorizer_args = {\n",
        "            'min_df': default_min_df_for_bertopic_vectorizer,\n",
        "            'ngram_range': (1, 3), # Common default for BERTopic's internal vectorizer\n",
        "            **self.vectorizer_args # User-provided vectorizer_args will override these defaults\n",
        "        }\n",
        "\n",
        "        vectorizer_model = TfidfVectorizer(**combined_vectorizer_args)\n",
        "\n",
        "\n",
        "        self.bertopic_model = BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            nr_topics=self.nr_topics,\n",
        "            calculate_probabilities=self.calculate_probabilities,\n",
        "            **self.bertopic_kwargs\n",
        "        )\n",
        "\n",
        "        # X is expected to be a pandas Series of preprocessed text\n",
        "        self.topics, self.probs = self.bertopic_model.fit_transform(X.tolist())\n",
        "        print(\"BERTopic model fitting complete.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.bertopic_model is None:\n",
        "            raise RuntimeError(\"BERTopic model not fitted. Call fit() first.\")\n",
        "        print(\"Transforming data with fitted BERTopic model...\")\n",
        "        topics, probs = self.bertopic_model.transform(X.tolist())\n",
        "        print(\"Transformation complete.\")\n",
        "        return topics # Return topic assignments\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.bertopic_model\n",
        "\n",
        "# --- Utility function to display topics (adapted for both LDA and BERTopic) ---\n",
        "def display_topics(model, vectorizer=None, no_top_words=10, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic.\n",
        "    Args:\n",
        "        model: The fitted topic model (LDA or BERTopic).\n",
        "        vectorizer (TfidfVectorizer, optional): The fitted TF-IDF vectorizer (for LDA).\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "        model_type (str): 'lda' or 'bertopic' to specify model type for appropriate display.\n",
        "    \"\"\"\n",
        "    topic_info = model.get_topic_info()\n",
        "    output_str = \"\\nBERTopic - Top Words per Topic:\\n\"\n",
        "    if file:\n",
        "        file.write(output_str)\n",
        "    else:\n",
        "        print(output_str)\n",
        "\n",
        "    # Iterate through all topics, excluding the noise topic (-1)\n",
        "    for topic_id in topic_info.Topic.unique():\n",
        "        if topic_id == -1: # Skip noise topic\n",
        "            continue\n",
        "        # Get the top words for the current topic\n",
        "        words = model.get_topic(topic_id)\n",
        "        if words:\n",
        "            top_words = \", \".join([word for word, _ in words[:no_top_words]])\n",
        "            topic_name = topic_info[topic_info['Topic'] == topic_id]['Name'].iloc[0]\n",
        "            output_str = f\"Topic {topic_id} ({topic_name}): {top_words}\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "        else:\n",
        "            output_str = f\"Topic {topic_id}: No words found.\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "\n",
        "\n",
        "# --- Main Topic Modeling Pipeline Class ---\n",
        "class TopicModelingPipeline:\n",
        "    def __init__(self, model_type='lda', **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the topic modeling pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of topic model to use ('lda' or 'bertopic').\n",
        "            **kwargs: Arguments specific to the chosen model or pipeline steps.\n",
        "                      For LDA: max_df, min_df, ngram_range (for TF-IDF), n_components, max_iter, etc.\n",
        "                      For BERTopic: embedding_model, umap_args, hdbscan_args, vectorizer_args, nr_topics, etc.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.pipeline = self._build_pipeline(**kwargs)\n",
        "\n",
        "    def _build_pipeline(self, **kwargs):\n",
        "        \"\"\"Builds the scikit-learn pipeline based on the specified model_type.\"\"\"\n",
        "        preprocessor_kwargs = {\n",
        "            'stop_words': kwargs.pop('stop_words', []),\n",
        "            'abbreviations': kwargs.pop('abbreviations', {})\n",
        "        }\n",
        "\n",
        "        pipeline_steps = [\n",
        "            ('preprocessor', TextPreprocessor(**preprocessor_kwargs))\n",
        "        ]\n",
        "\n",
        "        bertopic_kwargs = {\n",
        "            'embedding_model': kwargs.pop('embedding_model', 'all-MiniLM-L6-v2'),\n",
        "            'umap_args': kwargs.pop('umap_args', {}),\n",
        "            'hdbscan_args': kwargs.pop('hdbscan_args', {}),\n",
        "            'vectorizer_args': kwargs.pop('vectorizer_args', {}), # Pass custom vectorizer_args here\n",
        "            'nr_topics': kwargs.pop('nr_topics', \"auto\"),\n",
        "            'calculate_probabilities': kwargs.pop('calculate_probabilities', True),\n",
        "            **kwargs # Pass any remaining kwargs directly to BERTopicWrapper\n",
        "        }\n",
        "        pipeline_steps.append(('topic_modeler', BERTopicWrapper(**bertopic_kwargs)))\n",
        "\n",
        "        # Any remaining kwargs are ignored if not consumed by model-specific initializations\n",
        "        if kwargs:\n",
        "            print(f\"Warning: Unused keyword arguments passed to pipeline: {kwargs}\")\n",
        "\n",
        "        return Pipeline(pipeline_steps)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the entire pipeline to the input data.\"\"\"\n",
        "        print(f\"\\n--- Fitting {self.model_type.upper()} Topic Modeling Pipeline ---\")\n",
        "        self.pipeline.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforms the input data and returns topic assignments/distributions.\"\"\"\n",
        "        return self.pipeline.transform(X)\n",
        "\n",
        "    def get_topic_model(self):\n",
        "        \"\"\"Returns the underlying fitted topic model (LDA or BERTopic).\"\"\"\n",
        "        return self.pipeline.named_steps['topic_modeler'].get_model()\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\"Returns the fitted vectorizer (TF-IDF for LDA, None for BERTopic).\"\"\"\n",
        "        if self.model_type == 'lda':\n",
        "            return self.pipeline.named_steps['tfidf_vectorizer']\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goldman Sachs"
      ],
      "metadata": {
        "id": "8thgFdHAca1G"
      },
      "id": "8thgFdHAca1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "8mBJKOOwh5am"
      },
      "id": "8mBJKOOwh5am"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = gs_stopwords\n",
        "# target_df = processed_gs_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "y38SmyjzROEs"
      },
      "id": "y38SmyjzROEs",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "EwPps44wh7Go"
      },
      "id": "EwPps44wh7Go"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/qna/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = gs_stopwords\n",
        "# target_df = grouped_gs_qna_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "T4btRdxHi3Wf"
      },
      "id": "T4btRdxHi3Wf",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JP Morgan"
      ],
      "metadata": {
        "id": "SdmCQfm0cd_c"
      },
      "id": "SdmCQfm0cd_c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "BIDqnqPliEFl"
      },
      "id": "BIDqnqPliEFl"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = jp_stopwords\n",
        "target_df = processed_jp_discussion_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [5, 10], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kc5g4Ggjdl4",
        "outputId": "025bca3c-03d6-446c-f477-c2dc091e76a4"
      },
      "id": "3kc5g4Ggjdl4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running experiment 1 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended c61da37] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (89%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.68 KiB | 1.68 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   8a18230..c61da37  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 2 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 8a38b30] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (92%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.74 KiB | 1.74 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   c61da37..8a38b30  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 3 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 0d35f1e] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (89%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.59 KiB | 1.59 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   8a38b30..0d35f1e  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 4 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 67883c7] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (91%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.58 KiB | 1.58 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   0d35f1e..67883c7  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 5 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 2027a43] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 13 insertions(+), 15 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (83%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.39 KiB | 1.39 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   67883c7..2027a43  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 6 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 5ca313d] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 13 insertions(+), 15 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (86%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.41 KiB | 1.41 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   2027a43..5ca313d  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 7 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 07e6574] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 13 insertions(+), 15 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (82%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.34 KiB | 1.34 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   5ca313d..07e6574  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 8 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended aa43e75] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 13 insertions(+), 15 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (85%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.35 KiB | 1.35 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   07e6574..aa43e75  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 9 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended a104fa5] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 16 insertions(+), 16 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (84%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.57 KiB | 1.57 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   aa43e75..a104fa5  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 10 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 1b4fdaa] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 16 insertions(+), 16 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (87%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.60 KiB | 1.60 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   a104fa5..1b4fdaa  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 11 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended b8a7a2c] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 16 insertions(+), 16 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (84%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.51 KiB | 1.51 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   1b4fdaa..b8a7a2c  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 12 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended ffa44a2] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 16 insertions(+), 16 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (86%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.52 KiB | 1.52 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   b8a7a2c..ffa44a2  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 13 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 1d5c0af] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 12 insertions(+), 14 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (81%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.33 KiB | 1.33 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   ffa44a2..1d5c0af  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 14 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended eabefaf] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 12 insertions(+), 14 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (84%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.35 KiB | 1.35 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   1d5c0af..eabefaf  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 15 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended cf84d7a] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 12 insertions(+), 14 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (81%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.28 KiB | 1.28 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   eabefaf..cf84d7a  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 16 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 38df3da] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 12 insertions(+), 14 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (83%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.29 KiB | 1.29 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   cf84d7a..38df3da  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 17 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 842be75] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (89%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.67 KiB | 1.67 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   38df3da..842be75  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 18 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended af06ea6] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (92%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.73 KiB | 1.73 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   842be75..af06ea6  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 19 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 30d6287] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (89%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.58 KiB | 1.58 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   af06ea6..30d6287  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 20 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended fc15079] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 18 insertions(+), 22 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (91%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.60 KiB | 1.60 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   30d6287..fc15079  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 21 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended eeac2d1] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 10 insertions(+), 13 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (79%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.22 KiB | 1.22 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   fc15079..eeac2d1  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 22 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended ecb0d82] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 10 insertions(+), 13 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (83%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.22 KiB | 1.22 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   eeac2d1..ecb0d82  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 23 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 104b567] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 10 insertions(+), 13 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (79%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.20 KiB | 1.20 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   ecb0d82..104b567  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 24 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended fb8af47] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 10 insertions(+), 13 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (82%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.20 KiB | 1.20 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   104b567..fb8af47  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 25 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended e199287] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 12 insertions(+), 18 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (86%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.33 KiB | 1.33 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   fb8af47..e199287  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 26 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended c197c48] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 12 insertions(+), 18 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (89%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.33 KiB | 1.33 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   e199287..c197c48  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 27 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended c2680ad] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 12 insertions(+), 18 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt (86%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.30 KiB | 1.30 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   c197c48..c2680ad  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 28 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 803159a] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 12 insertions(+), 18 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt (88%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.32 KiB | 1.32 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   c2680ad..803159a  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 29 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 6fb9d5e] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 8 insertions(+), 12 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt (76%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.09 KiB | 1.09 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   803159a..6fb9d5e  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 30 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended f7b2495] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 8 insertions(+), 12 deletions(-)\n",
            " rewrite data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt (80%)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 17, done.\n",
            "Counting objects: 100% (17/17), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.10 KiB | 1.10 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   6fb9d5e..f7b2495  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 31 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Running experiment 32 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Grid Search Complete ---\n",
            "Summary of Runs:\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 8, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 8, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 8, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 8, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 11, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 11, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 11, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 11, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 13, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 5, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 5, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 5, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 5, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> Topics: 7, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions//bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} -> ERROR: max_df corresponds to < documents than min_df\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} -> ERROR: max_df corresponds to < documents than min_df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "2Z3hAYmLiFvF"
      },
      "id": "2Z3hAYmLiFvF"
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_df['content'])"
      ],
      "metadata": {
        "id": "v9XEaUjguTDL",
        "outputId": "e6ad15d5-db70-4012-f45e-de28ff878242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v9XEaUjguTDL",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = jp_stopwords\n",
        "target_df = grouped_jp_qna_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [2, 4], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [2, 4], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OhFadzgjUdO",
        "outputId": "a89ffccc-1123-4fec-c27e-513ff239c708"
      },
      "id": "1OhFadzgjUdO",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running experiment 1 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended af1ccbd] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.11 KiB | 1.11 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   f7b2495..af1ccbd  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 2 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 219f5f7] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.10 KiB | 1.10 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   af1ccbd..219f5f7  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 3 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)}: list index out of range\n",
            "\n",
            "--- Running experiment 4 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)}: list index out of range\n",
            "\n",
            "--- Running experiment 5 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 3a3fd5b] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 8 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.08 KiB | 1.08 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   219f5f7..3a3fd5b  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 6 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 8a90de9] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 8 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.08 KiB | 1.08 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   3a3fd5b..8a90de9  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 7 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended d39db92] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 7 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.03 KiB | 1.03 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   8a90de9..d39db92  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 8 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 0bc49d2] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 7 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.03 KiB | 1.03 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   d39db92..0bc49d2  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 9 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended c01a52f] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 8 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.07 KiB | 1.07 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   0bc49d2..c01a52f  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 10 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 9c46c37] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 8 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.07 KiB | 1.07 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   c01a52f..9c46c37  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 11 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)}: list index out of range\n",
            "\n",
            "--- Running experiment 12 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)}: list index out of range\n",
            "\n",
            "--- Running experiment 13 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 2720b52] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.13 KiB | 1.13 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   9c46c37..2720b52  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 14 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended b1322d3] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 16, done.\n",
            "Counting objects: 100% (16/16), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 1.13 KiB | 1.13 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   2720b52..b1322d3  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 15 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)}: list index out of range\n",
            "\n",
            "--- Running experiment 16 with parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)}: list index out of range\n",
            "\n",
            "--- Grid Search Complete ---\n",
            "Summary of Runs:\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 4, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 4, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> ERROR: list index out of range\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> ERROR: list index out of range\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 2, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 15, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 2, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_15_umap_n_components_4_hdbscan_min_cluster_size_4_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 3, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_2_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> ERROR: list index out of range\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 2, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> ERROR: list index out of range\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> Topics: 4, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 2, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> Topics: 4, Output: data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna//bertopic_topics_umap_n_neighbors_30_umap_n_components_4_hdbscan_min_cluster_size_2_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} -> ERROR: list index out of range\n",
            "  Parameters: {'umap_n_neighbors': 30, 'umap_n_components': 4, 'hdbscan_min_cluster_size': 4, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} -> ERROR: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "id": "bb15dc4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = nlp.stopwords\n",
        "# target_df = processed_jp_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}