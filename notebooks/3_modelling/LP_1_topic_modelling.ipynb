{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"lp_topic_modelling\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "ALL_TRANSCRIPTS_PATH = \"data/raw/JP Morgan/Transcripts\"\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "06c9977d-6e3d-45dc-f29f-863252d7e086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "    \n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f87c1ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4f87c1ff",
        "outputId": "a5156662-3175-436c-f078-7caafaec09f8"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbdee9f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "gs_discussion_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62823278",
      "metadata": {},
      "outputs": [],
      "source": [
        "gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a9a0fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7da4197",
      "metadata": {},
      "outputs": [],
      "source": [
        "jp_qna_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {},
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165610e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cf4579",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_list_from_text_file\n",
        "\n",
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "# gs_stopwords = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef849c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857d3584",
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_gs_qna_df = gs_qna_df.groupby(['question_answer_group_id', 'quarter', 'year'], as_index = False).agg({ 'content': ' '.join })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95b9b4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "all_stop_words = nlp.Defaults.stop_words.union(gs_stopwords)\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "    \n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "    \n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb801083",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_text(\"hello there this is the first quarter earnings report and i a questions about the you've you've\", all_stop_words, abbreviations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9d8da6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def vectorize_text(data_series, max_df=0.95, min_df=5, stop_words=None, ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Performs TF-IDF vectorization on a given text series.\n",
        "    Args:\n",
        "        data_series (pd.Series): A pandas Series containing preprocessed text content.\n",
        "        max_df (float): When building the vocabulary ignore terms that have a document\n",
        "                        frequency strictly higher than the given threshold (corpus-specific stop words).\n",
        "        min_df (int): When building the vocabulary ignore terms that have a document\n",
        "                      frequency strictly lower than the given threshold.\n",
        "        stop_words (str or list, optional): 'english' for a built-in stop word list,\n",
        "                                            a list of custom stop words, or None.\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "            - vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 3: Vectorization (TF-IDF)...\")\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words, ngram_range=ngram_range)\n",
        "    dtm = vectorizer.fit_transform(data_series)\n",
        "    print(f\"TF-IDF Vectorization complete. Document-Term Matrix shape: {dtm.shape}\")\n",
        "    return dtm, vectorizer\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic from an LDA model.\n",
        "    Args:\n",
        "        model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        feature_names (list): List of feature names (words) from the vectorizer.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "                                      Otherwise, they will be printed to the console.\n",
        "    \"\"\"\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        if file:\n",
        "            file.write(f\"\\nTopic {topic_idx + 1}:\\n\")\n",
        "            file.write(f\"{topic_words}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "            print(topic_words)\n",
        "\n",
        "def run_lda_topic_modeling(dtm, vectorizer, num_topics_range, output_dir=\"data/temp/leslie_topic_modelling_fine_tuning\", no_top_words=10, df_to_update=None):\n",
        "    \"\"\"\n",
        "    Performs LDA topic modeling for a range of topic numbers and saves the results.\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix from TF-IDF vectorization.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics_range (list): A list of integers representing the number of topics to experiment with.\n",
        "        output_dir (str): Directory to save the topic modeling output files.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        df_to_update (pd.DataFrame, optional): The DataFrame to which dominant topics will be\n",
        "                                                assigned. If None, dominant topics won't be added.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 4: Topic Modeling (LDA) for a range of topics...\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for num_topics in num_topics_range:\n",
        "        output_filename = f\"{output_dir}/lda_topics_k{num_topics}.txt\"\n",
        "        print(f\"\\n--- Processing with {num_topics} topics. Outputting to {output_filename} ---\")\n",
        "\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- LDA Topic Model with {num_topics} Topics ---\\n\\n\")\n",
        "\n",
        "            lda = LatentDirichletAllocation(n_components=num_topics,\n",
        "                                            max_iter=10,\n",
        "                                            learning_method='online',\n",
        "                                            learning_decay=0.7,\n",
        "                                            random_state=42,\n",
        "                                            n_jobs=-1,\n",
        "                                            evaluate_every=10)\n",
        "\n",
        "            lda.fit(dtm)\n",
        "            f.write(f\"LDA model fitting complete for {num_topics} topics.\\n\\n\")\n",
        "\n",
        "            f.write(\"\\nInterpreting Topics:\\n\")\n",
        "            display_topics(lda, feature_names, no_top_words, file=f)\n",
        "\n",
        "            if df_to_update is not None:\n",
        "                topic_distribution = lda.transform(dtm)\n",
        "                df_to_update[f'dominant_topic_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "                print(f\"\\nExample: Documents with their dominant topic (first 5 rows) for k={num_topics}:\")\n",
        "                for i in range(min(5, len(df_to_update))):\n",
        "                    original_content = df_to_update.loc[i, 'content']\n",
        "                    truncated_content = (original_content[:200] + '...') if len(original_content) > 200 else original_content\n",
        "                    dominant_topic = df_to_update.loc[i, f'dominant_topic_k{num_topics}']\n",
        "                    print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "\n",
        "grouped_gs_qna_df['preprocessed_content'] = grouped_gs_qna_df['content'].apply(lambda x: preprocess_text(x, all_stop_words, abbreviations))\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'], ngram_range=(1, 3))\n",
        "\n",
        "# num_topics_range = [8, 9, 10]\n",
        "# num_topics_range = [2, 5, 8, 11, 14, 17, 20, 23]\n",
        "num_topics_range = [5, 8, 11, 14]\n",
        "run_lda_topic_modeling(dtm_matrix, tfidf_vectorizer, num_topics_range, df_to_update=grouped_gs_qna_df, )\n",
        "\n",
        "print(\"\\nProcessing complete. Check the 'data/temp/leslie_topic_modelling_fine_tuning' directory for output files.\")\n",
        "print(\"\\nUpdated DataFrame with dominant topics (first 5 rows):\")\n",
        "print(grouped_gs_qna_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583eab95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Conceptual - requires FinBERT model loading and appropriate tokenization)\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def get_finbert_embeddings(texts):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "    model = AutoModel.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the [CLS] token embedding as the sentence embedding\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "def run_finbert_topic_clustering(df, num_topics_range, output_dir=\"data/temp/finbert_topic_clustering\"):\n",
        "    print(\"\\nStarting Topic Clustering using FinBERT embeddings...\")\n",
        "\n",
        "    # 1. Get FinBERT embeddings for your documents\n",
        "    document_embeddings = get_finbert_embeddings(df['content'].tolist())\n",
        "    print(f\"Generated {document_embeddings.shape[0]} document embeddings of size {document_embeddings.shape[1]}.\")\n",
        "\n",
        "    for num_topics in num_topics_range:\n",
        "        print(f\"\\n--- Processing with {num_topics} clusters (topics) using FinBERT embeddings ---\")\n",
        "        kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10) # n_init for stability\n",
        "        kmeans.fit(document_embeddings)\n",
        "        df[f'finbert_dominant_topic_k{num_topics}'] = kmeans.labels_\n",
        "\n",
        "        # 2. Interpret topics (example: finding top words per cluster using TF-IDF on cluster documents)\n",
        "        print(f\"\\nInterpreting Clusters for k={num_topics}:\")\n",
        "        for i in range(num_topics):\n",
        "            cluster_docs = df[df[f'finbert_dominant_topic_k{num_topics}'] == i]['content']\n",
        "            if len(cluster_docs) > 0:\n",
        "                # Use TF-IDF to find important words in this cluster\n",
        "                tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "                tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_docs)\n",
        "                feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "                sums_tfidf = tfidf_matrix.sum(axis=0)\n",
        "                # Get top words\n",
        "                top_words_indices = sums_tfidf.argsort()[0, ::-1][:10]\n",
        "                top_words = [feature_names[idx] for idx in top_words_indices.tolist()[0]]\n",
        "                print(f\"Cluster {i} (Top Words): {', '.join(top_words)}\")\n",
        "            else:\n",
        "                print(f\"Cluster {i}: No documents in this cluster.\")\n",
        "\n",
        "        print(f\"\\nExample: Documents with their dominant topic (first 5 rows) for k={num_topics}:\")\n",
        "        for i in range(min(5, len(df))):\n",
        "            original_content = df.loc[i, 'content']\n",
        "            truncated_content = (original_content[:200] + '...') if len(original_content) > 200 else original_content\n",
        "            dominant_topic = df.loc[i, f'finbert_dominant_topic_k{num_topics}']\n",
        "            print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# To run this, you would need to:\n",
        "# 1. Install transformers and torch: pip install transformers torch scikit-learn pandas\n",
        "# 2. Add the actual run_finbert_topic_clustering call with your dtm and df_to_update\n",
        "# For example: df_to_update = run_finbert_topic_clustering(df_to_update.copy(), num_topics_range=[2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9889a4cc",
      "metadata": {},
      "source": [
        "Conclusion and Recommendation\n",
        "The addition of stopwords has certainly helped in some areas, making certain topics clearer. However, some conversational noise still persists, especially words related to the Q&A format or general conversational patterns. The term \"Apple\" continues to be grouped with \"deposit\" in some k values, which is still a bit puzzling without specific context.\n",
        "\n",
        "Based on this comprehensive analysis, the most sensible k value is a trade-off between granularity, coherence, and minimizing \"junk\" topics.\n",
        "\n",
        "k=8: Offers good clarity for key themes (Credit Card, Headcount/Severance, Asset/Fundraising), but is still quite broad and has some remaining conversational noise.\n",
        "k=9: Introduces very strong \"GSIB\" and \"Wealth Management\" topics.\n",
        "k=10: Shows strong \"Investment/Platform\" and \"Fundraising\" themes.\n",
        "k=11: This k value demonstrates the best balance in this new set of runs.\n",
        "It produces several very distinct and interpretable financial/business topics: \"Wealth Management/European Footprint\" , \"Severance/Headcount/Capital\" , \"Credit Card/Consumer\" , \"GSIB/Allocation\" , \"Bank/Acquisition/Advisory\" , \"FICC/Equity/Commodity\" , and \"Deposit/Capital/Market/Exposure\".\n",
        "\n",
        "Crucially, the \"Apple\" anomaly is not present in the top words of any topic for k=11, suggesting a cleaner separation of terms.\n",
        "While some conversational noise is still present (Topics 2, 4, 9 in k=11), the quality of the interpretable topics is high.\n",
        "k=12, k=13, k=14: Beyond k=11, the topics generally become more fragmented, or reintroduce the \"Apple\" anomaly, and the number of less coherent/conversational topics increases, making overall interpretation more challenging. For example, k=12 recombines \"funding/deposits\" with \"severance/headcount\", which is less ideal.\n",
        "Therefore, my strongest recommendation is k=11. It provides a good level of detail for key financial aspects of Goldman Sachs' earnings calls while offering significantly improved topic coherence and distinctiveness, and effectively mitigating some of the persistent noise terms seen in other k values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaaa03d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Assuming this is already imported and used\n",
        "import pandas as pd # Assuming grouped_gs_qna_df is a pandas DataFrame\n",
        "\n",
        "def get_lda_topic_model_and_data(dtm, vectorizer, num_topics=11, no_top_words=10, **lda_params):\n",
        "    \"\"\"\n",
        "    Initializes and fits an LDA model, then extracts top words and their weights for each topic.\n",
        "\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics (int): The number of topics for the LDA model.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        **lda_params: Arbitrary keyword arguments to pass to the LatentDirichletAllocation constructor.\n",
        "                      Common parameters include max_iter, learning_method, learning_decay,\n",
        "                      random_state, n_jobs, evaluate_every.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "            - topics_data (list): A list of dictionaries, one for each topic,\n",
        "                                  containing 'topic_idx', 'top_words', and 'word_weights'.\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting LDA model fitting with {num_topics} topics...\")\n",
        "\n",
        "    default_lda_params = {\n",
        "        'max_iter': 10,\n",
        "        'learning_method': 'online',\n",
        "        'learning_decay': 0.7,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'evaluate_every': 10\n",
        "    }\n",
        "    effective_lda_params = {**default_lda_params, **lda_params}\n",
        "\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, **effective_lda_params)\n",
        "    lda_model.fit(dtm)\n",
        "    print(\"LDA model fitting complete.\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics_data = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words_indices = topic.argsort()[:-no_top_words - 1:-1]\n",
        "        top_words = [feature_names[i] for i in top_words_indices]\n",
        "        word_weights = [topic[i] for i in top_words_indices]\n",
        "\n",
        "        topics_data.append({\n",
        "            'topic_idx': topic_idx,\n",
        "            'top_words': top_words,\n",
        "            'word_weights': word_weights\n",
        "        })\n",
        "    return lda_model, topics_data\n",
        "\n",
        "def assign_dominant_topics_to_dataframe(df, lda_model, dtm, column_prefix='dominant_topic'):\n",
        "    \"\"\"\n",
        "    Assigns the dominant topic to each document in a DataFrame based on an LDA model.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to which dominant topics will be assigned.\n",
        "        lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix used for fitting the LDA model.\n",
        "        column_prefix (str): Prefix for the new column name (e.g., 'dominant_topic_k11').\n",
        "    \"\"\"\n",
        "    topic_distribution = lda_model.transform(dtm)\n",
        "    num_topics = lda_model.n_components\n",
        "    df[f'{column_prefix}_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "    print(f\"Dominant topics assigned to DataFrame under column '{column_prefix}_k{num_topics}'.\")\n",
        "\n",
        "\n",
        "# Phase 3: Vectorization (using the previous function)\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'])\n",
        "\n",
        "# Phase 4: Topic Modeling (using the new improved function)\n",
        "chosen_num_topics = 11 # From previous analysis, or experiment here\n",
        "top_words_count = 10\n",
        "\n",
        "# You can pass custom LDA parameters if needed, e.g., max_iter=20, learning_decay=0.9\n",
        "lda_model_fitted, topics_data_extracted = get_lda_topic_model_and_data(\n",
        "    dtm=dtm_matrix,\n",
        "    vectorizer=tfidf_vectorizer,\n",
        "    num_topics=chosen_num_topics,\n",
        "    no_top_words=top_words_count,\n",
        "    random_state=42 # Explicitly passing for clarity, though it's a default\n",
        ")\n",
        "\n",
        "# Print extracted topics data\n",
        "print(\"\\n--- Extracted Topics Data ---\")\n",
        "for topic in topics_data_extracted:\n",
        "    print(f\"Topic {topic['topic_idx'] + 1}: {' '.join(topic['top_words'])}\")\n",
        "\n",
        "# Assign dominant topics back to the DataFrame\n",
        "assign_dominant_topics_to_dataframe(grouped_gs_qna_df, lda_model_fitted, dtm_matrix)\n",
        "\n",
        "print(\"\\nProcessing complete. The DataFrame now contains dominant topics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a2cca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated DataFrame with dominant topics (first 5 rows)\n",
        "grouped_gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5525b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for topic in topics_data_extracted:\n",
        "    print(f\"\\nTopic {topic['topic_idx'] + 1}:\")\n",
        "    for word, weight in zip(topic['top_words'], topic['word_weights']):\n",
        "        print(f\"{word} (weight: {weight:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8108189b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "topic_labels_map = {\n",
        "    0: \"Strategic Positioning & Platform\",\n",
        "    1: \"Wealth Management & European Markets\",\n",
        "    2: \"Alternative Investments & Fee Income\",\n",
        "    3: \"Headcount & Workforce Management\",\n",
        "    4: \"Consumer Credit & Card Performance\",\n",
        "    5: \"FICC & Equity Trading Performance\",\n",
        "    6: \"Regulatory Capital & Institutional Allocation\",\n",
        "    7: \"Client-Centric Growth & Solutions\",\n",
        "    8: \"M&A, Valuations & Advisory\",\n",
        "    9: \"FICC & Market Environment\",\n",
        "    10: \"Deposits, Capital & Funding\"\n",
        "}\n",
        "\n",
        "# Assign labels to the topics_data\n",
        "for topic_info in topics_data:\n",
        "    topic_info['label'] = topic_labels_map.get(topic_info['topic_idx'], f\"Unlabeled Topic {topic_info['topic_idx']}\")\n",
        "\n",
        "print(\"\\n--- Topics with assigned labels and top words ---\")\n",
        "for topic_info in topics_data:\n",
        "    print(f\"Topic {topic_info['topic_idx'] + 1}: {topic_info['label']}\")\n",
        "    print(f\"  Top Words: {' '.join(topic_info['top_words'])}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- Visual Display of Topics ---\n",
        "print(\"\\n--- Generating visual display of topics ---\")\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = (num_topics + n_cols - 1) // n_cols\n",
        "plt.figure(figsize=(n_cols * 6, n_rows * 4), dpi=100)\n",
        "\n",
        "for i, topic_info in enumerate(topics_data):\n",
        "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "    df_plot = pd.DataFrame({\n",
        "        'word': topic_info['top_words'],\n",
        "        'weight': topic_info['word_weights']\n",
        "    })\n",
        "    df_plot = df_plot.sort_values(by='weight', ascending=True)\n",
        "    sns.barplot(x='weight', y='word', data=df_plot, palette='magma', ax=ax)\n",
        "    ax.set_title(f\"{topic_info['label']}\", fontsize=11, fontweight='bold', pad=10)\n",
        "    ax.set_xlabel(\"Word Importance (Weight)\", fontsize=9)\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "    sns.despine(ax=ax, top=True, right=True, left=False, bottom=False)\n",
        "    ax.tick_params(axis='y', length=0)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.suptitle(\n",
        "    f\"Top Words for {num_topics} Topics in Goldman Sachs Earnings Calls (Q&A Section)\",\n",
        "    y=1.00, fontsize=16, fontweight='bold'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisual display generated. Please review the plots and verify the topic labels.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f760200",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959f9349",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='year',\n",
        "    hue='quarter',\n",
        "    palette='tab10'\n",
        ")\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Distribution of Documents by Year and Quarter\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Quarter')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Now, for dominant topics over both year and quarter:\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='dominant_topic_k11',\n",
        "    hue='year',\n",
        "    palette='tab10'\n",
        ")\n",
        "topic_labels = [topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())]\n",
        "plt.xticks(ticks=range(len(topic_labels)), labels=topic_labels, rotation=45, ha='right')\n",
        "plt.xlabel('Dominant Topic (k=11)')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Dominant Topics (k=11) by Year\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Year')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e53850b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "years = sorted(grouped_gs_qna_df['year'].unique())\n",
        "n_years = len(years)\n",
        "\n",
        "fig, axes = plt.subplots(n_years, 1, figsize=(10, 4 * n_years), sharex=True)\n",
        "\n",
        "if n_years == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette='tab10',\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    ax.legend(\n",
        "        title='Dominant Topic',\n",
        "        loc='upper right',\n",
        "        labels=[topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(data['dominant_topic_k11'].unique())]\n",
        "    )\n",
        "\n",
        "# Fix color mapping so each topic always has the same color across years\n",
        "unique_topics = sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())\n",
        "topic_palette = sns.color_palette('tab10', n_colors=len(unique_topics))\n",
        "topic_color_dict = {topic: topic_palette[i % len(topic_palette)] for i, topic in enumerate(unique_topics)}\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    # Use the same color mapping for all years\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette=topic_color_dict,\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    # Always use the same order and labels for legend\n",
        "    ordered_labels = [topic_labels_map.get(t, f\"Topic {t}\") for t in unique_topics]\n",
        "    ax.legend(handles, ordered_labels, title='Dominant Topic', loc='upper right')\n",
        "\n",
        "plt.suptitle(\n",
        "    \"Quarterly Distribution of Dominant Topics (k=11) by Year\\nGoldman Sachs Earnings Call Transcript (Q&A Section)\",\n",
        "    fontsize=16, fontweight='bold', y=1.02\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7817bea",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb15dc4f",
        "outputId": "230dcdf8-192c-4bd7-97fe-1b128a442bd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_dir = 'data/temp/'\n",
        "file_name = 'dummy_test_output_new.csv'\n",
        "dummy_pf = pd.DataFrame({'from_colab': [IS_COLAB, True, 'hello']})\n",
        "\n",
        "\n",
        "if OUTPUT_PROCESSED_FILES:\n",
        "    if IS_COLAB:\n",
        "        AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "\n",
        "        # Configure Git user (important for committing)\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {target_dir}{file_name}\n",
        "        print(f\"Added '{target_dir}{file_name}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {target_dir}{file_name}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "        print(\"Push command executed. Check output for success or prompt.\")\n",
        "    else:\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "        print(\"Processed files saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
