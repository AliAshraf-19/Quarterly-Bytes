{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"lp_topic_modelling\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "ALL_TRANSCRIPTS_PATH = \"data/raw/JP Morgan/Transcripts\"\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "06c9977d-6e3d-45dc-f29f-863252d7e086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2==3.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: nltk>=3.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: spacy>=3.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.7)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 6)) (3.10.3)\n",
            "Requirement already satisfied: seaborn in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: click in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (75.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->nltk>=3.0.0->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\zifen\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2.19.1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\zifen\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "    \n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f87c1ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4f87c1ff",
        "outputId": "a5156662-3175-436c-f078-7caafaec09f8"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bbdee9f5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>content</th>\n",
              "      <th>quarter</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>David Solomon</td>\n",
              "      <td>Chairman, Chief Executive Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>Thank you, operator , good morning, everyone....</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Denis Coleman</td>\n",
              "      <td>Chief Financial Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>Thank you, David. Good morning. Let's start w...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>David Solomon</td>\n",
              "      <td>CEO, Chairman</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>Thank you, operator , and good morning, every...</td>\n",
              "      <td>4</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Denis Coleman</td>\n",
              "      <td>CFO</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>Thank you, David, and good morning. Let's sta...</td>\n",
              "      <td>4</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Carey Halio</td>\n",
              "      <td>Chief Strategy Ofﬁcer, Head of Investor Relations</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>Thank you. Good morning. This is Carey Halio,...</td>\n",
              "      <td>2</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         speaker                                               role  \\\n",
              "0  David Solomon                   Chairman, Chief Executive Ofﬁcer   \n",
              "1  Denis Coleman                             Chief Financial Ofﬁcer   \n",
              "2  David Solomon                                      CEO, Chairman   \n",
              "3  Denis Coleman                                                CFO   \n",
              "4    Carey Halio  Chief Strategy Ofﬁcer, Head of Investor Relations   \n",
              "\n",
              "         company                                            content  quarter  \\\n",
              "0  Goldman Sachs   Thank you, operator , good morning, everyone....        3   \n",
              "1  Goldman Sachs   Thank you, David. Good morning. Let's start w...        3   \n",
              "2  Goldman Sachs   Thank you, operator , and good morning, every...        4   \n",
              "3  Goldman Sachs   Thank you, David, and good morning. Let's sta...        4   \n",
              "4  Goldman Sachs   Thank you. Good morning. This is Carey Halio,...        2   \n",
              "\n",
              "   year  \n",
              "0  2024  \n",
              "1  2024  \n",
              "2  2024  \n",
              "3  2024  \n",
              "4  2023  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_discussion_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "62823278",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_order</th>\n",
              "      <th>question_answer_group_id</th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>content_type</th>\n",
              "      <th>content</th>\n",
              "      <th>quarter</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Glenn Schorr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>question</td>\n",
              "      <td>so, trading question, i mean, markets busines...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>David Solomon</td>\n",
              "      <td>Chairman, Chief Executive Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>answer</td>\n",
              "      <td>i appreciate the question, glenn, and i mean,...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Glenn Schorr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>question</td>\n",
              "      <td>i appreciate that. this one will be a short f...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Denis Coleman</td>\n",
              "      <td>Chief Financial Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>answer</td>\n",
              "      <td>glenn, its denis. i guess what i would sugges...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Ebrahim Poonawala</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>question</td>\n",
              "      <td>i just had a follow-up ﬁrst on trading and ma...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_order  question_answer_group_id            speaker  \\\n",
              "0               0                         0       Glenn Schorr   \n",
              "1               1                         0      David Solomon   \n",
              "2               2                         0       Glenn Schorr   \n",
              "3               3                         0      Denis Coleman   \n",
              "4               0                         1  Ebrahim Poonawala   \n",
              "\n",
              "                               role          company content_type  \\\n",
              "0                               NaN         Evercore     question   \n",
              "1  Chairman, Chief Executive Ofﬁcer    Goldman Sachs       answer   \n",
              "2                               NaN         Evercore     question   \n",
              "3            Chief Financial Ofﬁcer    Goldman Sachs       answer   \n",
              "4                               NaN  Bank of America     question   \n",
              "\n",
              "                                             content  quarter  year  \n",
              "0   so, trading question, i mean, markets busines...        3  2024  \n",
              "1   i appreciate the question, glenn, and i mean,...        3  2024  \n",
              "2   i appreciate that. this one will be a short f...        3  2024  \n",
              "3   glenn, its denis. i guess what i would sugges...        3  2024  \n",
              "4   i just had a follow-up ﬁrst on trading and ma...        3  2024  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f4a9a0fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_order</th>\n",
              "      <th>question_answer_group_id</th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>content_type</th>\n",
              "      <th>content</th>\n",
              "      <th>quarter</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Glenn Schorr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>question</td>\n",
              "      <td>so, trading question, i mean, markets busines...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>David Solomon</td>\n",
              "      <td>Chairman, Chief Executive Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>answer</td>\n",
              "      <td>i appreciate the question, glenn, and i mean,...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Glenn Schorr</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Evercore</td>\n",
              "      <td>question</td>\n",
              "      <td>i appreciate that. this one will be a short f...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Denis Coleman</td>\n",
              "      <td>Chief Financial Ofﬁcer</td>\n",
              "      <td>Goldman Sachs</td>\n",
              "      <td>answer</td>\n",
              "      <td>glenn, its denis. i guess what i would sugges...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Ebrahim Poonawala</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>question</td>\n",
              "      <td>i just had a follow-up ﬁrst on trading and ma...</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_order  question_answer_group_id            speaker  \\\n",
              "0               0                         0       Glenn Schorr   \n",
              "1               1                         0      David Solomon   \n",
              "2               2                         0       Glenn Schorr   \n",
              "3               3                         0      Denis Coleman   \n",
              "4               0                         1  Ebrahim Poonawala   \n",
              "\n",
              "                               role          company content_type  \\\n",
              "0                               NaN         Evercore     question   \n",
              "1  Chairman, Chief Executive Ofﬁcer    Goldman Sachs       answer   \n",
              "2                               NaN         Evercore     question   \n",
              "3            Chief Financial Ofﬁcer    Goldman Sachs       answer   \n",
              "4                               NaN  Bank of America     question   \n",
              "\n",
              "                                             content  quarter  year  \n",
              "0   so, trading question, i mean, markets busines...        3  2024  \n",
              "1   i appreciate the question, glenn, and i mean,...        3  2024  \n",
              "2   i appreciate that. this one will be a short f...        3  2024  \n",
              "3   glenn, its denis. i guess what i would sugges...        3  2024  \n",
              "4   i just had a follow-up ﬁrst on trading and ma...        3  2024  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7da4197",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_order</th>\n",
              "      <th>question_answer_group_id</th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>content</th>\n",
              "      <th>year</th>\n",
              "      <th>quarter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>John E. McDonald</td>\n",
              "      <td>Analyst</td>\n",
              "      <td>Autonomous Research</td>\n",
              "      <td>Thank you. Morning, Jeremy. Was wondering abou...</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>Good morning, John. Good question. Yeah, look,...</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>John E. McDonald</td>\n",
              "      <td>Analyst</td>\n",
              "      <td>Autonomous Research</td>\n",
              "      <td>Okay. And as my follow up, could you give us s...</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>Yeah. I guess I would direct you to my comment...</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>John E. McDonald</td>\n",
              "      <td>Analyst</td>\n",
              "      <td>Autonomous Research</td>\n",
              "      <td>Okay. Thanks.</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_order  question_answer_group_id           speaker  \\\n",
              "0               0                         0  John E. McDonald   \n",
              "1               1                         0     Jeremy Barnum   \n",
              "2               2                         0  John E. McDonald   \n",
              "3               3                         0     Jeremy Barnum   \n",
              "4               4                         0  John E. McDonald   \n",
              "\n",
              "                      role               company  \\\n",
              "0                  Analyst   Autonomous Research   \n",
              "1  Chief Financial Officer  JPMorgan Chase & Co.   \n",
              "2                  Analyst   Autonomous Research   \n",
              "3  Chief Financial Officer  JPMorgan Chase & Co.   \n",
              "4                  Analyst   Autonomous Research   \n",
              "\n",
              "                                             content  year  quarter  \n",
              "0  Thank you. Morning, Jeremy. Was wondering abou...  2022        1  \n",
              "1  Good morning, John. Good question. Yeah, look,...  2022        1  \n",
              "2  Okay. And as my follow up, could you give us s...  2022        1  \n",
              "3  Yeah. I guess I would direct you to my comment...  2022        1  \n",
              "4                                      Okay. Thanks.  2022        1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jp_qna_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {},
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "165610e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:09\n",
            "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
            "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 1.4 MB/s eta 0:00:09\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 1.5 MB/s eta 0:00:08\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     -------- ------------------------------- 2.6/12.8 MB 1.7 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 3.1/12.8 MB 1.3 MB/s eta 0:00:08\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 1.3 MB/s eta 0:00:08\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 1.3 MB/s eta 0:00:08\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------- -------------------------- 4.2/12.8 MB 1.2 MB/s eta 0:00:08\n",
            "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
            "     -------------- ------------------------- 4.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 1.3 MB/s eta 0:00:06\n",
            "     ------------------ --------------------- 5.8/12.8 MB 1.3 MB/s eta 0:00:06\n",
            "     ------------------- -------------------- 6.3/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     --------------------- ------------------ 6.8/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     --------------------- ------------------ 6.8/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     ----------------------- ---------------- 7.6/12.8 MB 1.5 MB/s eta 0:00:04\n",
            "     -------------------------- ------------- 8.4/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 8.9/12.8 MB 1.5 MB/s eta 0:00:03\n",
            "     ------------------------------- -------- 10.0/12.8 MB 1.6 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.2/12.8 MB 1.6 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 1.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 1.8 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23cf4579",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_list_from_text_file\n",
        "\n",
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "# gs_stopwords = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7ef849c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "857d3584",
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_gs_qna_df = gs_qna_df.groupby(['question_answer_group_id', 'quarter', 'year'], as_index = False).agg({ 'content': ' '.join })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b95b9b4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "all_stop_words = nlp.Defaults.stop_words.union(gs_stopwords)\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "    \n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "    \n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fb801083",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello 1q earning report question'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_text(\"hello there this is the first quarter earnings report and i a questions about the you've you've\", all_stop_words, abbreviations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9d8da6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Phase 3: Vectorization (TF-IDF)...\n",
            "TF-IDF Vectorization complete. Document-Term Matrix shape: (149, 1519)\n",
            "\n",
            "Starting Phase 4: Topic Modeling (LDA) for a range of topics...\n",
            "\n",
            "--- Processing with 2 topics. Outputting to data/temp/leslie_topic_modelling_fine_tuning/lda_topics_k2.txt ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def tyyyyyyyyqqklllllllllllllllllllllllll444(data_series, max_df=0.95, min_df=5, stop_words=None, ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Performs TF-IDF vectorization on a given text series.\n",
        "    Args:\n",
        "        data_series (pd.Series): A pandas Series containing preprocessed text content.\n",
        "        max_df (float): When building the vocabulary ignore terms that have a document\n",
        "                        frequency strictly higher than the given threshold (corpus-specific stop words).\n",
        "        min_df (int): When building the vocabulary ignore terms that have a document\n",
        "                      frequency strictly lower than the given threshold.\n",
        "        stop_words (str or list, optional): 'english' for a built-in stop word list,\n",
        "                                            a list of custom stop words, or None.\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "            - vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 3: Vectorization (TF-IDF)...\")\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words, ngram_range=ngram_range)\n",
        "    dtm = vectorizer.fit_transform(data_series)\n",
        "    print(f\"TF-IDF Vectorization complete. Document-Term Matrix shape: {dtm.shape}\")\n",
        "    return dtm, vectorizer\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic from an LDA model.\n",
        "    Args:\n",
        "        model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        feature_names (list): List of feature names (words) from the vectorizer.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "                                      Otherwise, they will be printed to the console.\n",
        "    \"\"\"\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        if file:\n",
        "            file.write(f\"\\nTopic {topic_idx + 1}:\\n\")\n",
        "            file.write(f\"{topic_words}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "            print(topic_words)\n",
        "\n",
        "def run_lda_topic_modeling(dtm, vectorizer, num_topics_range, output_dir=\"data/temp/leslie_topic_modelling_fine_tuning\", no_top_words=10, df_to_update=None):\n",
        "    \"\"\"\n",
        "    Performs LDA topic modeling for a range of topic numbers and saves the results.\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix from TF-IDF vectorization.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics_range (list): A list of integers representing the number of topics to experiment with.\n",
        "        output_dir (str): Directory to save the topic modeling output files.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        df_to_update (pd.DataFrame, optional): The DataFrame to which dominant topics will be\n",
        "                                                assigned. If None, dominant topics won't be added.\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting Phase 4: Topic Modeling (LDA) for a range of topics...\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for num_topics in num_topics_range:\n",
        "        output_filename = f\"{output_dir}/lda_topics_k{num_topics}.txt\"\n",
        "        print(f\"\\n--- Processing with {num_topics} topics. Outputting to {output_filename} ---\")\n",
        "\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- LDA Topic Model with {num_topics} Topics ---\\n\\n\")\n",
        "\n",
        "            lda = LatentDirichletAllocation(n_components=num_topics,\n",
        "                                            max_iter=10,\n",
        "                                            learning_method='online',\n",
        "                                            learning_decay=0.7,\n",
        "                                            random_state=42,\n",
        "                                            n_jobs=-1,\n",
        "                                            evaluate_every=10)\n",
        "\n",
        "            lda.fit(dtm)\n",
        "            f.write(f\"LDA model fitting complete for {num_topics} topics.\\n\\n\")\n",
        "\n",
        "            f.write(\"\\nInterpreting Topics:\\n\")\n",
        "            display_topics(lda, feature_names, no_top_words, file=f)\n",
        "\n",
        "            if df_to_update is not None:\n",
        "                topic_distribution = lda.transform(dtm)\n",
        "                df_to_update[f'dominant_topic_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "                print(f\"\\nExample: Documents with their dominant topic (first 5 rows) for k={num_topics}:\")\n",
        "                for i in range(min(5, len(df_to_update))):\n",
        "                    original_content = df_to_update.loc[i, 'content']\n",
        "                    truncated_content = (original_content[:200] + '...') if len(original_content) > 200 else original_content\n",
        "                    dominant_topic = df_to_update.loc[i, f'dominant_topic_k{num_topics}']\n",
        "                    print(f\"Doc {i}: Topic {dominant_topic} - Content: \\\"{truncated_content}\\\"\")\n",
        "\n",
        "grouped_gs_qna_df['preprocessed_content'] = grouped_gs_qna_df['content'].apply(lambda x: preprocess_text(x, all_stop_words, abbreviations))\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'], ngram_range=(1, 3))\n",
        "\n",
        "# num_topics_range = [8, 9, 10]\n",
        "num_topics_range = [2, 5, 8, 11, 14, 17, 20, 23]\n",
        "run_lda_topic_modeling(dtm_matrix, tfidf_vectorizer, num_topics_range, df_to_update=grouped_gs_qna_df, )\n",
        "\n",
        "print(\"\\nProcessing complete. Check the 'data/temp/leslie_topic_modelling_fine_tuning' directory for output files.\")\n",
        "print(\"\\nUpdated DataFrame with dominant topics (first 5 rows):\")\n",
        "print(grouped_gs_qna_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9889a4cc",
      "metadata": {},
      "source": [
        "Conclusion and Recommendation\n",
        "The addition of stopwords has certainly helped in some areas, making certain topics clearer. However, some conversational noise still persists, especially words related to the Q&A format or general conversational patterns. The term \"Apple\" continues to be grouped with \"deposit\" in some k values, which is still a bit puzzling without specific context.\n",
        "\n",
        "Based on this comprehensive analysis, the most sensible k value is a trade-off between granularity, coherence, and minimizing \"junk\" topics.\n",
        "\n",
        "k=8: Offers good clarity for key themes (Credit Card, Headcount/Severance, Asset/Fundraising), but is still quite broad and has some remaining conversational noise.\n",
        "k=9: Introduces very strong \"GSIB\" and \"Wealth Management\" topics.\n",
        "k=10: Shows strong \"Investment/Platform\" and \"Fundraising\" themes.\n",
        "k=11: This k value demonstrates the best balance in this new set of runs.\n",
        "It produces several very distinct and interpretable financial/business topics: \"Wealth Management/European Footprint\" , \"Severance/Headcount/Capital\" , \"Credit Card/Consumer\" , \"GSIB/Allocation\" , \"Bank/Acquisition/Advisory\" , \"FICC/Equity/Commodity\" , and \"Deposit/Capital/Market/Exposure\".\n",
        "\n",
        "Crucially, the \"Apple\" anomaly is not present in the top words of any topic for k=11, suggesting a cleaner separation of terms.\n",
        "While some conversational noise is still present (Topics 2, 4, 9 in k=11), the quality of the interpretable topics is high.\n",
        "k=12, k=13, k=14: Beyond k=11, the topics generally become more fragmented, or reintroduce the \"Apple\" anomaly, and the number of less coherent/conversational topics increases, making overall interpretation more challenging. For example, k=12 recombines \"funding/deposits\" with \"severance/headcount\", which is less ideal.\n",
        "Therefore, my strongest recommendation is k=11. It provides a good level of detail for key financial aspects of Goldman Sachs' earnings calls while offering significantly improved topic coherence and distinctiveness, and effectively mitigating some of the persistent noise terms seen in other k values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaaa03d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Assuming this is already imported and used\n",
        "import pandas as pd # Assuming grouped_gs_qna_df is a pandas DataFrame\n",
        "\n",
        "def get_lda_topic_model_and_data(dtm, vectorizer, num_topics=11, no_top_words=10, **lda_params):\n",
        "    \"\"\"\n",
        "    Initializes and fits an LDA model, then extracts top words and their weights for each topic.\n",
        "\n",
        "    Args:\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix.\n",
        "        vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "        num_topics (int): The number of topics for the LDA model.\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        **lda_params: Arbitrary keyword arguments to pass to the LatentDirichletAllocation constructor.\n",
        "                      Common parameters include max_iter, learning_method, learning_decay,\n",
        "                      random_state, n_jobs, evaluate_every.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "            - topics_data (list): A list of dictionaries, one for each topic,\n",
        "                                  containing 'topic_idx', 'top_words', and 'word_weights'.\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting LDA model fitting with {num_topics} topics...\")\n",
        "\n",
        "    default_lda_params = {\n",
        "        'max_iter': 10,\n",
        "        'learning_method': 'online',\n",
        "        'learning_decay': 0.7,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'evaluate_every': 10\n",
        "    }\n",
        "    effective_lda_params = {**default_lda_params, **lda_params}\n",
        "\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, **effective_lda_params)\n",
        "    lda_model.fit(dtm)\n",
        "    print(\"LDA model fitting complete.\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics_data = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words_indices = topic.argsort()[:-no_top_words - 1:-1]\n",
        "        top_words = [feature_names[i] for i in top_words_indices]\n",
        "        word_weights = [topic[i] for i in top_words_indices]\n",
        "\n",
        "        topics_data.append({\n",
        "            'topic_idx': topic_idx,\n",
        "            'top_words': top_words,\n",
        "            'word_weights': word_weights\n",
        "        })\n",
        "    return lda_model, topics_data\n",
        "\n",
        "def assign_dominant_topics_to_dataframe(df, lda_model, dtm, column_prefix='dominant_topic'):\n",
        "    \"\"\"\n",
        "    Assigns the dominant topic to each document in a DataFrame based on an LDA model.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to which dominant topics will be assigned.\n",
        "        lda_model (LatentDirichletAllocation): The fitted LDA model.\n",
        "        dtm (scipy.sparse.csr_matrix): The Document-Term Matrix used for fitting the LDA model.\n",
        "        column_prefix (str): Prefix for the new column name (e.g., 'dominant_topic_k11').\n",
        "    \"\"\"\n",
        "    topic_distribution = lda_model.transform(dtm)\n",
        "    num_topics = lda_model.n_components\n",
        "    df[f'{column_prefix}_k{num_topics}'] = np.argmax(topic_distribution, axis=1)\n",
        "    print(f\"Dominant topics assigned to DataFrame under column '{column_prefix}_k{num_topics}'.\")\n",
        "\n",
        "\n",
        "# Phase 3: Vectorization (using the previous function)\n",
        "dtm_matrix, tfidf_vectorizer = vectorize_text(grouped_gs_qna_df['preprocessed_content'])\n",
        "\n",
        "# Phase 4: Topic Modeling (using the new improved function)\n",
        "chosen_num_topics = 11 # From previous analysis, or experiment here\n",
        "top_words_count = 10\n",
        "\n",
        "# You can pass custom LDA parameters if needed, e.g., max_iter=20, learning_decay=0.9\n",
        "lda_model_fitted, topics_data_extracted = get_lda_topic_model_and_data(\n",
        "    dtm=dtm_matrix,\n",
        "    vectorizer=tfidf_vectorizer,\n",
        "    num_topics=chosen_num_topics,\n",
        "    no_top_words=top_words_count,\n",
        "    random_state=42 # Explicitly passing for clarity, though it's a default\n",
        ")\n",
        "\n",
        "# Print extracted topics data\n",
        "print(\"\\n--- Extracted Topics Data ---\")\n",
        "for topic in topics_data_extracted:\n",
        "    print(f\"Topic {topic['topic_idx'] + 1}: {' '.join(topic['top_words'])}\")\n",
        "\n",
        "# Assign dominant topics back to the DataFrame\n",
        "assign_dominant_topics_to_dataframe(grouped_gs_qna_df, lda_model_fitted, dtm_matrix)\n",
        "\n",
        "print(\"\\nProcessing complete. The DataFrame now contains dominant topics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a2cca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated DataFrame with dominant topics (first 5 rows)\n",
        "grouped_gs_qna_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5525b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for topic in topics_data_extracted:\n",
        "    print(f\"\\nTopic {topic['topic_idx'] + 1}:\")\n",
        "    for word, weight in zip(topic['top_words'], topic['word_weights']):\n",
        "        print(f\"{word} (weight: {weight:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8108189b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "topic_labels_map = {\n",
        "    0: \"Strategic Positioning & Platform\",\n",
        "    1: \"Wealth Management & European Markets\",\n",
        "    2: \"Alternative Investments & Fee Income\",\n",
        "    3: \"Headcount & Workforce Management\",\n",
        "    4: \"Consumer Credit & Card Performance\",\n",
        "    5: \"FICC & Equity Trading Performance\",\n",
        "    6: \"Regulatory Capital & Institutional Allocation\",\n",
        "    7: \"Client-Centric Growth & Solutions\",\n",
        "    8: \"M&A, Valuations & Advisory\",\n",
        "    9: \"FICC & Market Environment\",\n",
        "    10: \"Deposits, Capital & Funding\"\n",
        "}\n",
        "\n",
        "# Assign labels to the topics_data\n",
        "for topic_info in topics_data:\n",
        "    topic_info['label'] = topic_labels_map.get(topic_info['topic_idx'], f\"Unlabeled Topic {topic_info['topic_idx']}\")\n",
        "\n",
        "print(\"\\n--- Topics with assigned labels and top words ---\")\n",
        "for topic_info in topics_data:\n",
        "    print(f\"Topic {topic_info['topic_idx'] + 1}: {topic_info['label']}\")\n",
        "    print(f\"  Top Words: {' '.join(topic_info['top_words'])}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- Visual Display of Topics ---\n",
        "print(\"\\n--- Generating visual display of topics ---\")\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = (num_topics + n_cols - 1) // n_cols\n",
        "plt.figure(figsize=(n_cols * 6, n_rows * 4), dpi=100)\n",
        "\n",
        "for i, topic_info in enumerate(topics_data):\n",
        "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "    df_plot = pd.DataFrame({\n",
        "        'word': topic_info['top_words'],\n",
        "        'weight': topic_info['word_weights']\n",
        "    })\n",
        "    df_plot = df_plot.sort_values(by='weight', ascending=True)\n",
        "    sns.barplot(x='weight', y='word', data=df_plot, palette='magma', ax=ax)\n",
        "    ax.set_title(f\"{topic_info['label']}\", fontsize=11, fontweight='bold', pad=10)\n",
        "    ax.set_xlabel(\"Word Importance (Weight)\", fontsize=9)\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "    sns.despine(ax=ax, top=True, right=True, left=False, bottom=False)\n",
        "    ax.tick_params(axis='y', length=0)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.suptitle(\n",
        "    f\"Top Words for {num_topics} Topics in Goldman Sachs Earnings Calls (Q&A Section)\",\n",
        "    y=1.00, fontsize=16, fontweight='bold'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisual display generated. Please review the plots and verify the topic labels.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f760200",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959f9349",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='year',\n",
        "    hue='quarter',\n",
        "    palette='tab10'\n",
        ")\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Distribution of Documents by Year and Quarter\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Quarter')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Now, for dominant topics over both year and quarter:\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.countplot(\n",
        "    data=grouped_gs_qna_df,\n",
        "    x='dominant_topic_k11',\n",
        "    hue='year',\n",
        "    palette='tab10'\n",
        ")\n",
        "topic_labels = [topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())]\n",
        "plt.xticks(ticks=range(len(topic_labels)), labels=topic_labels, rotation=45, ha='right')\n",
        "plt.xlabel('Dominant Topic (k=11)')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Dominant Topics (k=11) by Year\\nGoldman Sachs Q&A')\n",
        "plt.legend(title='Year')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e53850b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "years = sorted(grouped_gs_qna_df['year'].unique())\n",
        "n_years = len(years)\n",
        "\n",
        "fig, axes = plt.subplots(n_years, 1, figsize=(10, 4 * n_years), sharex=True)\n",
        "\n",
        "if n_years == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette='tab10',\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    ax.legend(\n",
        "        title='Dominant Topic',\n",
        "        loc='upper right',\n",
        "        labels=[topic_labels_map.get(i, f\"Topic {i}\") for i in sorted(data['dominant_topic_k11'].unique())]\n",
        "    )\n",
        "\n",
        "# Fix color mapping so each topic always has the same color across years\n",
        "unique_topics = sorted(grouped_gs_qna_df['dominant_topic_k11'].unique())\n",
        "topic_palette = sns.color_palette('tab10', n_colors=len(unique_topics))\n",
        "topic_color_dict = {topic: topic_palette[i % len(topic_palette)] for i, topic in enumerate(unique_topics)}\n",
        "\n",
        "for idx, year in enumerate(years):\n",
        "    ax = axes[idx]\n",
        "    data = grouped_gs_qna_df[grouped_gs_qna_df['year'] == year]\n",
        "    # Use the same color mapping for all years\n",
        "    sns.countplot(\n",
        "        data=data,\n",
        "        x='quarter',\n",
        "        hue='dominant_topic_k11',\n",
        "        palette=topic_color_dict,\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'Distribution of Dominant Topics (k=11) by Quarter - {year}')\n",
        "    ax.set_xlabel('Quarter')\n",
        "    ax.set_ylabel('Number of Documents')\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    # Always use the same order and labels for legend\n",
        "    ordered_labels = [topic_labels_map.get(t, f\"Topic {t}\") for t in unique_topics]\n",
        "    ax.legend(handles, ordered_labels, title='Dominant Topic', loc='upper right')\n",
        "\n",
        "plt.suptitle(\n",
        "    \"Quarterly Distribution of Dominant Topics (k=11) by Year\\nGoldman Sachs Earnings Call Transcript (Q&A Section)\",\n",
        "    fontsize=16, fontweight='bold', y=1.02\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7817bea",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb15dc4f",
        "outputId": "230dcdf8-192c-4bd7-97fe-1b128a442bd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_dir = 'data/temp/'\n",
        "file_name = 'dummy_test_output_new.csv'\n",
        "dummy_pf = pd.DataFrame({'from_colab': [IS_COLAB, True, 'hello']})\n",
        "\n",
        "\n",
        "if OUTPUT_PROCESSED_FILES:\n",
        "    if IS_COLAB:\n",
        "        AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "\n",
        "        # Configure Git user (important for committing)\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {target_dir}{file_name}\n",
        "        print(f\"Added '{target_dir}{file_name}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {target_dir}{file_name}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "        print(\"Push command executed. Check output for success or prompt.\")\n",
        "    else:\n",
        "        dummy_pf.to_csv(f\"{target_dir}{file_name}\", index=False)\n",
        "        print(\"Processed files saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
