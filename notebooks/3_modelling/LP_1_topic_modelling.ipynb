{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"LP_topic_modelling_extended\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "8d029170-7213-4dfa-9f19-5d0efa686c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not in a git directory\n",
            "Cloning repository into 'src'...\n",
            "Clone complete.\n",
            "/content/src\n",
            "Collecting PyPDF2==3.0.1 (from -r requirements.txt (line 1))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.52.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.0.2)\n",
            "Collecting bertopic (from -r requirements.txt (line 12))\n",
            "  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.5.7)\n",
            "Collecting python-dev-tools (from -r requirements.txt (line 14))\n",
            "  Downloading python_dev_tools-2023.3.24-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: hdbscan==0.8.40 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.8.40)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.1.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.33.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.5.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic->-r requirements.txt (line 12)) (5.24.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.5.13)\n",
            "Collecting Sphinx<7,>=6 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading sphinx-6.2.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting autoflake<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading autoflake-1.7.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting black<24,>=23 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading black-23.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coverage<8,>=7 (from coverage[toml]<8,>=7->python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading coverage-7.9.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting darglint<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading darglint-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dlint<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading dlint-0.16.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting doc8<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading doc8-1.1.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting docformatter<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading docformatter-1.7.7-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting flake8<6,>=5 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting flake8-2020<2,>=1 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_2020-1.8.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting flake8-aaa<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_aaa-0.17.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting flake8-annotations<4,>=3 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations-3.1.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-annotations-complexity<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations_complexity-0.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-annotations-coverage<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_annotations_coverage-0.0.6-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-bandit<5,>=4 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_bandit-4.1.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-black<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8_black-0.3.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-blind-except<1,>=0 (from python-dev-tools->-r requirements.txt (line 14))\n",
            "  Downloading flake8-blind-except-0.2.1.tar.gz (3.7 kB)\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Install Imports"
      ],
      "metadata": {
        "id": "ryvB1Hpx1C3V"
      },
      "id": "ryvB1Hpx1C3V"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic\n"
      ],
      "metadata": {
        "id": "x4EyBg8j1FgP"
      },
      "id": "x4EyBg8j1FgP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d72bb0",
      "metadata": {
        "id": "02d72bb0"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6147c7a",
      "metadata": {
        "id": "c6147c7a"
      },
      "source": [
        "## Local Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2963e1f",
      "metadata": {
        "id": "f2963e1f"
      },
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file, read_list_from_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2569d135",
      "metadata": {
        "id": "2569d135"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60bdaad",
      "metadata": {
        "id": "a60bdaad"
      },
      "outputs": [],
      "source": [
        "def group_df(df, group_by_columns, agg_column='content'):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by specified columns and aggregates the content column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to group\n",
        "    - group_by_columns: List of columns to group by\n",
        "    - agg_column: Column to aggregate (default is 'content')\n",
        "\n",
        "    Returns:\n",
        "    - Grouped DataFrame with aggregated content\n",
        "    \"\"\"\n",
        "    return df.groupby(group_by_columns, as_index=False).agg({agg_column: ' '.join})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f87c1ff",
      "metadata": {
        "id": "4f87c1ff"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')\n",
        "\n",
        "\n",
        "# Goldman Sachs\n",
        "# grouped_gs_discussion_df = group_df(gs_discussion_df, ['quarter', 'year'])\n",
        "grouped_gs_qna_df = group_df(gs_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n",
        "\n",
        "# JP Morgan\n",
        "# grouped_jp_discussion_df = group_df(jp_discussion_df, ['quarter', 'year'])\n",
        "grouped_jp_qna_df = group_df(jp_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Global variable to store the spaCy model, to prevent re-loading multiple times\n",
        "_nlp_model = None\n",
        "\n",
        "def get_spacy_model():\n",
        "    \"\"\"\n",
        "    Loads and returns the spaCy 'en_core_web_sm' model.\n",
        "    Downloads it if not already present.\n",
        "    \"\"\"\n",
        "    global _nlp_model\n",
        "    if _nlp_model is None:\n",
        "        try:\n",
        "            # Try loading without parser and NER for speed if only sentencizer is needed\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "        except OSError:\n",
        "            print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "            # This command is for direct execution in environments like Colab\n",
        "            # In a standard Python script, you might run this once before your script\n",
        "            import subprocess\n",
        "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "    return _nlp_model\n",
        "\n",
        "def split_text_into_sentence_chunks(\n",
        "    df: pd.DataFrame,\n",
        "    text_column: str,\n",
        "    sentences_per_chunk: int = 2,\n",
        "    new_column_name: str = 'content_chunked'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Splits the content of a specified text column in a DataFrame into\n",
        "    chunks of a given number of sentences, and returns a new DataFrame\n",
        "    with rows exploded by these chunks.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input Pandas DataFrame.\n",
        "    - text_column: The name of the column in the DataFrame containing the text to be split.\n",
        "    - sentences_per_chunk: The approximate number of sentences per chunk. Defaults to 2.\n",
        "    - new_column_name: The name for the new column containing the text chunks.\n",
        "                       Defaults to 'content_chunked'.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with text content split into chunks and exploded into new rows.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # Ensure spaCy model is loaded\n",
        "    nlp_model_instance = get_spacy_model()\n",
        "\n",
        "    def _split_content_into_sentences_internal(text):\n",
        "        \"\"\"\n",
        "        Internal helper to split a single text into chunks of sentences.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return [] # Return empty list for non-string input\n",
        "\n",
        "        doc = nlp_model_instance(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()] # Filter empty sentences\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            current_chunk.append(sentence)\n",
        "            # Create a chunk if we've reached the desired number of sentences\n",
        "            # or if it's the last sentence(s) of the text\n",
        "            if (i + 1) % sentences_per_chunk == 0 or i == len(sentences) - 1:\n",
        "                if current_chunk: # Only add if the chunk is not empty\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [] # Reset for the next chunk\n",
        "        return chunks\n",
        "    df_copy[new_column_name] = df_copy[text_column].apply(_split_content_into_sentences_internal)\n",
        "    exploded_df = df_copy.explode(new_column_name)\n",
        "    exploded_df = exploded_df.drop(columns=[text_column])\n",
        "    exploded_df = exploded_df.rename(columns={new_column_name: text_column})\n",
        "\n",
        "    return exploded_df\n",
        "\n",
        "processed_gs_discussion_df = split_text_into_sentence_chunks(\n",
        "    gs_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")\n",
        "\n",
        "processed_jp_discussion_df = split_text_into_sentence_chunks(\n",
        "    jp_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")"
      ],
      "metadata": {
        "id": "xmjQuyT6hJJs"
      },
      "id": "xmjQuyT6hJJs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_jp_discussion_df.head(2)"
      ],
      "metadata": {
        "id": "H3-z0lfGijmo"
      },
      "id": "H3-z0lfGijmo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {
        "id": "73420e11"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cf4579",
      "metadata": {
        "id": "23cf4579"
      },
      "outputs": [],
      "source": [
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "jp_stopwords = set(read_list_from_text_file('src/data_processing/jp_morgan_topic_modelling_stopwords.txt'))\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2504.15683\n",
        "Use FinTextSim"
      ],
      "metadata": {
        "id": "Ctb_E-tzCBU_"
      },
      "id": "Ctb_E-tzCBU_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95b9b4e",
      "metadata": {
        "id": "b95b9b4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "gs_stopwords = nlp.Defaults.stop_words.union(gs_stopwords)\n",
        "jp_stopwords = nlp.Defaults.stop_words.union(jp_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb801083",
      "metadata": {
        "id": "fb801083"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "\n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "\n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# --- Custom Transformer for Text Preprocessing ---\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn transformer to apply text preprocessing.\n",
        "    It wraps the 'preprocess_text' function.\n",
        "    \"\"\"\n",
        "    def __init__(self, stop_words=None, abbreviations=None):\n",
        "        self.stop_words = stop_words\n",
        "        self.abbreviations = abbreviations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(\"Starting Phase 1: Preprocessing...\")\n",
        "        preprocessed_X = [preprocess_text(text, self.stop_words, self.abbreviations) for text in X]\n",
        "        print(\"Preprocessing complete.\")\n",
        "        return pd.Series(preprocessed_X)\n",
        "\n",
        "\n",
        "# --- Custom Estimator for BERTopic Modeling ---\n",
        "class BERTopicWrapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn estimator that wraps BERTopic.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model='all-MiniLM-L6-v2', umap_args=None, hdbscan_args=None,\n",
        "                 vectorizer_args=None, nr_topics=\"auto\", calculate_probabilities=True, **bertopic_kwargs):\n",
        "\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.umap_args = umap_args if umap_args is not None else {}\n",
        "        self.hdbscan_args = hdbscan_args if hdbscan_args is not None else {}\n",
        "        self.vectorizer_args = vectorizer_args if vectorizer_args is not None else {}\n",
        "        self.nr_topics = nr_topics\n",
        "        self.calculate_probabilities = calculate_probabilities\n",
        "        self.bertopic_kwargs = bertopic_kwargs\n",
        "        self.bertopic_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"\\nStarting Phase 3: Topic Modeling (BERTopic)...\")\n",
        "\n",
        "        # Initialize UMAP and HDBSCAN models\n",
        "        umap_model = UMAP(**self.umap_args)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(\n",
        "            # metric='euclidean',\n",
        "            # cluster_selection_method='eom',\n",
        "            # prediction_data=True, # Required for transform to assign topics to new data\n",
        "            **self.hdbscan_args\n",
        "        )\n",
        "\n",
        "        # Initialize SentenceTransformer\n",
        "        embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        default_min_df_for_bertopic_vectorizer = 1 # Changed from 10 to 1 for higher permissiveness\n",
        "\n",
        "        # Combine default vectorizer args with user-provided args\n",
        "        combined_vectorizer_args = {\n",
        "            'min_df': default_min_df_for_bertopic_vectorizer,\n",
        "            'ngram_range': (1, 3), # Common default for BERTopic's internal vectorizer\n",
        "            **self.vectorizer_args # User-provided vectorizer_args will override these defaults\n",
        "        }\n",
        "\n",
        "        vectorizer_model = TfidfVectorizer(**combined_vectorizer_args)\n",
        "\n",
        "\n",
        "        self.bertopic_model = BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            nr_topics=self.nr_topics,\n",
        "            calculate_probabilities=self.calculate_probabilities,\n",
        "            **self.bertopic_kwargs\n",
        "        )\n",
        "\n",
        "        # X is expected to be a pandas Series of preprocessed text\n",
        "        self.topics, self.probs = self.bertopic_model.fit_transform(X.tolist())\n",
        "        print(\"BERTopic model fitting complete.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.bertopic_model is None:\n",
        "            raise RuntimeError(\"BERTopic model not fitted. Call fit() first.\")\n",
        "        print(\"Transforming data with fitted BERTopic model...\")\n",
        "        topics, probs = self.bertopic_model.transform(X.tolist())\n",
        "        print(\"Transformation complete.\")\n",
        "        return topics # Return topic assignments\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.bertopic_model\n",
        "\n",
        "# --- Utility function to display topics (adapted for both LDA and BERTopic) ---\n",
        "def display_topics(model, vectorizer=None, no_top_words=10, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic.\n",
        "    Args:\n",
        "        model: The fitted topic model (LDA or BERTopic).\n",
        "        vectorizer (TfidfVectorizer, optional): The fitted TF-IDF vectorizer (for LDA).\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "        model_type (str): 'lda' or 'bertopic' to specify model type for appropriate display.\n",
        "    \"\"\"\n",
        "    topic_info = model.get_topic_info()\n",
        "    output_str = \"\\nBERTopic - Top Words per Topic:\\n\"\n",
        "    if file:\n",
        "        file.write(output_str)\n",
        "    else:\n",
        "        print(output_str)\n",
        "\n",
        "    # Iterate through all topics, excluding the noise topic (-1)\n",
        "    for topic_id in topic_info.Topic.unique():\n",
        "        if topic_id == -1: # Skip noise topic\n",
        "            continue\n",
        "        # Get the top words for the current topic\n",
        "        words = model.get_topic(topic_id)\n",
        "        if words:\n",
        "            top_words = \", \".join([word for word, _ in words[:no_top_words]])\n",
        "            topic_name = topic_info[topic_info['Topic'] == topic_id]['Name'].iloc[0]\n",
        "            output_str = f\"Topic {topic_id} ({topic_name}): {top_words}\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "        else:\n",
        "            output_str = f\"Topic {topic_id}: No words found.\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "\n",
        "\n",
        "# --- Main Topic Modeling Pipeline Class ---\n",
        "class TopicModelingPipeline:\n",
        "    def __init__(self, model_type='lda', **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the topic modeling pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of topic model to use ('lda' or 'bertopic').\n",
        "            **kwargs: Arguments specific to the chosen model or pipeline steps.\n",
        "                      For LDA: max_df, min_df, ngram_range (for TF-IDF), n_components, max_iter, etc.\n",
        "                      For BERTopic: embedding_model, umap_args, hdbscan_args, vectorizer_args, nr_topics, etc.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.pipeline = self._build_pipeline(**kwargs)\n",
        "\n",
        "    def _build_pipeline(self, **kwargs):\n",
        "        \"\"\"Builds the scikit-learn pipeline based on the specified model_type.\"\"\"\n",
        "        preprocessor_kwargs = {\n",
        "            'stop_words': kwargs.pop('stop_words', []),\n",
        "            'abbreviations': kwargs.pop('abbreviations', {})\n",
        "        }\n",
        "\n",
        "        pipeline_steps = [\n",
        "            ('preprocessor', TextPreprocessor(**preprocessor_kwargs))\n",
        "        ]\n",
        "\n",
        "        bertopic_kwargs = {\n",
        "            'embedding_model': kwargs.pop('embedding_model', 'all-MiniLM-L6-v2'),\n",
        "            'umap_args': kwargs.pop('umap_args', {}),\n",
        "            'hdbscan_args': kwargs.pop('hdbscan_args', {}),\n",
        "            'vectorizer_args': kwargs.pop('vectorizer_args', {}), # Pass custom vectorizer_args here\n",
        "            'nr_topics': kwargs.pop('nr_topics', \"auto\"),\n",
        "            'calculate_probabilities': kwargs.pop('calculate_probabilities', True),\n",
        "            **kwargs # Pass any remaining kwargs directly to BERTopicWrapper\n",
        "        }\n",
        "        pipeline_steps.append(('topic_modeler', BERTopicWrapper(**bertopic_kwargs)))\n",
        "\n",
        "        # Any remaining kwargs are ignored if not consumed by model-specific initializations\n",
        "        if kwargs:\n",
        "            print(f\"Warning: Unused keyword arguments passed to pipeline: {kwargs}\")\n",
        "\n",
        "        return Pipeline(pipeline_steps)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the entire pipeline to the input data.\"\"\"\n",
        "        print(f\"\\n--- Fitting {self.model_type.upper()} Topic Modeling Pipeline ---\")\n",
        "        self.pipeline.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforms the input data and returns topic assignments/distributions.\"\"\"\n",
        "        return self.pipeline.transform(X)\n",
        "\n",
        "    def get_topic_model(self):\n",
        "        \"\"\"Returns the underlying fitted topic model (LDA or BERTopic).\"\"\"\n",
        "        return self.pipeline.named_steps['topic_modeler'].get_model()\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\"Returns the fitted vectorizer (TF-IDF for LDA, None for BERTopic).\"\"\"\n",
        "        if self.model_type == 'lda':\n",
        "            return self.pipeline.named_steps['tfidf_vectorizer']\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goldman Sachs"
      ],
      "metadata": {
        "id": "8thgFdHAca1G"
      },
      "id": "8thgFdHAca1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "8mBJKOOwh5am"
      },
      "id": "8mBJKOOwh5am"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = gs_stopwords\n",
        "# target_df = processed_gs_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "y38SmyjzROEs"
      },
      "id": "y38SmyjzROEs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "EwPps44wh7Go"
      },
      "id": "EwPps44wh7Go"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/qna/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = gs_stopwords\n",
        "# target_df = grouped_gs_qna_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "T4btRdxHi3Wf"
      },
      "id": "T4btRdxHi3Wf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JP Morgan"
      ],
      "metadata": {
        "id": "SdmCQfm0cd_c"
      },
      "id": "SdmCQfm0cd_c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "BIDqnqPliEFl"
      },
      "id": "BIDqnqPliEFl"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = jp_stopwords\n",
        "# target_df = processed_jp_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "3kc5g4Ggjdl4"
      },
      "id": "3kc5g4Ggjdl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "2Z3hAYmLiFvF"
      },
      "id": "2Z3hAYmLiFvF"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = jp_stopwords\n",
        "# target_df = grouped_jp_qna_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [2, 4], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [2, 4], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "1OhFadzgjUdO"
      },
      "id": "1OhFadzgjUdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Fine Tuning Analysis"
      ],
      "metadata": {
        "id": "8OwPP3gncE1n"
      },
      "id": "8OwPP3gncE1n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GS Management Discussion"
      ],
      "metadata": {
        "id": "Yrkw-NiVB_sp"
      },
      "id": "Yrkw-NiVB_sp"
    },
    {
      "cell_type": "code",
      "source": [
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "target_stopwords = gs_stopwords\n",
        "target_df = processed_gs_discussion_df\n",
        "bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2',\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': 30, 'n_components': 10, 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': 5, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': 1, 'ngram_range': (1, 2)},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "bertopic_pipeline_instance.fit(target_df['content'])\n",
        "bertopic_gs_discussion_best_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "\n",
        "output_filename_bertopic = \"data/models/bert/gs_discussions_model/\"\n",
        "\n",
        "# output_dir =\n",
        "# You need to explicitly provide the embedding model name if you want it saved/reloaded correctly\n",
        "embedding_model_name = 'all-MiniLM-L6-v2'\n",
        "\n",
        "bertopic_gs_discussion_best_model.save(\n",
        "    output_filename_bertopic,\n",
        "    serialization=\"safetensors\",\n",
        "    save_ctfidf=True,  # Recommended to save c-TF-IDF for topic representations\n",
        "    save_embedding_model=embedding_model_name\n",
        ")\n",
        "\n",
        "\n",
        "!git config user.email \"{GITHUB_EMAIL}\"\n",
        "!git config user.name \"{GITHUB_USERNAME}\"\n",
        "!git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "# Add the file to staging\n",
        "!git add {output_filename_bertopic}\n",
        "print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "# Commit the changes\n",
        "commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "print(f\"Committed changes with message: '{commit_message}'\")\n",
        "print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "# Add this line to debug:\n",
        "print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "!git push --set-upstream origin {REPO_BRANCH} --force\n"
      ],
      "metadata": {
        "id": "0W08GMdIcU-q"
      },
      "id": "0W08GMdIcU-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goldman_sachs_topic_mapper = {\n",
        "    0: \"Client & Wealth Management Strategy\",\n",
        "    1: \"FICC & Intermediation Revenue\",\n",
        "    2: \"Earnings & ROE Presentation\",\n",
        "    3: \"Equity Capital & Dividends\",\n",
        "    4: \"Macroeconomic Conditions\",\n",
        "    5: \"Compensation & Operating Expenses\",\n",
        "    6: \"Assets under Supervision - AUS\",\n",
        "    7: \"Investment Banking Underwriting & Rankings\",\n",
        "    8: \"Loan Portfolio & Credit Provisions\",\n",
        "    9: \"Fee-Based Management & Private Banking\",\n",
        "    10: \"Commercial Real Estate Exposure\",\n",
        "    11: \"Question and Answer Session\",\n",
        "    12: \"Market Volume & Acquisition Financing\",\n",
        "    13: \"Investor Relations Conference Logistics\",\n",
        "    14: \"Executive Leadership & Governance\",\n",
        "    15: \"External Event Commentary\"\n",
        "}\n",
        "\n",
        "gs_topics_to_exclude = [11, 15]\n",
        "# It's good practice to ensure all active topics get a new name, and handle the noise topic if it exists\n",
        "current_topic_ids = sorted([t for t in bertopic_gs_discussion_best_model.get_topic_info().Topic.unique() if t != -1])\n",
        "actual_new_topic_names = {old_id: new_name for old_id, new_name in goldman_sachs_topic_mapper.items() if old_id in current_topic_ids}\n",
        "\n",
        "bertopic_gs_discussion_best_model.set_topic_labels(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "oON0UlnWrKoy"
      },
      "id": "oON0UlnWrKoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_gs_discussion_df['topic_id'] = bertopic_pipeline_instance.transform(processed_gs_discussion_df['content'])\n",
        "processed_gs_discussion_df['topic_name'] = processed_gs_discussion_df['topic_id'].map(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "FmrCaVhWrt64"
      },
      "id": "FmrCaVhWrt64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'topic_name' and time columns exist\n",
        "if 'topic_name' not in processed_gs_discussion_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in processed_gs_discussion_df.\")\n",
        "    # You might want to add a step here to assign topic names based on topic_id if needed\n",
        "elif 'year' not in processed_gs_discussion_df.columns or 'quarter' not in processed_gs_discussion_df.columns:\n",
        "     print(\"Error: 'year' or 'quarter' column not found in processed_gs_discussion_df.\")\n",
        "else:\n",
        "    # Calculate value counts per quarter and year for topic names\n",
        "    topic_counts = processed_gs_discussion_df.loc[~processed_gs_discussion_df['topic_id'].isin(gs_topics_to_exclude)].groupby(['year', 'quarter'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "    topic_proportions = topic_counts.div(topic_counts.sum(axis=1), axis=0)\n",
        "\n",
        "    num_topics = len(topic_proportions.columns)\n",
        "    # colors = sns.color_palette(palette=\"RdBu\", n_colors=num_topics) # Using 'viridis' for a professional look\n",
        "\n",
        "    colors = sns.color_palette(\"tab20\", n_colors=num_topics)\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "    plt.title('Topic Distribution per Quarter and Year (Goldman Sachs Management Discussion)')\n",
        "    plt.xlabel('Year-Quarter')\n",
        "    plt.ylabel('Number of Documents')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hS9myGiDsmc6"
      },
      "id": "hS9myGiDsmc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_gs_discussion_df['speaker'] = processed_gs_discussion_df['speaker'].replace('Dennis Coleman', 'Denis Coleman')"
      ],
      "metadata": {
        "id": "ehpzfGV5yETy"
      },
      "id": "ehpzfGV5yETy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_gs_discussion_df.loc[processed_gs_discussion_df['speaker'] == 'David Solomon']['role'].iloc[0]"
      ],
      "metadata": {
        "id": "tuhF2YveynZn"
      },
      "id": "tuhF2YveynZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_speaker_mapper = {}\n",
        "\n",
        "for speaker in list(processed_gs_discussion_df['speaker'].unique()):\n",
        "  gs_speaker_mapper[speaker] = speaker + \"\\n\" + processed_gs_discussion_df.loc[processed_gs_discussion_df['speaker'] == speaker]['role'].iloc[0].split(\",\")[-1]"
      ],
      "metadata": {
        "id": "2hoVhAJu1G73"
      },
      "id": "2hoVhAJu1G73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'speaker' not in processed_gs_discussion_df.columns:\n",
        "    print(\"Error: 'speaker' column not found in processed_gs_discussion_df.\")\n",
        "elif 'topic_name' not in processed_gs_discussion_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in processed_gs_discussion_df.\")\n",
        "else:\n",
        "    # Filter out excluded topics and noise topic\n",
        "    relevant_data = processed_gs_discussion_df.loc[~processed_gs_discussion_df['topic_id'].isin(gs_topics_to_exclude + [-1])].copy()\n",
        "\n",
        "    if not relevant_data.empty:\n",
        "        # Calculate topic counts per speaker\n",
        "        speaker_topic_counts = relevant_data.groupby(['speaker'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "        # print(speaker_topic_counts.columns)\n",
        "        # formatter_speaker_name = lambda row: row['speaker'] + \", \" + speaker_topic_counts.loc[speaker_topic_counts['speaker'] == row['speaker']]['role'].iloc[0]\n",
        "        # relevant_data['speaker'] = relevant_data.apply(formatter_speaker_name, axis=1)\n",
        "        # speaker_topic_counts['speaker'] = speaker_topic_counts.loc[speaker_topic_counts['speaker'] == 'David Solomon']['role'].iloc[0]\n",
        "        # Calculate topic proportions per speaker\n",
        "        speaker_topic_proportions = speaker_topic_counts.div(speaker_topic_counts.sum(axis=1), axis=0)\n",
        "        speaker_topic_proportions = speaker_topic_proportions.rename(index=gs_speaker_mapper)\n",
        "\n",
        "        num_topics_relevant = len(speaker_topic_proportions.columns)\n",
        "        colors = sns.color_palette(\"tab20\", n_colors=num_topics_relevant)\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        speaker_topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "        plt.title('Topic Distribution per Speaker (Goldman Sachs Management Discussion)')\n",
        "        plt.xlabel('Speaker')\n",
        "        plt.ylabel('Proportion of Documents')\n",
        "        plt.xticks(rotation=90, ha='right') # Rotate speaker labels if many speakers\n",
        "        plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No relevant data found after filtering topics.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "d5sG6d77xgi_"
      },
      "id": "d5sG6d77xgi_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_topic_counts_quarterly = relevant_data.groupby(['year', 'quarter', 'speaker'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Calculate topic proportions per speaker, year, and quarter\n",
        "# Ensure that the sum(axis=1) is not zero to avoid division by zero\n",
        "speaker_topic_proportions_quarterly = speaker_topic_counts_quarterly.div(\n",
        "    speaker_topic_counts_quarterly.sum(axis=1).replace(0, 1), # Replace 0 with 1 to prevent division by zero\n",
        "    axis=0\n",
        ")\n",
        "\n",
        "# Rename speakers in the MultiIndex using gs_speaker_mapper\n",
        "# 'level='speaker'' ensures only the speaker level of the MultiIndex is renamed\n",
        "# speaker_topic_proportions_quarterly = speaker_topic_proportions_quarterly.rename(index=gs_speaker_mapper, level='speaker')\n",
        "\n",
        "\n",
        "num_topics_relevant = len(speaker_topic_proportions_quarterly.columns)\n",
        "colors = sns.color_palette(\"tab20\", n_colors=num_topics_relevant)\n",
        "\n",
        "# Plotting\n",
        "# Increased figure width to provide more space for quarter groupings\n",
        "fig, ax = plt.subplots(figsize=(25, 10))\n",
        "# Plot the stacked bars. Reduced `width` for better visual separation between bars within a group.\n",
        "speaker_topic_proportions_quarterly.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.7) # Adjusted width\n",
        "\n",
        "plt.title('Topic Distribution by Speaker, Grouped by Year and Quarter (Goldman Sachs Management Discussion)')\n",
        "plt.ylabel('Proportion of Documents')\n",
        "\n",
        "# --- Custom X-axis Labeling for enhanced visual grouping ---\n",
        "\n",
        "# 1. Set individual speaker names as primary x-tick labels\n",
        "speaker_labels = speaker_topic_proportions_quarterly.index.get_level_values('speaker').tolist()\n",
        "ax.set_xticks(range(len(speaker_labels)))\n",
        "ax.set_xticklabels(speaker_labels, rotation=45, ha='right', fontsize=9) # Reduced rotation for readability\n",
        "ax.set_xlabel('') # Remove default x-label as we'll add custom ones\n",
        "\n",
        "# 2. Add major group labels (Year, Quarter) above the speaker labels\n",
        "# and vertical lines between these groups\n",
        "unique_quarter_years_data = [] # To store (year, quarter) and their bar indices\n",
        "current_year_quarter = None\n",
        "group_start_idx = 0\n",
        "\n",
        "for i, (year, quarter, speaker) in enumerate(speaker_topic_proportions_quarterly.index):\n",
        "    if (year, quarter) != current_year_quarter:\n",
        "        if current_year_quarter is not None:\n",
        "            # Calculate center position for the previous group\n",
        "            group_end_idx = i - 1\n",
        "            center_pos = (group_start_idx + group_end_idx) / 2\n",
        "            unique_quarter_years_data.append({\n",
        "                'label': f'Q{current_year_quarter[1]} {current_year_quarter[0]}',\n",
        "                'center': center_pos,\n",
        "                'start_idx': group_start_idx,\n",
        "                'end_idx': group_end_idx\n",
        "            })\n",
        "            # Add vertical line before the new group\n",
        "            ax.axvline(x=group_start_idx - 0.5, color='grey', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        current_year_quarter = (year, quarter)\n",
        "        group_start_idx = i\n",
        "    # Handle the last group after the loop\n",
        "    if i == len(speaker_topic_proportions_quarterly.index) - 1:\n",
        "        group_end_idx = i\n",
        "        center_pos = (group_start_idx + group_end_idx) / 2\n",
        "        unique_quarter_years_data.append({\n",
        "            'label': f'Q{current_year_quarter[1]} {current_year_quarter[0]}',\n",
        "            'center': center_pos,\n",
        "            'start_idx': group_start_idx,\n",
        "            'end_idx': group_end_idx\n",
        "        })\n",
        "\n",
        "\n",
        "# Add the 'Quarter, Year' labels using ax.text for precise control\n",
        "# We need to determine the y-coordinate for these labels\n",
        "y_label_pos = -0.15 # Adjust this value as needed, relative to the bottom of the plot area (0 is bottom)\n",
        "\n",
        "\n",
        "for group_info in unique_quarter_years_data:\n",
        "    ax.text(\n",
        "        group_info['center'],\n",
        "        y_label_pos, # Y-position for the label\n",
        "        group_info['label'],\n",
        "        ha='center', # Horizontal alignment\n",
        "        va='top',    # Vertical alignment (top of text aligned with y_label_pos)\n",
        "        transform=ax.get_xaxis_transform(), # Position relative to x-axis\n",
        "        fontsize=11,\n",
        "        fontweight='bold',\n",
        "        color='black'\n",
        "    )\n",
        "    # Add a secondary xlabel below the main text labels\n",
        "    ax.text(\n",
        "        group_info['center'],\n",
        "        y_label_pos - 0.05, # Further down for the sub-label\n",
        "        '(Speaker Contributions)',\n",
        "        ha='center',\n",
        "        va='top',\n",
        "        transform=ax.get_xaxis_transform(),\n",
        "        fontsize=8,\n",
        "        color='gray'\n",
        "    )\n",
        "\n",
        "speaker_map_text = \"Speaker Roles:\\n\"\n",
        "for original, abbreviation in gs_speaker_mapper.items():\n",
        "    speaker_map_text += f\"• {abbreviation}: {original}\\n\"\n",
        "\n",
        "# Position the text box relative to the figure\n",
        "fig.text(\n",
        "    1,  # X-coordinate (0 is left, 1 is right of the figure)\n",
        "    0.6,  # Y-coordinate (0 is bottom, 1 is top of the figure)\n",
        "    speaker_map_text,\n",
        "    fontsize=10,\n",
        "    verticalalignment='top',\n",
        "    bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5) # Optional: adds a background box\n",
        ")\n",
        "\n",
        "# Adjust layout to make room for the new labels and legend\n",
        "# Increased bottom rect value to give more space for the two lines of labels\n",
        "plt.tight_layout(rect=[0, 0.15, 0.95, 1]) # Adjusted bottom margin (0.15) and right margin (0.95)\n",
        "\n",
        "plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.savefig('topic_distribution_speaker_quarter_year_enhanced.png') # Save the plot with a new name\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "NPupJ1IO-hQc"
      },
      "id": "NPupJ1IO-hQc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GS QnA"
      ],
      "metadata": {
        "id": "iWPqsxpwCC0a"
      },
      "id": "iWPqsxpwCC0a"
    },
    {
      "cell_type": "code",
      "source": [
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "target_stopwords = gs_stopwords\n",
        "target_df = grouped_gs_qna_df\n",
        "bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2',\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': 30, 'n_components': 10, 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': 5, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': 1, 'ngram_range': (1, 2)},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "bertopic_pipeline_instance.fit(target_df['content'])\n",
        "bertopic_gs_qna_best_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "\n",
        "output_filename_bertopic = \"data/models/bert/gs_qna_model/\"\n",
        "\n",
        "# output_dir =\n",
        "# You need to explicitly provide the embedding model name if you want it saved/reloaded correctly\n",
        "embedding_model_name = 'all-MiniLM-L6-v2'\n",
        "\n",
        "bertopic_gs_qna_best_model.save(\n",
        "    output_filename_bertopic,\n",
        "    serialization=\"safetensors\",\n",
        "    save_ctfidf=True,  # Recommended to save c-TF-IDF for topic representations\n",
        "    save_embedding_model=embedding_model_name\n",
        ")\n",
        "\n",
        "\n",
        "!git config user.email \"{GITHUB_EMAIL}\"\n",
        "!git config user.name \"{GITHUB_USERNAME}\"\n",
        "!git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "# Add the file to staging\n",
        "!git add {output_filename_bertopic}\n",
        "print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "# Commit the changes\n",
        "commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "print(f\"Committed changes with message: '{commit_message}'\")\n",
        "print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "# Add this line to debug:\n",
        "print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "!git push --set-upstream origin {REPO_BRANCH} --force\n"
      ],
      "metadata": {
        "id": "iSKq04LbCEwd"
      },
      "id": "iSKq04LbCEwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goldman_sachs_topic_mapper = {\n",
        "    0: \"Client & Wealth Management Strategy\",\n",
        "    1: \"Firm Financials & Operational Efficiency\",\n",
        "    2: \"Alternative Investments & Private Credit\",\n",
        "    3: \"Capital Management & Key Financial Metrics\",\n",
        "    4: \"Market & Policy Risk Assessment\"\n",
        "}\n",
        "\n",
        "target_topics_to_exclude = []\n",
        "# It's good practice to ensure all active topics get a new name, and handle the noise topic if it exists\n",
        "current_topic_ids = sorted([t for t in bertopic_gs_qna_best_model.get_topic_info().Topic.unique() if t != -1])\n",
        "actual_new_topic_names = {old_id: new_name for old_id, new_name in goldman_sachs_topic_mapper.items() if old_id in current_topic_ids}\n",
        "\n",
        "bertopic_gs_qna_best_model.set_topic_labels(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "_1uNl2ryaefN"
      },
      "id": "_1uNl2ryaefN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_df['topic_id'] = bertopic_pipeline_instance.transform(target_df['content'])\n",
        "target_df['topic_name'] = target_df['topic_id'].map(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "LvSttJl4sgTT"
      },
      "id": "LvSttJl4sgTT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'topic_name' and time columns exist\n",
        "if 'topic_name' not in target_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in target_df.\")\n",
        "    # You might want to add a step here to assign topic names based on topic_id if needed\n",
        "elif 'year' not in target_df.columns or 'quarter' not in target_df.columns:\n",
        "     print(\"Error: 'year' or 'quarter' column not found in target_df.\")\n",
        "else:\n",
        "    # Calculate value counts per quarter and year for topic names\n",
        "    topic_counts = target_df.loc[~target_df['topic_id'].isin(target_topics_to_exclude)].groupby(['year', 'quarter'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "    topic_proportions = topic_counts.div(topic_counts.sum(axis=1), axis=0)\n",
        "\n",
        "    num_topics = len(topic_proportions.columns)\n",
        "    # colors = sns.color_palette(palette=\"RdBu\", n_colors=num_topics) # Using 'viridis' for a professional look\n",
        "\n",
        "    colors = sns.color_palette(\"tab20\", n_colors=num_topics)\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "    plt.title('Topic Distribution per Quarter and Year (Goldman Sachs QnA)')\n",
        "    plt.xlabel('Year-Quarter')\n",
        "    plt.ylabel('Number of Documents')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "BwCtSxSssojr"
      },
      "id": "BwCtSxSssojr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JP Morgan Management Discussion"
      ],
      "metadata": {
        "id": "FRrRI4aXyJIk"
      },
      "id": "FRrRI4aXyJIk"
    },
    {
      "cell_type": "code",
      "source": [
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "target_stopwords = jp_stopwords\n",
        "target_df = processed_jp_discussion_df\n",
        "bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2',\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': 15, 'n_components': 10, 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': 5, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': 5, 'ngram_range': (1, 2)},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "bertopic_pipeline_instance.fit(target_df['content'])\n",
        "bertopic_jp_discussion_best_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "\n",
        "output_filename_bertopic = \"data/models/bert/jp_discussions_model/\"\n",
        "\n",
        "# output_dir =\n",
        "# You need to explicitly provide the embedding model name if you want it saved/reloaded correctly\n",
        "embedding_model_name = 'all-MiniLM-L6-v2'\n",
        "\n",
        "bertopic_gs_discussion_best_model.save(\n",
        "    output_filename_bertopic,\n",
        "    serialization=\"safetensors\",\n",
        "    save_ctfidf=True,  # Recommended to save c-TF-IDF for topic representations\n",
        "    save_embedding_model=embedding_model_name\n",
        ")\n",
        "\n",
        "\n",
        "!git config user.email \"{GITHUB_EMAIL}\"\n",
        "!git config user.name \"{GITHUB_USERNAME}\"\n",
        "!git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "# Add the file to staging\n",
        "!git add {output_filename_bertopic}\n",
        "print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "# Commit the changes\n",
        "commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "print(f\"Committed changes with message: '{commit_message}'\")\n",
        "print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "# Add this line to debug:\n",
        "print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "!git push --set-upstream origin {REPO_BRANCH} --force\n"
      ],
      "metadata": {
        "id": "xU7WjbUpyJIo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xU7WjbUpyJIo"
    },
    {
      "cell_type": "code",
      "source": [
        "topic_mapper = {\n",
        "    0: \"Net Interest Income & Market Rates\",\n",
        "    1: \"Fixed Income & Security Services Revenue\",\n",
        "    2: \"Commercial Banking Operations & Provisions\",\n",
        "    3: \"Capital Position & Quarter-End Analysis\",\n",
        "    4: \"Consumer Business Performance & Deposits\",\n",
        "    5: \"Initial Earnings Report & Revenue Summary\",\n",
        "    6: \"Asset Inflows & Equity Levels\",\n",
        "    7: \"Asset & Wealth Management Profitability\",\n",
        "    8: \"Investment Banking & Deposit Impact\",\n",
        "    9: \"Loan Portfolio & Client Activity Trends\",\n",
        "    10: \"CIB & Investment Banking Fees/Rankings\",\n",
        "    11: \"Underwriting & Advisory Market Conditions\",\n",
        "    12: \"Question and Answer Session\"\n",
        "}\n",
        "\n",
        "jp_topics_to_exclude = [12]\n",
        "# It's good practice to ensure all active topics get a new name, and handle the noise topic if it exists\n",
        "current_topic_ids = sorted([t for t in bertopic_jp_discussion_best_model.get_topic_info().Topic.unique() if t != -1])\n",
        "actual_new_topic_names = {old_id: new_name for old_id, new_name in topic_mapper.items() if old_id in current_topic_ids}\n",
        "\n",
        "bertopic_jp_discussion_best_model.set_topic_labels(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "eX2_ffNDyJIp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eX2_ffNDyJIp"
    },
    {
      "cell_type": "code",
      "source": [
        "target_df['topic_id'] = bertopic_pipeline_instance.transform(target_df['content'])\n",
        "target_df['topic_name'] = target_df['topic_id'].map(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "uYicNv8myJIp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uYicNv8myJIp"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'topic_name' and time columns exist\n",
        "if 'topic_name' not in target_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in target_df.\")\n",
        "    # You might want to add a step here to assign topic names based on topic_id if needed\n",
        "elif 'year' not in target_df.columns or 'quarter' not in target_df.columns:\n",
        "     print(\"Error: 'year' or 'quarter' column not found in target_df.\")\n",
        "else:\n",
        "    # Calculate value counts per quarter and year for topic names\n",
        "    topic_counts = target_df.loc[~target_df['topic_id'].isin(jp_topics_to_exclude)].groupby(['year', 'quarter'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "    topic_proportions = topic_counts.div(topic_counts.sum(axis=1), axis=0)\n",
        "\n",
        "    num_topics = len(topic_proportions.columns)\n",
        "    # colors = sns.color_palette(palette=\"RdBu\", n_colors=num_topics) # Using 'viridis' for a professional look\n",
        "\n",
        "    colors = sns.color_palette(\"tab20\", n_colors=num_topics)\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "    plt.title('Topic Distribution per Quarter and Year (JP Morgan Management Discussion)')\n",
        "    plt.xlabel('Year-Quarter')\n",
        "    plt.ylabel('Number of Documents')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dVJuzvm0yJIp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dVJuzvm0yJIp"
    },
    {
      "cell_type": "code",
      "source": [
        "target_df['speaker'] = target_df['speaker'].replace('Dennis Coleman', 'Denis Coleman')"
      ],
      "metadata": {
        "id": "wrSlGsqsyJIq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wrSlGsqsyJIq"
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_mapper = {}\n",
        "\n",
        "for speaker in list(target_df['speaker'].unique()):\n",
        "  speaker_mapper[speaker] = speaker + \"\\n\" + target_df.loc[target_df['speaker'] == speaker]['role'].iloc[0].split(\",\")[-1]"
      ],
      "metadata": {
        "id": "TcBU21j9yJIq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TcBU21j9yJIq"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'speaker' not in target_df.columns:\n",
        "    print(\"Error: 'speaker' column not found in target_df.\")\n",
        "elif 'topic_name' not in target_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in target_df.\")\n",
        "else:\n",
        "    # Filter out excluded topics and noise topic\n",
        "    relevant_data = target_df.loc[~target_df['topic_id'].isin(jp_topics_to_exclude + [-1])].copy()\n",
        "\n",
        "    if not relevant_data.empty:\n",
        "        # Calculate topic counts per speaker\n",
        "        speaker_topic_counts = relevant_data.groupby(['speaker'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "        # print(speaker_topic_counts.columns)\n",
        "        # formatter_speaker_name = lambda row: row['speaker'] + \", \" + speaker_topic_counts.loc[speaker_topic_counts['speaker'] == row['speaker']]['role'].iloc[0]\n",
        "        # relevant_data['speaker'] = relevant_data.apply(formatter_speaker_name, axis=1)\n",
        "        # speaker_topic_counts['speaker'] = speaker_topic_counts.loc[speaker_topic_counts['speaker'] == 'David Solomon']['role'].iloc[0]\n",
        "        # Calculate topic proportions per speaker\n",
        "        speaker_topic_proportions = speaker_topic_counts.div(speaker_topic_counts.sum(axis=1), axis=0)\n",
        "        speaker_topic_proportions = speaker_topic_proportions.rename(index=speaker_mapper)\n",
        "\n",
        "        num_topics_relevant = len(speaker_topic_proportions.columns)\n",
        "        colors = sns.color_palette(\"tab20\", n_colors=num_topics_relevant)\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        speaker_topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "        plt.title('Topic Distribution per Speaker (JP Morgan Management Discussion)')\n",
        "        plt.xlabel('Speaker')\n",
        "        plt.ylabel('Proportion of Documents')\n",
        "        plt.xticks(rotation=90, ha='right') # Rotate speaker labels if many speakers\n",
        "        plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No relevant data found after filtering topics.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iNVSekMyyJIq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iNVSekMyyJIq"
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_topic_counts_quarterly = relevant_data.groupby(['year', 'quarter', 'speaker'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Calculate topic proportions per speaker, year, and quarter\n",
        "# Ensure that the sum(axis=1) is not zero to avoid division by zero\n",
        "speaker_topic_proportions_quarterly = speaker_topic_counts_quarterly.div(\n",
        "    speaker_topic_counts_quarterly.sum(axis=1).replace(0, 1), # Replace 0 with 1 to prevent division by zero\n",
        "    axis=0\n",
        ")\n",
        "\n",
        "# Rename speakers in the MultiIndex using gs_speaker_mapper\n",
        "# 'level='speaker'' ensures only the speaker level of the MultiIndex is renamed\n",
        "# speaker_topic_proportions_quarterly = speaker_topic_proportions_quarterly.rename(index=gs_speaker_mapper, level='speaker')\n",
        "\n",
        "\n",
        "num_topics_relevant = len(speaker_topic_proportions_quarterly.columns)\n",
        "colors = sns.color_palette(\"tab20\", n_colors=num_topics_relevant)\n",
        "\n",
        "# Plotting\n",
        "# Increased figure width to provide more space for quarter groupings\n",
        "fig, ax = plt.subplots(figsize=(25, 10))\n",
        "# Plot the stacked bars. Reduced `width` for better visual separation between bars within a group.\n",
        "speaker_topic_proportions_quarterly.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.7) # Adjusted width\n",
        "\n",
        "plt.title('Topic Distribution by Speaker, Grouped by Year and Quarter (JP Morgan Management Discussion)')\n",
        "plt.ylabel('Proportion of Documents')\n",
        "\n",
        "# --- Custom X-axis Labeling for enhanced visual grouping ---\n",
        "\n",
        "# 1. Set individual speaker names as primary x-tick labels\n",
        "speaker_labels = speaker_topic_proportions_quarterly.index.get_level_values('speaker').tolist()\n",
        "ax.set_xticks(range(len(speaker_labels)))\n",
        "ax.set_xticklabels(speaker_labels, rotation=45, ha='right', fontsize=9) # Reduced rotation for readability\n",
        "ax.set_xlabel('') # Remove default x-label as we'll add custom ones\n",
        "\n",
        "# 2. Add major group labels (Year, Quarter) above the speaker labels\n",
        "# and vertical lines between these groups\n",
        "unique_quarter_years_data = [] # To store (year, quarter) and their bar indices\n",
        "current_year_quarter = None\n",
        "group_start_idx = 0\n",
        "\n",
        "for i, (year, quarter, speaker) in enumerate(speaker_topic_proportions_quarterly.index):\n",
        "    if (year, quarter) != current_year_quarter:\n",
        "        if current_year_quarter is not None:\n",
        "            # Calculate center position for the previous group\n",
        "            group_end_idx = i - 1\n",
        "            center_pos = (group_start_idx + group_end_idx) / 2\n",
        "            unique_quarter_years_data.append({\n",
        "                'label': f'Q{current_year_quarter[1]} {current_year_quarter[0]}',\n",
        "                'center': center_pos,\n",
        "                'start_idx': group_start_idx,\n",
        "                'end_idx': group_end_idx\n",
        "            })\n",
        "            # Add vertical line before the new group\n",
        "            ax.axvline(x=group_start_idx - 0.5, color='grey', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        current_year_quarter = (year, quarter)\n",
        "        group_start_idx = i\n",
        "    # Handle the last group after the loop\n",
        "    if i == len(speaker_topic_proportions_quarterly.index) - 1:\n",
        "        group_end_idx = i\n",
        "        center_pos = (group_start_idx + group_end_idx) / 2\n",
        "        unique_quarter_years_data.append({\n",
        "            'label': f'Q{current_year_quarter[1]} {current_year_quarter[0]}',\n",
        "            'center': center_pos,\n",
        "            'start_idx': group_start_idx,\n",
        "            'end_idx': group_end_idx\n",
        "        })\n",
        "\n",
        "\n",
        "# Add the 'Quarter, Year' labels using ax.text for precise control\n",
        "# We need to determine the y-coordinate for these labels\n",
        "y_label_pos = -0.15 # Adjust this value as needed, relative to the bottom of the plot area (0 is bottom)\n",
        "\n",
        "\n",
        "for group_info in unique_quarter_years_data:\n",
        "    ax.text(\n",
        "        group_info['center'],\n",
        "        y_label_pos, # Y-position for the label\n",
        "        group_info['label'],\n",
        "        ha='center', # Horizontal alignment\n",
        "        va='top',    # Vertical alignment (top of text aligned with y_label_pos)\n",
        "        transform=ax.get_xaxis_transform(), # Position relative to x-axis\n",
        "        fontsize=11,\n",
        "        fontweight='bold',\n",
        "        color='black'\n",
        "    )\n",
        "    # Add a secondary xlabel below the main text labels\n",
        "    ax.text(\n",
        "        group_info['center'],\n",
        "        y_label_pos - 0.05, # Further down for the sub-label\n",
        "        '(Speaker Contributions)',\n",
        "        ha='center',\n",
        "        va='top',\n",
        "        transform=ax.get_xaxis_transform(),\n",
        "        fontsize=8,\n",
        "        color='gray'\n",
        "    )\n",
        "\n",
        "speaker_map_text = \"Speaker Roles:\\n\"\n",
        "for original, abbreviation in gs_speaker_mapper.items():\n",
        "    speaker_map_text += f\"• {abbreviation}: {original}\\n\"\n",
        "\n",
        "# Position the text box relative to the figure\n",
        "fig.text(\n",
        "    1,  # X-coordinate (0 is left, 1 is right of the figure)\n",
        "    0.6,  # Y-coordinate (0 is bottom, 1 is top of the figure)\n",
        "    speaker_map_text,\n",
        "    fontsize=10,\n",
        "    verticalalignment='top',\n",
        "    bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5) # Optional: adds a background box\n",
        ")\n",
        "\n",
        "# Adjust layout to make room for the new labels and legend\n",
        "# Increased bottom rect value to give more space for the two lines of labels\n",
        "plt.tight_layout(rect=[0, 0.15, 0.95, 1]) # Adjusted bottom margin (0.15) and right margin (0.95)\n",
        "\n",
        "plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.savefig('topic_distribution_speaker_quarter_year_enhanced.png') # Save the plot with a new name\n",
        "plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "qD4fjNLbyJIq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qD4fjNLbyJIq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JP Morgan QnA"
      ],
      "metadata": {
        "id": "9a8ezAEEyJIr"
      },
      "id": "9a8ezAEEyJIr"
    },
    {
      "cell_type": "code",
      "source": [
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "target_stopwords = jp_stopwords\n",
        "target_df = grouped_jp_qna_df\n",
        "bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2',\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': 15, 'n_components': 4, 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': 2, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': 1, 'ngram_range': (1, 2)},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "bertopic_pipeline_instance.fit(target_df['content'])\n",
        "bertopic_jp_qna_best_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "\n",
        "output_filename_bertopic = \"data/models/bert/jp_qna_model/\"\n",
        "\n",
        "# output_dir =\n",
        "# You need to explicitly provide the embedding model name if you want it saved/reloaded correctly\n",
        "embedding_model_name = 'all-MiniLM-L6-v2'\n",
        "\n",
        "bertopic_jp_qna_best_model.save(\n",
        "    output_filename_bertopic,\n",
        "    serialization=\"safetensors\",\n",
        "    save_ctfidf=True,  # Recommended to save c-TF-IDF for topic representations\n",
        "    save_embedding_model=embedding_model_name\n",
        ")\n",
        "\n",
        "\n",
        "!git config user.email \"{GITHUB_EMAIL}\"\n",
        "!git config user.name \"{GITHUB_USERNAME}\"\n",
        "!git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "# Add the file to staging\n",
        "!git add {output_filename_bertopic}\n",
        "print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "# Commit the changes\n",
        "commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "print(f\"Committed changes with message: '{commit_message}'\")\n",
        "print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "# Add this line to debug:\n",
        "print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "!git push --set-upstream origin {REPO_BRANCH} --force\n"
      ],
      "metadata": {
        "id": "3XD-Aa9LyJIr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3XD-Aa9LyJIr"
    },
    {
      "cell_type": "code",
      "source": [
        "topic_mapper = {\n",
        "    0: \"Capital, Growth & Strategic Intent\",\n",
        "    1: \"AOCI, Economic Factors & Timings\",\n",
        "    2: \"Net Interest Income & Capital Allocation\",\n",
        "    3: \"Product Performance, Liquidity & Quarterly Results\"\n",
        "}\n",
        "\n",
        "target_topics_to_exclude = []\n",
        "# It's good practice to ensure all active topics get a new name, and handle the noise topic if it exists\n",
        "current_topic_ids = sorted([t for t in bertopic_jp_qna_best_model.get_topic_info().Topic.unique() if t != -1])\n",
        "actual_new_topic_names = {old_id: new_name for old_id, new_name in topic_mapper.items() if old_id in current_topic_ids}\n",
        "\n",
        "bertopic_jp_qna_best_model.set_topic_labels(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "k5h5dDfiyJIr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "k5h5dDfiyJIr"
    },
    {
      "cell_type": "code",
      "source": [
        "target_df['topic_id'] = bertopic_pipeline_instance.transform(target_df['content'])\n",
        "target_df['topic_name'] = target_df['topic_id'].map(actual_new_topic_names)"
      ],
      "metadata": {
        "id": "pKqmPWGOyJIr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pKqmPWGOyJIr"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'topic_name' and time columns exist\n",
        "if 'topic_name' not in target_df.columns:\n",
        "    print(\"Error: 'topic_name' column not found in target_df.\")\n",
        "    # You might want to add a step here to assign topic names based on topic_id if needed\n",
        "elif 'year' not in target_df.columns or 'quarter' not in target_df.columns:\n",
        "     print(\"Error: 'year' or 'quarter' column not found in target_df.\")\n",
        "else:\n",
        "    # Calculate value counts per quarter and year for topic names\n",
        "    topic_counts = target_df.loc[~target_df['topic_id'].isin(target_topics_to_exclude)].groupby(['year', 'quarter'])['topic_name'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "    topic_proportions = topic_counts.div(topic_counts.sum(axis=1), axis=0)\n",
        "\n",
        "    num_topics = len(topic_proportions.columns)\n",
        "    # colors = sns.color_palette(palette=\"RdBu\", n_colors=num_topics) # Using 'viridis' for a professional look\n",
        "\n",
        "    colors = sns.color_palette(\"tab20\", n_colors=num_topics)\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 8), color=colors)\n",
        "\n",
        "    plt.title('Topic Distribution per Quarter and Year (JP Morgan QnA)')\n",
        "    plt.xlabel('Year-Quarter')\n",
        "    plt.ylabel('Number of Documents')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend(title='Topic Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3gLMYK5QyJIr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3gLMYK5QyJIr"
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "id": "bb15dc4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = nlp.stopwords\n",
        "# target_df = processed_jp_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}