{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3390045b",
      "metadata": {
        "id": "3390045b"
      },
      "source": [
        "# Setup, Constants, and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70ea4469",
      "metadata": {
        "id": "70ea4469"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd643fb0",
      "metadata": {
        "id": "fd643fb0"
      },
      "source": [
        "## Notebook Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c08565b",
      "metadata": {
        "id": "3c08565b"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "OUTPUT_PROCESSED_FILES = False # TODO: Use this if you want to output save files (optional - see below)\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_USERNAME = userdata.get('github_user')\n",
        "    GITHUB_TOKEN = userdata.get('github_token')\n",
        "    GITHUB_EMAIL = userdata.get('github_email')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9d4e41",
      "metadata": {
        "id": "cd9d4e41"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5129180d",
      "metadata": {
        "id": "5129180d"
      },
      "outputs": [],
      "source": [
        "REPO_URL = \"https://github.com/EErlando/Quarterly-Bytes.git\"\n",
        "REPO_NAME = \"src\"\n",
        "REPO_BRANCH = \"LP_topic_modelling_extended\" # TODO: UPDATE THIS TO YOU BRANCH - DEFAULT TO MAIN\n",
        "NOTEBOOK_DIR = \"3_modelling\" # TODO: UPDATE THIS TO YOUR NOTEBOOK DIRECTORY (e.g. 1_data_extraction_and_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0864529e",
      "metadata": {
        "id": "0864529e"
      },
      "source": [
        "## Clone and Pull Latest from Repository - Colab Specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "91c87440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c87440",
        "outputId": "169fd6d9-5feb-4b24-f5cd-f58446fabed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not in a git directory\n",
            "Directory 'src' already exists. Pulling latest changes...\n",
            "/content/src\n",
            "/content\n",
            "/content/src\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.52.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.17.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.5.7)\n",
            "Requirement already satisfied: python-dev-tools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (2023.3.24)\n",
            "Requirement already satisfied: hdbscan==0.8.40 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.8.40)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.1.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan==0.8.40->-r requirements.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.33.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 10)) (0.5.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic->-r requirements.txt (line 12)) (5.24.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn->-r requirements.txt (line 13)) (0.5.13)\n",
            "Requirement already satisfied: Sphinx<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.2.1)\n",
            "Requirement already satisfied: autoflake<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.8)\n",
            "Requirement already satisfied: black<24,>=23 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (23.12.1)\n",
            "Requirement already satisfied: coverage[toml]<8,>=7 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (7.9.1)\n",
            "Requirement already satisfied: darglint<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: dlint<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: doc8<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.2)\n",
            "Requirement already satisfied: docformatter<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.7)\n",
            "Requirement already satisfied: flake8<6,>=5 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (5.0.4)\n",
            "Requirement already satisfied: flake8-2020<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: flake8-aaa<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.17.0)\n",
            "Requirement already satisfied: flake8-annotations<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.1.1)\n",
            "Requirement already satisfied: flake8-annotations-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.0)\n",
            "Requirement already satisfied: flake8-annotations-coverage<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.6)\n",
            "Requirement already satisfied: flake8-bandit<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.1)\n",
            "Requirement already satisfied: flake8-black<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.6)\n",
            "Requirement already satisfied: flake8-blind-except<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.1)\n",
            "Requirement already satisfied: flake8-breakpoint<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: flake8-broken-line<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: flake8-bugbear<24,>=23 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (23.3.12)\n",
            "Requirement already satisfied: flake8-builtins<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.3)\n",
            "Requirement already satisfied: flake8-class-attributes-order<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-coding<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.3.2)\n",
            "Requirement already satisfied: flake8-cognitive-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.0)\n",
            "Requirement already satisfied: flake8-comments<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: flake8-comprehensions<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.16.0)\n",
            "Requirement already satisfied: flake8-debugger<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.2)\n",
            "Requirement already satisfied: flake8-django<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4)\n",
            "Requirement already satisfied: flake8-docstrings<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.0)\n",
            "Requirement already satisfied: flake8-encodings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.5.1)\n",
            "Requirement already satisfied: flake8-eradicate<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.0)\n",
            "Requirement already satisfied: flake8-executable<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: flake8-expression-complexity<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.11)\n",
            "Requirement already satisfied: flake8-fastapi<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: flake8-fixme<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.1.1)\n",
            "Requirement already satisfied: flake8-functions<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.8)\n",
            "Requirement already satisfied: flake8-functions-names<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: flake8-future-annotations<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.5)\n",
            "Requirement already satisfied: flake8-isort<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.1.2)\n",
            "Requirement already satisfied: flake8-literal<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: flake8-logging-format<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: flake8-markdown<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: flake8-mutable<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: flake8-no-pep420<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.8.0)\n",
            "Requirement already satisfied: flake8-noqa<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: flake8-pie<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: flake8-pyi<23,>=22 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (22.11.0)\n",
            "Requirement already satisfied: flake8-pylint<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.1)\n",
            "Requirement already satisfied: flake8-pytest-style<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.7.2)\n",
            "Requirement already satisfied: flake8-quotes<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.4.0)\n",
            "Requirement already satisfied: flake8-rst-docstrings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.1)\n",
            "Requirement already satisfied: flake8-secure-coding-standard<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: flake8-simplify<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.22.0)\n",
            "Requirement already satisfied: flake8-string-format<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-tidy-imports<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.11.0)\n",
            "Requirement already satisfied: flake8-typing-imports<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: flake8-use-fstring<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.4)\n",
            "Requirement already satisfied: flake8-use-pathlib<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: flake8-useless-assert<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.4)\n",
            "Requirement already satisfied: flake8-variables-names<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.0.6)\n",
            "Requirement already satisfied: flake8-warnings<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: jupyterlab-flake8<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.7.1)\n",
            "Requirement already satisfied: pandas-vet<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.2.3)\n",
            "Requirement already satisfied: pep8-naming<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.15.1)\n",
            "Requirement already satisfied: pip<23,>=22 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (22.3.1)\n",
            "Requirement already satisfied: pybetter<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.4.1)\n",
            "Requirement already satisfied: pycln<3,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: pycodestyle<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (2.9.1)\n",
            "Requirement already satisfied: pydocstyle<7,>=6 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (6.3.0)\n",
            "Requirement already satisfied: pytest<8,>=7 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (7.4.4)\n",
            "Requirement already satisfied: pytest-cov<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.1.0)\n",
            "Requirement already satisfied: pytest-sugar<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.9.7)\n",
            "Requirement already satisfied: pyupgrade<4,>=3 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (3.20.0)\n",
            "Requirement already satisfied: removestar<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (1.5.2)\n",
            "Requirement already satisfied: ssort<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: tox<5,>=4 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (4.27.0)\n",
            "Requirement already satisfied: tox-travis<1,>=0 in /usr/local/lib/python3.11/dist-packages (from python-dev-tools->-r requirements.txt (line 14)) (0.12)\n",
            "Requirement already satisfied: pyflakes<3,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autoflake<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black<24,>=23->python-dev-tools->-r requirements.txt (line 14)) (4.3.8)\n",
            "Requirement already satisfied: docutils<=0.21.2,>=0.19 in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.19)\n",
            "Requirement already satisfied: restructuredtext-lint>=0.7 in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: stevedore in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (5.4.1)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.19.1)\n",
            "Requirement already satisfied: charset_normalizer<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from docformatter<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (3.4.2)\n",
            "Requirement already satisfied: untokenize<0.2.0,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from docformatter<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.1.1)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from flake8<6,>=5->python-dev-tools->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: asttokens>=2 in /usr/local/lib/python3.11/dist-packages (from flake8-aaa<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: attrs>=21.4 in /usr/local/lib/python3.11/dist-packages (from flake8-annotations<4,>=3->python-dev-tools->-r requirements.txt (line 14)) (25.3.0)\n",
            "Requirement already satisfied: bandit>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from flake8-bandit<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (1.8.5)\n",
            "Requirement already satisfied: flake8-plugin-utils<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from flake8-breakpoint<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.3.3)\n",
            "Requirement already satisfied: cognitive-complexity in /usr/local/lib/python3.11/dist-packages (from flake8-cognitive-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: astroid<3.0.0,>=2.15.2 in /usr/local/lib/python3.11/dist-packages (from flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.15.8)\n",
            "Requirement already satisfied: astatine>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.3.3)\n",
            "Requirement already satisfied: domdf-python-tools>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: flake8-helper>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: eradicate<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from flake8-eradicate<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (2.3.0)\n",
            "Requirement already satisfied: astpretty in /usr/local/lib/python3.11/dist-packages (from flake8-expression-complexity<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: fastapi>=0.65.1 in /usr/local/lib/python3.11/dist-packages (from flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.115.12)\n",
            "Requirement already satisfied: mr-proper in /usr/local/lib/python3.11/dist-packages (from flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.0.7)\n",
            "Requirement already satisfied: isort<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from flake8-isort<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (5.13.2)\n",
            "Requirement already satisfied: pylint in /usr/local/lib/python3.11/dist-packages (from flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (2.17.7)\n",
            "Requirement already satisfied: astor>=0.1 in /usr/local/lib/python3.11/dist-packages (from flake8-simplify<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 10)) (1.1.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn->-r requirements.txt (line 13)) (0.43.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 12)) (9.1.2)\n",
            "Requirement already satisfied: hypothesmith<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.1.9)\n",
            "Requirement already satisfied: libcst<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.4.10)\n",
            "Requirement already satisfied: pyemojify<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: tomlkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from pycln<3,>=1->python-dev-tools->-r requirements.txt (line 14)) (0.13.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pydocstyle<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (3.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from pytest<8,>=7->python-dev-tools->-r requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: termcolor>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytest-sugar<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: tokenize-rt>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from pyupgrade<4,>=3->python-dev-tools->-r requirements.txt (line 14)) (6.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->-r requirements.txt (line 5)) (2025.6.15)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (2.17.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from Sphinx<7,>=6->python-dev-tools->-r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.5)\n",
            "Requirement already satisfied: cachetools>=5.5.1 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (5.5.2)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (5.2.0)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: pyproject-api>=1.8 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.31 in /usr/local/lib/python3.11/dist-packages (from tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (20.31.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 5)) (7.1.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from astroid<3.0.0,>=2.15.2->flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.11.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from astroid<3.0.0,>=2.15.2->flake8-django<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (1.17.2)\n",
            "Requirement already satisfied: natsort>=7.0.1 in /usr/local/lib/python3.11/dist-packages (from domdf-python-tools>=2.8.1->flake8-encodings<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (8.4.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.46.2)\n",
            "Requirement already satisfied: hypothesis>=5.41.0 in /usr/local/lib/python3.11/dist-packages (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (6.135.14)\n",
            "Requirement already satisfied: lark-parser>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from libcst<0.5.0,>=0.4.1->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore->doc8<2,>=1->python-dev-tools->-r requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.31->tox<5,>=4->python-dev-tools->-r requirements.txt (line 14)) (0.3.9)\n",
            "Requirement already satisfied: stdlib-list>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mr-proper->flake8-functions<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.11.1)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from pylint->flake8-pylint<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (0.3.7)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis>=5.41.0->hypothesmith<0.2.0,>=0.1.8->pybetter<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.65.1->flake8-fastapi<1,>=0->python-dev-tools->-r requirements.txt (line 14)) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !git config pull.rebase false\n",
        "    if os.path.exists(REPO_NAME):\n",
        "        print(f\"Directory '{REPO_NAME}' already exists. Pulling latest changes...\")\n",
        "        %cd {REPO_NAME}\n",
        "        !git pull origin {REPO_BRANCH} --quiet\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{REPO_NAME}'...\")\n",
        "        !git clone --quiet --branch {REPO_BRANCH} {REPO_URL} {REPO_NAME}\n",
        "        print(\"Clone complete.\")\n",
        "\n",
        "    sys.path.append('/content/src/')\n",
        "    %cd /content/src/\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    if os.path.basename(os.getcwd()) == NOTEBOOK_DIR:\n",
        "        os.chdir('../../') # TODO: UPDATE THIS TO ROOT OF REPO\n",
        "\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Install Imports"
      ],
      "metadata": {
        "id": "ryvB1Hpx1C3V"
      },
      "id": "ryvB1Hpx1C3V"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4EyBg8j1FgP",
        "outputId": "5e55afa5-e506-454a-8d20-645a58e7e8c7"
      },
      "id": "x4EyBg8j1FgP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02d72bb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02d72bb0",
        "outputId": "62be790b-a30d-4068-fd2d-62454407e026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6147c7a",
      "metadata": {
        "id": "c6147c7a"
      },
      "source": [
        "## Local Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f2963e1f",
      "metadata": {
        "id": "f2963e1f"
      },
      "outputs": [],
      "source": [
        "from src.utils.common_helpers import read_yaml_file, read_list_from_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2569d135",
      "metadata": {
        "id": "2569d135"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a60bdaad",
      "metadata": {
        "id": "a60bdaad"
      },
      "outputs": [],
      "source": [
        "def group_df(df, group_by_columns, agg_column='content'):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by specified columns and aggregates the content column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to group\n",
        "    - group_by_columns: List of columns to group by\n",
        "    - agg_column: Column to aggregate (default is 'content')\n",
        "\n",
        "    Returns:\n",
        "    - Grouped DataFrame with aggregated content\n",
        "    \"\"\"\n",
        "    return df.groupby(group_by_columns, as_index=False).agg({agg_column: ' '.join})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b50738",
      "metadata": {
        "id": "62b50738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "4f87c1ff",
      "metadata": {
        "id": "4f87c1ff"
      },
      "outputs": [],
      "source": [
        "gs_discussion_df = pd.read_csv('data/processed/Goldman Sachs/discussion_df.csv')\n",
        "gs_qna_df = pd.read_csv('data/processed/Goldman Sachs/qna_df.csv')\n",
        "jp_discussion_df = pd.read_csv('data/processed/JP Morgan/discussion_df.csv')\n",
        "jp_qna_df = pd.read_csv('data/processed/JP Morgan/qna_df.csv')\n",
        "\n",
        "\n",
        "# Goldman Sachs\n",
        "# grouped_gs_discussion_df = group_df(gs_discussion_df, ['quarter', 'year'])\n",
        "grouped_gs_qna_df = group_df(gs_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n",
        "\n",
        "# JP Morgan\n",
        "# grouped_jp_discussion_df = group_df(jp_discussion_df, ['quarter', 'year'])\n",
        "grouped_jp_qna_df = group_df(jp_qna_df, ['question_answer_group_id', 'quarter', 'year'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Global variable to store the spaCy model, to prevent re-loading multiple times\n",
        "_nlp_model = None\n",
        "\n",
        "def get_spacy_model():\n",
        "    \"\"\"\n",
        "    Loads and returns the spaCy 'en_core_web_sm' model.\n",
        "    Downloads it if not already present.\n",
        "    \"\"\"\n",
        "    global _nlp_model\n",
        "    if _nlp_model is None:\n",
        "        try:\n",
        "            # Try loading without parser and NER for speed if only sentencizer is needed\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "        except OSError:\n",
        "            print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "            # This command is for direct execution in environments like Colab\n",
        "            # In a standard Python script, you might run this once before your script\n",
        "            import subprocess\n",
        "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "            _nlp_model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "            _nlp_model.add_pipe('sentencizer')\n",
        "    return _nlp_model\n",
        "\n",
        "def split_text_into_sentence_chunks(\n",
        "    df: pd.DataFrame,\n",
        "    text_column: str,\n",
        "    sentences_per_chunk: int = 2,\n",
        "    new_column_name: str = 'content_chunked'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Splits the content of a specified text column in a DataFrame into\n",
        "    chunks of a given number of sentences, and returns a new DataFrame\n",
        "    with rows exploded by these chunks.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input Pandas DataFrame.\n",
        "    - text_column: The name of the column in the DataFrame containing the text to be split.\n",
        "    - sentences_per_chunk: The approximate number of sentences per chunk. Defaults to 2.\n",
        "    - new_column_name: The name for the new column containing the text chunks.\n",
        "                       Defaults to 'content_chunked'.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with text content split into chunks and exploded into new rows.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # Ensure spaCy model is loaded\n",
        "    nlp_model_instance = get_spacy_model()\n",
        "\n",
        "    def _split_content_into_sentences_internal(text):\n",
        "        \"\"\"\n",
        "        Internal helper to split a single text into chunks of sentences.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return [] # Return empty list for non-string input\n",
        "\n",
        "        doc = nlp_model_instance(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()] # Filter empty sentences\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            current_chunk.append(sentence)\n",
        "            # Create a chunk if we've reached the desired number of sentences\n",
        "            # or if it's the last sentence(s) of the text\n",
        "            if (i + 1) % sentences_per_chunk == 0 or i == len(sentences) - 1:\n",
        "                if current_chunk: # Only add if the chunk is not empty\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [] # Reset for the next chunk\n",
        "        return chunks\n",
        "    df_copy[new_column_name] = df_copy[text_column].apply(_split_content_into_sentences_internal)\n",
        "    exploded_df = df_copy.explode(new_column_name)\n",
        "    exploded_df = exploded_df.drop(columns=[text_column])\n",
        "    exploded_df = exploded_df.rename(columns={new_column_name: text_column})\n",
        "\n",
        "    return exploded_df\n",
        "\n",
        "processed_gs_discussion_df = split_text_into_sentence_chunks(\n",
        "    gs_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")\n",
        "\n",
        "processed_jp_discussion_df = split_text_into_sentence_chunks(\n",
        "    jp_discussion_df,\n",
        "    text_column='content',\n",
        "    sentences_per_chunk=4\n",
        ")"
      ],
      "metadata": {
        "id": "xmjQuyT6hJJs"
      },
      "id": "xmjQuyT6hJJs",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_jp_discussion_df.head(2)"
      ],
      "metadata": {
        "id": "H3-z0lfGijmo",
        "outputId": "74a677ac-0b8d-46a4-9588-7763dd993ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "id": "H3-z0lfGijmo",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         speaker                     role               company  year  \\\n",
              "0  Jeremy Barnum  Chief Financial Officer  JPMorgan Chase & Co.  2022   \n",
              "0  Jeremy Barnum  Chief Financial Officer  JPMorgan Chase & Co.  2022   \n",
              "\n",
              "   quarter                                            content  \n",
              "0        1  Thanks, operator. Good morning, everyone. The ...  \n",
              "0        1  These results include approximately $900 milli...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c396ae31-659c-41e8-8aa0-82a5296b46c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>role</th>\n",
              "      <th>company</th>\n",
              "      <th>year</th>\n",
              "      <th>quarter</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "      <td>Thanks, operator. Good morning, everyone. The ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jeremy Barnum</td>\n",
              "      <td>Chief Financial Officer</td>\n",
              "      <td>JPMorgan Chase &amp; Co.</td>\n",
              "      <td>2022</td>\n",
              "      <td>1</td>\n",
              "      <td>These results include approximately $900 milli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c396ae31-659c-41e8-8aa0-82a5296b46c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c396ae31-659c-41e8-8aa0-82a5296b46c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c396ae31-659c-41e8-8aa0-82a5296b46c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b1268252-08df-4a85-9550-f4bb124d466d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1268252-08df-4a85-9550-f4bb124d466d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b1268252-08df-4a85-9550-f4bb124d466d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_jp_discussion_df",
              "summary": "{\n  \"name\": \"processed_jp_discussion_df\",\n  \"rows\": 341,\n  \"fields\": [\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Jeremy Barnum\",\n          \"Jamie Dimon\",\n          \"Jamie  Dimon\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"role\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Chairman, Chief Executive Officer\",\n          \"Chief Financial Officer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"company\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"JPMorgan Chase & Co.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2022,\n        \"max\": 2025,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 340,\n        \"samples\": [\n          \"Finally, credit costs were $316 million, driven by higher net lending activity, including in Markets,  and downgrades,\\npartially offset by improved macroeconomic variables. Then to complete our lines of business, AWM on page 6. Asset & Wealth Management reported net income of $1.4 billion with pre -tax margin\\nof 33%. For the quarter, revenue of $5.4 billion was up 9% year -on-year, driven by growth in management fees on higher average market\\nlevels and strong net inflows, investment valuation gains compared to losses in the prior -year, and higher brokerage activity, partially offset by\\ndeposit margin compression.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73420e11",
      "metadata": {
        "id": "73420e11"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23cf4579",
      "metadata": {
        "id": "23cf4579"
      },
      "outputs": [],
      "source": [
        "gs_stopwords = set(read_list_from_text_file('src/data_processing/goldman_sachs_topic_modelling_stopwords.txt'))\n",
        "abbreviations = read_yaml_file('src/abbreviations.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2504.15683\n",
        "Use FinTextSim"
      ],
      "metadata": {
        "id": "Ctb_E-tzCBU_"
      },
      "id": "Ctb_E-tzCBU_"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b95b9b4e",
      "metadata": {
        "id": "b95b9b4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\"SpaCy 'en_core_web_sm' model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "gs_stopwords = nlp.Defaults.stop_words.union(gs_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "fb801083",
      "metadata": {
        "id": "fb801083"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def preprocess_text(text: str, stop_words: set, abbreviations: dict) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'[-_]+', ' ', processed_text).strip()\n",
        "\n",
        "    sorted_phrases = sorted(abbreviations.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "\n",
        "    for abbrev, phrase in sorted_phrases:\n",
        "        processed_text = re.sub(r'\\b' + re.escape(phrase.lower()) + r'\\b', abbrev.lower(), processed_text)\n",
        "\n",
        "    processed_text = re.sub(r'\\b\\d+\\b', '', processed_text).strip()\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text not in stop_words or token.text in abbreviations.keys():\n",
        "            tokens.append(token.lemma_) # Lemmatize the token (abbreviations won't change)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# --- Custom Transformer for Text Preprocessing ---\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn transformer to apply text preprocessing.\n",
        "    It wraps the 'preprocess_text' function.\n",
        "    \"\"\"\n",
        "    def __init__(self, stop_words=None, abbreviations=None):\n",
        "        self.stop_words = stop_words\n",
        "        self.abbreviations = abbreviations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        print(\"Starting Phase 1: Preprocessing...\")\n",
        "        preprocessed_X = [preprocess_text(text, self.stop_words, self.abbreviations) for text in X]\n",
        "        print(\"Preprocessing complete.\")\n",
        "        return pd.Series(preprocessed_X)\n",
        "\n",
        "\n",
        "# --- Custom Estimator for BERTopic Modeling ---\n",
        "class BERTopicWrapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn estimator that wraps BERTopic.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model='all-MiniLM-L6-v2', umap_args=None, hdbscan_args=None,\n",
        "                 vectorizer_args=None, nr_topics=\"auto\", calculate_probabilities=True, **bertopic_kwargs):\n",
        "\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.umap_args = umap_args if umap_args is not None else {}\n",
        "        self.hdbscan_args = hdbscan_args if hdbscan_args is not None else {}\n",
        "        self.vectorizer_args = vectorizer_args if vectorizer_args is not None else {}\n",
        "        self.nr_topics = nr_topics\n",
        "        self.calculate_probabilities = calculate_probabilities\n",
        "        self.bertopic_kwargs = bertopic_kwargs\n",
        "        self.bertopic_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"\\nStarting Phase 3: Topic Modeling (BERTopic)...\")\n",
        "\n",
        "        # Initialize UMAP and HDBSCAN models\n",
        "        umap_model = UMAP(**self.umap_args)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(\n",
        "            # metric='euclidean',\n",
        "            # cluster_selection_method='eom',\n",
        "            # prediction_data=True, # Required for transform to assign topics to new data\n",
        "            **self.hdbscan_args\n",
        "        )\n",
        "\n",
        "        # Initialize SentenceTransformer\n",
        "        embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "\n",
        "        default_min_df_for_bertopic_vectorizer = 1 # Changed from 10 to 1 for higher permissiveness\n",
        "\n",
        "        # Combine default vectorizer args with user-provided args\n",
        "        combined_vectorizer_args = {\n",
        "            'min_df': default_min_df_for_bertopic_vectorizer,\n",
        "            'ngram_range': (1, 3), # Common default for BERTopic's internal vectorizer\n",
        "            **self.vectorizer_args # User-provided vectorizer_args will override these defaults\n",
        "        }\n",
        "\n",
        "        vectorizer_model = TfidfVectorizer(**combined_vectorizer_args)\n",
        "\n",
        "\n",
        "        self.bertopic_model = BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            nr_topics=self.nr_topics,\n",
        "            calculate_probabilities=self.calculate_probabilities,\n",
        "            **self.bertopic_kwargs\n",
        "        )\n",
        "\n",
        "        # X is expected to be a pandas Series of preprocessed text\n",
        "        self.topics, self.probs = self.bertopic_model.fit_transform(X.tolist())\n",
        "        print(\"BERTopic model fitting complete.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.bertopic_model is None:\n",
        "            raise RuntimeError(\"BERTopic model not fitted. Call fit() first.\")\n",
        "        print(\"Transforming data with fitted BERTopic model...\")\n",
        "        topics, probs = self.bertopic_model.transform(X.tolist())\n",
        "        print(\"Transformation complete.\")\n",
        "        return topics # Return topic assignments\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.bertopic_model\n",
        "\n",
        "# --- Utility function to display topics (adapted for both LDA and BERTopic) ---\n",
        "def display_topics(model, vectorizer=None, no_top_words=10, file=None):\n",
        "    \"\"\"\n",
        "    Prints or writes the top words for each topic.\n",
        "    Args:\n",
        "        model: The fitted topic model (LDA or BERTopic).\n",
        "        vectorizer (TfidfVectorizer, optional): The fitted TF-IDF vectorizer (for LDA).\n",
        "        no_top_words (int): The number of top words to display for each topic.\n",
        "        file (file object, optional): If provided, topics will be written to this file.\n",
        "        model_type (str): 'lda' or 'bertopic' to specify model type for appropriate display.\n",
        "    \"\"\"\n",
        "    topic_info = model.get_topic_info()\n",
        "    output_str = \"\\nBERTopic - Top Words per Topic:\\n\"\n",
        "    if file:\n",
        "        file.write(output_str)\n",
        "    else:\n",
        "        print(output_str)\n",
        "\n",
        "    # Iterate through all topics, excluding the noise topic (-1)\n",
        "    for topic_id in topic_info.Topic.unique():\n",
        "        if topic_id == -1: # Skip noise topic\n",
        "            continue\n",
        "        # Get the top words for the current topic\n",
        "        words = model.get_topic(topic_id)\n",
        "        if words:\n",
        "            top_words = \", \".join([word for word, _ in words[:no_top_words]])\n",
        "            topic_name = topic_info[topic_info['Topic'] == topic_id]['Name'].iloc[0]\n",
        "            output_str = f\"Topic {topic_id} ({topic_name}): {top_words}\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "        else:\n",
        "            output_str = f\"Topic {topic_id}: No words found.\\n\"\n",
        "            if file:\n",
        "                file.write(output_str)\n",
        "            else:\n",
        "                print(output_str)\n",
        "\n",
        "\n",
        "# --- Main Topic Modeling Pipeline Class ---\n",
        "class TopicModelingPipeline:\n",
        "    def __init__(self, model_type='lda', **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the topic modeling pipeline.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of topic model to use ('lda' or 'bertopic').\n",
        "            **kwargs: Arguments specific to the chosen model or pipeline steps.\n",
        "                      For LDA: max_df, min_df, ngram_range (for TF-IDF), n_components, max_iter, etc.\n",
        "                      For BERTopic: embedding_model, umap_args, hdbscan_args, vectorizer_args, nr_topics, etc.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.pipeline = self._build_pipeline(**kwargs)\n",
        "\n",
        "    def _build_pipeline(self, **kwargs):\n",
        "        \"\"\"Builds the scikit-learn pipeline based on the specified model_type.\"\"\"\n",
        "        preprocessor_kwargs = {\n",
        "            'stop_words': kwargs.pop('stop_words', []),\n",
        "            'abbreviations': kwargs.pop('abbreviations', {})\n",
        "        }\n",
        "\n",
        "        pipeline_steps = [\n",
        "            ('preprocessor', TextPreprocessor(**preprocessor_kwargs))\n",
        "        ]\n",
        "\n",
        "        bertopic_kwargs = {\n",
        "            'embedding_model': kwargs.pop('embedding_model', 'all-MiniLM-L6-v2'),\n",
        "            'umap_args': kwargs.pop('umap_args', {}),\n",
        "            'hdbscan_args': kwargs.pop('hdbscan_args', {}),\n",
        "            'vectorizer_args': kwargs.pop('vectorizer_args', {}), # Pass custom vectorizer_args here\n",
        "            'nr_topics': kwargs.pop('nr_topics', \"auto\"),\n",
        "            'calculate_probabilities': kwargs.pop('calculate_probabilities', True),\n",
        "            **kwargs # Pass any remaining kwargs directly to BERTopicWrapper\n",
        "        }\n",
        "        pipeline_steps.append(('topic_modeler', BERTopicWrapper(**bertopic_kwargs)))\n",
        "\n",
        "        # Any remaining kwargs are ignored if not consumed by model-specific initializations\n",
        "        if kwargs:\n",
        "            print(f\"Warning: Unused keyword arguments passed to pipeline: {kwargs}\")\n",
        "\n",
        "        return Pipeline(pipeline_steps)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the entire pipeline to the input data.\"\"\"\n",
        "        print(f\"\\n--- Fitting {self.model_type.upper()} Topic Modeling Pipeline ---\")\n",
        "        self.pipeline.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforms the input data and returns topic assignments/distributions.\"\"\"\n",
        "        return self.pipeline.transform(X)\n",
        "\n",
        "    def get_topic_model(self):\n",
        "        \"\"\"Returns the underlying fitted topic model (LDA or BERTopic).\"\"\"\n",
        "        return self.pipeline.named_steps['topic_modeler'].get_model()\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\"Returns the fitted vectorizer (TF-IDF for LDA, None for BERTopic).\"\"\"\n",
        "        if self.model_type == 'lda':\n",
        "            return self.pipeline.named_steps['tfidf_vectorizer']\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goldman Sachs"
      ],
      "metadata": {
        "id": "8thgFdHAca1G"
      },
      "id": "8thgFdHAca1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "8mBJKOOwh5am"
      },
      "id": "8mBJKOOwh5am"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/management_discussions/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = gs_stopwords\n",
        "target_df = processed_gs_discussion_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [5, 10], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y38SmyjzROEs",
        "outputId": "81085c3c-f1a2-4bf9-c2bd-8c29fbc2fbbd"
      },
      "id": "y38SmyjzROEs",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running experiment 1 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "On branch LP_topic_modelling_extended\n",
            "Your branch is ahead of 'origin/LP_topic_modelling_extended' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_20_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_20_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_30_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\u001b[m\n",
            "\t\u001b[31mdata/temp/leslie_topic_modelling_fine_tuning/bertopic_topics.txt\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 52, done.\n",
            "Counting objects: 100% (52/52), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (30/30), done.\n",
            "Writing objects: 100% (37/37), 48.49 KiB | 48.49 MiB/s, done.\n",
            "Total 37 (delta 20), reused 21 (delta 7), pack-reused 0\n",
            "remote: Resolving deltas: 100% (20/20), completed with 8 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            " + c58fabe...23a883d LP_topic_modelling_extended -> LP_topic_modelling_extended (forced update)\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 2 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended bd5bcc3] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 1.10 KiB | 1.10 MiB/s, done.\n",
            "Total 7 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   23a883d..bd5bcc3  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 3 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 3399829] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 1.00 KiB | 1.00 MiB/s, done.\n",
            "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   bd5bcc3..3399829  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 4 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 1c0d635] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_5_vectorizer_min_df_5_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 1.00 KiB | 1.00 MiB/s, done.\n",
            "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   3399829..1c0d635  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 5 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended 01aaa27] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 7 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 909 bytes | 909.00 KiB/s, done.\n",
            "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   1c0d635..01aaa27  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 6 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt' to staging.\n",
            "[LP_topic_modelling_extended 8764548] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            " 1 file changed, 7 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_5_hdbscan_min_cluster_size_10_vectorizer_min_df_1_vectorizer_ngram_range_1_2.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 912 bytes | 912.00 KiB/s, done.\n",
            "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   01aaa27..8764548  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 7 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Running experiment 8 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 10, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Running experiment 9 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)}: list index out of range\n",
            "\n",
            "--- Running experiment 10 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)}: list index out of range\n",
            "\n",
            "--- Running experiment 11 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 1)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Running experiment 12 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "Error running experiment with parameters {'umap_n_neighbors': 15, 'umap_n_components': 5, 'hdbscan_min_cluster_size': 20, 'vectorizer_min_df': 5, 'vectorizer_ngram_range': (1, 2)}: max_df corresponds to < documents than min_df\n",
            "\n",
            "--- Running experiment 13 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 1)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n",
            "Preprocessing complete.\n",
            "\n",
            "Starting Phase 3: Topic Modeling (BERTopic)...\n",
            "BERTopic model fitting complete.\n",
            "BERTopic Topics saved to data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Added 'data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt' to staging.\n",
            "[LP_topic_modelling_extended ec151bc] Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            " 1 file changed, 9 insertions(+)\n",
            " create mode 100644 data/temp/leslie_topic_modelling_fine_tuning/bert/bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt\n",
            "Committed changes with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Attempted commit with message: 'Add new data file: data/temp/leslie_topic_modelling_fine_tuning/bert//bertopic_topics_umap_n_neighbors_15_umap_n_components_10_hdbscan_min_cluster_size_5_vectorizer_min_df_1_vectorizer_ngram_range_1_1.txt'\n",
            "Value of REPO_BRANCH before push: LP_topic_modelling_extended\n",
            "Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\n",
            "Enumerating objects: 12, done.\n",
            "Counting objects: 100% (12/12), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (7/7), 1.02 KiB | 1.02 MiB/s, done.\n",
            "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/EErlando/Quarterly-Bytes.git\n",
            "   8764548..ec151bc  LP_topic_modelling_extended -> LP_topic_modelling_extended\n",
            "Branch 'LP_topic_modelling_extended' set up to track remote branch 'LP_topic_modelling_extended' from 'origin'.\n",
            "\n",
            "--- Running experiment 14 with parameters: {'umap_n_neighbors': 15, 'umap_n_components': 10, 'hdbscan_min_cluster_size': 5, 'vectorizer_min_df': 1, 'vectorizer_ngram_range': (1, 2)} ---\n",
            "\n",
            "--- Fitting BERTOPIC Topic Modeling Pipeline ---\n",
            "Starting Phase 1: Preprocessing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-541480733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mbertopic_pipeline_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_gs_qna_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mbertopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertopic_pipeline_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-3096219870.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"Fits the entire pipeline to the input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Fitting {self.model_type.upper()} Topic Modeling Pipeline ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-3096219870.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Phase 1: Preprocessing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mpreprocessed_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabbreviations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-3096219870.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Phase 1: Preprocessing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mpreprocessed_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabbreviations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24-3096219870.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text, stop_words, abbreviations)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b\\d+\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/tok2vec.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtokvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUMPY_OPS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray1i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mYf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/residual.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/maxout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape1f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "EwPps44wh7Go"
      },
      "id": "EwPps44wh7Go"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/gs/qna/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = gs_stopwords\n",
        "target_df = grouped_gs_qna_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [5, 10], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "T4btRdxHi3Wf"
      },
      "id": "T4btRdxHi3Wf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JP Morgan"
      ],
      "metadata": {
        "id": "SdmCQfm0cd_c"
      },
      "id": "SdmCQfm0cd_c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Management Discussion"
      ],
      "metadata": {
        "id": "BIDqnqPliEFl"
      },
      "id": "BIDqnqPliEFl"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = nlp.stopwords\n",
        "target_df = processed_jp_discussion_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [5, 10], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "3kc5g4Ggjdl4"
      },
      "id": "3kc5g4Ggjdl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "2Z3hAYmLiFvF"
      },
      "id": "2Z3hAYmLiFvF"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/qna/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "no_top_words = 10\n",
        "\n",
        "target_stopwords = nlp.stopwords\n",
        "target_df = grouped_jp_qna_df\n",
        "\n",
        "# Define parameter grids for grid-like search\n",
        "param_grid = {\n",
        "    'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "    'umap_n_components': [5, 10], # Common values: 2-15\n",
        "    'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "    'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "    'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "}\n",
        "\n",
        "# Generate all combinations of parameters\n",
        "keys = param_grid.keys()\n",
        "combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    params = dict(zip(keys, combo))\n",
        "\n",
        "    # Construct filename\n",
        "    filename_parts = []\n",
        "    for k, v in params.items():\n",
        "        if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "            filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "        else:\n",
        "            filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "    output_filename_base = \"_\".join(filename_parts)\n",
        "    output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "    model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "    print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline with current parameters\n",
        "        bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "            embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "            model_type='bertopic',\n",
        "            nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "            calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "            umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "            hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "            vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "            stop_words=target_stopwords,\n",
        "            abbreviations=abbreviations\n",
        "        )\n",
        "\n",
        "        bertopic_pipeline_instance.fit(target_df['content'])\n",
        "        bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "        # Save topic info to file\n",
        "        with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "            f.write(\"Interpreting Topics:\\n\")\n",
        "            display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "        print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "        !git config user.email \"{GITHUB_EMAIL}\"\n",
        "        !git config user.name \"{GITHUB_USERNAME}\"\n",
        "        !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "        # Add the file to staging\n",
        "        !git add {output_filename_bertopic}\n",
        "        print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "        # Commit the changes\n",
        "        commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "        print(f\"Committed changes with message: '{commit_message}'\")\n",
        "        print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "        # Add this line to debug:\n",
        "        print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "        print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "        !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "        results.append({\n",
        "            'params': params,\n",
        "            'output_file': output_filename_bertopic,\n",
        "            'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "            # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "        results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "print(\"\\n--- Grid Search Complete ---\")\n",
        "print(\"Summary of Runs:\")\n",
        "for res in results:\n",
        "    if 'error' in res:\n",
        "        print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ],
      "metadata": {
        "id": "1OhFadzgjUdO"
      },
      "id": "1OhFadzgjUdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8f10d7a1",
      "metadata": {
        "id": "8f10d7a1"
      },
      "source": [
        "# Save Data Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb15dc4f",
      "metadata": {
        "id": "bb15dc4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import itertools\n",
        "# output_dir = \"data/temp/leslie_topic_modelling_fine_tuning/bert/jp/management_discussions/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# AUTHENTICATED_REPO_URL = REPO_URL.replace(\"https://\", f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@\")\n",
        "# no_top_words = 10\n",
        "\n",
        "# target_stopwords = nlp.stopwords\n",
        "# target_df = processed_jp_discussion_df\n",
        "\n",
        "# # Define parameter grids for grid-like search\n",
        "# param_grid = {\n",
        "#     'umap_n_neighbors': [15, 30], # Common values: 5-50\n",
        "#     'umap_n_components': [5, 10], # Common values: 2-15\n",
        "#     'hdbscan_min_cluster_size': [5, 10], # Common values: 5-50+ depending on dataset size\n",
        "#     'vectorizer_min_df': [1, 5], # Common values: 1-10 or 0.01-0.05 (percentage)\n",
        "#     'vectorizer_ngram_range': [(1, 1), (1, 2)] # (1,1) for unigrams, (1,2) for unigrams and bigrams\n",
        "# }\n",
        "\n",
        "# # Generate all combinations of parameters\n",
        "# keys = param_grid.keys()\n",
        "# combinations = itertools.product(*(param_grid[key] for key in keys))\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for i, combo in enumerate(combinations):\n",
        "#     params = dict(zip(keys, combo))\n",
        "\n",
        "#     # Construct filename\n",
        "#     filename_parts = []\n",
        "#     for k, v in params.items():\n",
        "#         if isinstance(v, tuple): # Handle tuples like ngram_range\n",
        "#             filename_parts.append(f\"{k}_{'_'.join(map(str, v))}\")\n",
        "#         else:\n",
        "#             filename_parts.append(f\"{k}_{v}\")\n",
        "\n",
        "#     output_filename_base = \"_\".join(filename_parts)\n",
        "#     output_filename_bertopic = f\"{output_dir}/bertopic_topics_{output_filename_base}.txt\"\n",
        "#     model_save_path = f\"{output_dir}/bertopic_model_{output_filename_base}.joblib\"\n",
        "\n",
        "#     print(f\"\\n--- Running experiment {i+1} with parameters: {params} ---\")\n",
        "\n",
        "#     try:\n",
        "#         # Initialize pipeline with current parameters\n",
        "#         bertopic_pipeline_instance = TopicModelingPipeline(\n",
        "#             embedding_model='all-MiniLM-L6-v2', # Keep embedding model constant for this grid search\n",
        "#             model_type='bertopic',\n",
        "#             nr_topics=\"auto\", # Let HDBSCAN determine topics first, then prune if needed\n",
        "#             calculate_probabilities=False, # Set to False for faster runs if probabilities aren't immediately needed for tuning\n",
        "#             umap_args={'n_neighbors': params['umap_n_neighbors'], 'n_components': params['umap_n_components'], 'random_state': 42},\n",
        "#             hdbscan_args={'min_cluster_size': params['hdbscan_min_cluster_size'], 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True},\n",
        "#             vectorizer_args={'min_df': params['vectorizer_min_df'], 'ngram_range': params['vectorizer_ngram_range']},\n",
        "#             stop_words=target_stopwords,\n",
        "#             abbreviations=abbreviations\n",
        "#         )\n",
        "\n",
        "#         bertopic_pipeline_instance.fit(target_df['content'])\n",
        "#         bertopic_model = bertopic_pipeline_instance.get_topic_model()\n",
        "\n",
        "#         # Save topic info to file\n",
        "#         with open(output_filename_bertopic, 'w', encoding='utf-8') as f:\n",
        "#             f.write(f\"--- BERTopic Model - Parameters: {params} ---\\n\\n\")\n",
        "#             f.write(\"Interpreting Topics:\\n\")\n",
        "#             display_topics(bertopic_model, no_top_words=no_top_words, file=f)\n",
        "#         print(f\"BERTopic Topics saved to {output_filename_bertopic}\")\n",
        "\n",
        "#         !git config user.email \"{GITHUB_EMAIL}\"\n",
        "#         !git config user.name \"{GITHUB_USERNAME}\"\n",
        "#         !git remote set-url origin {AUTHENTICATED_REPO_URL}\n",
        "\n",
        "#         # Add the file to staging\n",
        "#         !git add {output_filename_bertopic}\n",
        "#         print(f\"Added '{output_filename_bertopic}' to staging.\")\n",
        "\n",
        "#         # Commit the changes\n",
        "#         commit_message = f\"Add new data file: {output_filename_bertopic}\"\n",
        "#         !git commit -m \"{commit_message}\"\n",
        "#         print(f\"Committed changes with message: '{commit_message}'\")\n",
        "#         print(f\"Attempted commit with message: '{commit_message}'\")\n",
        "\n",
        "#         # Add this line to debug:\n",
        "#         print(f\"Value of REPO_BRANCH before push: {REPO_BRANCH}\")\n",
        "\n",
        "#         print(\"Pushing changes to GitHub. Please enter your GitHub username and Personal Access Token when prompted.\")\n",
        "#         !git push --set-upstream origin {REPO_BRANCH} --force\n",
        "\n",
        "#         results.append({\n",
        "#             'params': params,\n",
        "#             'output_file': output_filename_bertopic,\n",
        "#             'num_topics': len(bertopic_model.get_topic_info()) - 1 # Exclude -1 topic\n",
        "#             # Add more metrics here if you implement them (e.g., coherence scores)\n",
        "#         })\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running experiment with parameters {params}: {e}\")\n",
        "#         results.append({'params': params, 'error': str(e)})\n",
        "\n",
        "# print(\"\\n--- Grid Search Complete ---\")\n",
        "# print(\"Summary of Runs:\")\n",
        "# for res in results:\n",
        "#     if 'error' in res:\n",
        "#         print(f\"  Parameters: {res['params']} -> ERROR: {res['error']}\")\n",
        "#     else:\n",
        "#         print(f\"  Parameters: {res['params']} -> Topics: {res['num_topics']}, Output: {res['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAa-7pTX73f4",
      "metadata": {
        "id": "QAa-7pTX73f4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}